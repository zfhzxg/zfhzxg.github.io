<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[HDFS]]></title>
    <url>%2F2018%2F10%2F01%2FHDFS%2F</url>
    <content type="text"><![CDATA[HDFS 的全分布配置12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061(1)hdfs-site.xml: &lt;!—配置数据块的冗余度，默认是3—&gt; &lt;!—原则冗余度跟数据节点保持一致，最大不要超过3—&gt; &lt;property&gt; &lt;name&gt;dis.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;!—是否开启HDFS权限检查，默认是true—&gt; &lt;property&gt; &lt;name&gt;dis.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; (2)core-site.xml: &lt;!—配置HDFS主节点位置，就是NameNode的位置—&gt; &lt;!—9000是RPC的通信端口—&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://RedHat112:9000&lt;/value&gt; &lt;/property&gt; &lt;!—HDFS元信息和数据块保存在操作目录的位置—&gt; &lt;!—默认是系统的tmp文件夹，会随断电而清除—&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/root/training/hadoop-2.7.3/tnp&lt;/value&gt; &lt;/porperty&gt; (3)mapped-site.xml &lt;!—MR运行容器或框架—&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; (4)yarn-site.xml &lt;!—配置yarn主节点的位置—&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager,hostname&lt;/name&gt; &lt;value&gt;RedHat112&lt;/value&gt; &lt;/property&gt; &lt;!—NodeManager执行MR任务的方式是Shuffle洗牌—&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;(4)slaves配置从节点地址: RedHat113 RedHat114 (5)对NameNode进行格式化(6)把RedHat上安装好的目录复制到从节点上 scp -r hadoop-2.7.3/ root@RedHat113:/root/training scp -r hadoop-2.7.3/ root@RedHat114:/root/training (7)在主节点上启动集群 start-all.sh HDFS的体系架构NameNode：名称节点 是HDFS的主节点、管理员 接收客户端（命令行、java程序）的请求：创建目录、上传数据、下载数据、删除数据等 管理和维护HDFS的日志和元信息 日志文件（edits文件）：记录的是客户端的所有操作，是一个二进制文件（JSON） 位置：/root/training/hadoop/tmp/dfs/name/current edit_inprogress_00000000000000XXXXX：正在操作的日志文件 hdfs oev -i edits_inprogress_00000000000000XXXXX -o ~/a.xml：通过日志查看器（edits viewer），把edits文件转换成文本（xml）格式 元信息（fsimage文件）：记录的是数据块的位置信息，数据块的冗余信息，是一个二进制文件 位置：/root/training/hadoop/tmp/dfs/name/current fsimage_0000000000000XXXXX：元信息记录文件 hdfs oiv -i fsimage_000000000000XXXXX -o ~/b.xml：将元信息记录文件转换成文本（xml或txt）格式 DataNode：数据节点 按照数据块保存数据 1.X : 64M2.X : 128M 数据块：表现形式就是一个文件（blk打头） 位置：/root/training/hadoop-2.7.3/tmp/dfs/data/current/BP-XXX-数据节点-XXX/current/finalized/subdir0/subdir0/ 一个数据块对应的是一对文件，‘.meta’记录的是数据块的元信息 设置数据块冗余度规则：一般跟数据节点个数相同，但最大不要超过3 Hadoop 3.X之前，会造成存储空间极大的浪费 Hadoop 3.X之后，采用HDFS纠删码技术，使得存储空间节约一半 SecondaryNameNode：第二名称节点 职责：进行日志信息的合并 SecondaryNameNode向NameNode下载edits日志文件和fsimage元信息文件 将edits中最新的信息写入fsimage文件 将合并后的文件上传给NameNode 当上次合并发生以后，用户进行新的操作，NameNode将产生新的edits_inprogress 当HDFS发出检查点（checkpoint）的时候，会进行日志信息合并 默认情况下，HDFS每隔60分钟或edits文件达到了64M产生一个检查点 由于edits文件记录了最新的状态信息，并且随着操作越多，edits就会越大 把edits中的最新信息写到fsimage中 edits文件就可以清空]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[主从结构的单点故障]]></title>
    <url>%2F2018%2F09%2F28%2F%E4%B8%BB%E4%BB%8E%E7%BB%93%E6%9E%84%E7%9A%84%E5%8D%95%E7%82%B9%E6%95%85%E9%9A%9C%2F</url>
    <content type="text"><![CDATA[主从结构的主出现故障时视为单点故障HDFS NameNode（主）+DataNode（从） Yarn ResourceManager（主）+NodeManager（从） HBase HMaster（主）+RefionServer（从） Storm nimbus（主）+supervisor Spark Master（主）+Worker（从） #单点故障的解决方法（HA） 使用zookeeper实现HA功能，当主节点（active）出现故障时，通过主节点（standby）操作HDFS]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hive]]></title>
    <url>%2F2018%2F09%2F26%2Fhive%2F</url>
    <content type="text"><![CDATA[数据分析引擎一、Hadoop中 （1）Hive：支持SQL（2）Pig：支持PigLation 二、Spark中__ （*）Spark SQL：类似Hive，支持SQL、DSL 三、另一个：Impala 什么是Hive一、Hive是基于HDFS之上的一个数据仓库 Hive —-&gt; HDFS表 ——&gt; 目录数据 —-&gt; 文件分区 —-&gt; 目录桶 ——&gt; 文件 二、Hive是基于Hadoop之上的一个数据分析引擎 Hive 2.X 以前：SQL —-&gt; Hive —-&gt; MapReduceHive 2.X 以后：推荐使用Spark作为SQL的执行引擎（只针对Hadoop 3.X以前）（《Hive on Spark文档》） Hive的体系架构 一、CLI（命令行）：直接由Hive Dirver翻译二、JDBC（标准接口）：1.X由Thrift Server，2.X由Hive Server翻译为SQL语句交由Hive Dirver，端口号都为10000三、HWI（Hive Web Interface）：只在Hive 2.2前提供HWI网页工具，推荐使用HUE，由Hive Dirver翻译*、在Hive的体系架构中还需要有关系型数据库用来存储Hive元信息（推荐使用MySQL） 安装和配置Hive 安装模式： 准备工作： 12345678910111213141516171819202122232425262728293031321、解压 tar -zxvf apache-hive-2.3.0-bin.tar.gz -C ~/training/2、设置环境变量 vi ~/.bash_profile HIVE_HOME=/root/training/apache-hive-2.3.0-bin export HIVE_HOME PATH=$HIVE_HOME/bin:$PATH export PATH3.安装配置MySQL数据库 在虚拟机上安装MySQL： yum remove mysql-libs rpm -ivh mysql-community-common-5.7.19-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-5.7.19-1.el7.x86_64.rpm rpm -ivh mysql-community-client-5.7.19-1.el7.x86_64.rpm rpm -ivh mysql-community-server-5.7.19-1.el7.x86_64.rpm rpm -ivh mysql-community-devel-5.7.19-1.el7.x86_64.rpm （可选，但还是装上，后面装HUE的时候会用到。） 启动MySQL：service mysqld start 或者：systemctl start mysqld.service 查看root用户的密码：cat /var/log/mysqld.log | grep password 登录后修改密码：alter user &apos;root&apos;@&apos;localhost&apos; identified by &apos;Welcome_1&apos;; MySQL数据库的配置： 创建一个新的数据库：create database hive; 创建一个新的用户： create user &apos;hiveowner&apos;@&apos;%&apos; identified by &apos;Welcome_1&apos;; 给该用户授权 grant all on hive.* TO &apos;hiveowner&apos;@&apos;%&apos;; grant all on hive.* TO &apos;hiveowner&apos;@&apos;localhost&apos; identified by &apos;Welcome_1&apos;; 免费工具：http://www.mysqlfront.de/ 嵌入模式：不需要MySQL，使用Hive自带的Derby数据库存储Hive的元信息 12345678910111213141516171819202122232425262728293031323334特点： （1）使用自带的Derby （2）只支持一个连接 （3）用于开发和测试创建hive-site.xml： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:derby:;databaseName=metastore_db;create=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;org.apache.derby.jdbc.EmbeddedDriver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;file:///root/training/apache-hive-2.3.0-bin/warehouse&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 初始化MetaStore： schematool -dbType derby -initSchema 日志： Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. 本地模式、远程模式：都需要MySQL 12345678910111213141516171819202122232425262728创建hive-site.xml： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive?useSSL=false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hiveowner&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;tiger&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;初始化MetaStore： schematool -dbType mysql -initSchema *Hive的数据类型 一、内部表：类似MySQL、Oracle中的表二、外部表三、分区表：提高性能 （*）补充：如何提高性能？（SQL执行计划）四、桶表：类似Hash分区五、视图：View 执行Hive的查询：执行SQL（HQL） *、HQL是SQL的一个子集 使用JDBC查询Hive Hive的自定义函数]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HBase开发]]></title>
    <url>%2F2018%2F09%2F21%2FHBase%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[###NoSQL简介 一、什么是NoSQL数据库？（not only sql） 二、常见NoSQL数据库 HBaseRedis：基于内存的NoSQL数据库，前身MemCached（不支持持久化）MongoDB：基于文档型（BSON）的NoSQL数据库Cassandra：跟HBase类似 ###HBsae的体系架构（主从结构）和表结构 一、Hadoop的生态体系圈 二、HBase 基于HDFS之上的NoSQL数据库 123HBase HDFS表 ---&gt; 目录数据 ---&gt; 文件（HFile，默认大小：128MB） 三、HBase的体系架构（主节点：HMaster + 从节点：RegionServer） 单点故障 通过Zookeeper ###HBase的搭建模式 一、本地模式 二、伪分布模式 三、全分布模式 四、实现HBsae的HA ###操作HBsae 一、]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop免密码登陆原理]]></title>
    <url>%2F2018%2F09%2F14%2FHadoop%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E9%99%86%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[###hadoop免密码登陆原理 *不对称密码 密匙对（两个文件）： 公钥–锁（.ssh/id_rsa）：给别人加密私钥–钥匙（.ssh/id_rsa.pub）：给自己，解密 *对称加密（加密和解密使用同一文件） 一、生成密钥对（请求登陆方-A） 1ssh-keygen -t rsa 二、把请求登陆方生成的公钥拷贝给被请求方（B） 1ssh-copy-id -i .ssh/id_rsa.pub root@RedHat111 三、B收到请求方发来的公钥，并自动保存在.ssh/authorized_keys文件 四、B随机产生一个字符串：hello（前三部是配置，第四步开始，是认证的过程） 五、B使用请求登陆方的公钥进行加密（*），并发回给A 六、A收到B发来的加密字符串，使用自己的私钥进行解密（hello） 七、A把解密后的字符串（hello）发回给B进行认证 八、B收到A解密的字符串（hello） 九、B对从A收到的字符串（hello）与自己生成的字符串（hello）进行对比]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记]]></title>
    <url>%2F2018%2F09%2F10%2FMapReduce%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[###MapReduce高级特性 一、序列化 核心接口：Writable如果一个类实现了Writable该类的对象可以作为Key和Value 二、排序 规则：按照Key2排序（可以是基本数据类型，也可以是对象） 基本数据类型：数字（默认升序），字符串（默认字典顺序）可以通过创建自己的比较规则改变排序（extends IntWritable.Comparator/extends Text.Comparator）对象SQL排序：order by 列名、表达式、别名、序号 desc/asc（desc/asc只作用于最近的一列）MapReduce排序：1.该对象必须是Key2；2.必须实现序列化接口Writable；3.对象必须是可排序的（自定义排序使用java.long 接口 Comparable） 三、分区 什么是分区：partition查询： 1.没有分区：执行全表扫描2.有分区，只扫描分区分区的类型：Orcale： 1.范围分区；2.列表分区；3.Hash分区；4.Hash范围分区；5.Hash列表分区MR的分区：默认情况下，MR的输出只有一个分区（一个分区就是一个文件）自定义分区：按照字段进行分区（根据Map的输出&lt;Key2,Value2&gt;分区）*通过SQL的执行计划，判断效率是否提高 四、合并 合并是一种特殊的Reduce合并是在Map端执行一次合并，用于减少Mapper输出到Reduce的数据量，可以提高效率平均值不能使用combiner无论有没有combiner，都没不能改变Map和Reduce对应的数据类型 ###MapReduce核心：Shuffle（洗牌） Hadoop3.X之前会有数据落地（产生I/O操作） ###MapReduce常用案例 一、数据去重 相同Key名交给同一地址进行处理，处理后输出给Reduce，Key值唯一，Value形成数组 二、多表查询（笛卡尔集：列数相加，行数相乘） 等值链接的处理过程（以表作为Value1） Mapper阶段，通过分词后的列数或其他方法区分表Mapper阶段，在字段前添加标识输出给ReduceReduce进行处理 三、自连接 同一个表经Mapper输出两次非法数据要先经过处理（数据清理）Reduce进行处理 四、倒排索引 五、单元测试（MRUnit）]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 2.X管理与开发]]></title>
    <url>%2F2018%2F09%2F05%2FHadoop-2-X%E7%AE%A1%E7%90%86%E4%B8%8E%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[hadoop的起源与背景知识一、什么是大数据 举例: 1.电商的推荐系统(可能会用到推荐算法:协同过滤,ALS,逻辑回归…)2.天气预报 核心的问题:数据的存储,数据的计算(不是算法) 数据的存储:采用分布式的文件系统HDFS(hadoop Distributed file system) 数据的计算:采用分布式的计算MapReduce,Spark(RDD:弹性分布式数据集) 二、数据仓库和大数据传统方式:搭建数据仓库(Data Werehouse)来解决大数据的问题 1.数据仓库就是一个数据库(Orcale,MySQL,MS)2.数据仓库和大数据一般只做查询(分析)3.搭建数据仓库的过程 (1)数据源RDBMS:关系型数据库(结构化数据)/文本数据/其他数据(2)利用ETL抽取/转化/加载数据后搭建数据仓库(保存为原始数据)(3)分析处理数据(SQL,PL/SQL,JDBC)(4)经过分析后搭建数据集市(hr,sales)(5)提供给hr系统或销售系统 Hadoop和Spark都可以看成是数据仓库的一种实现 1.从ETL到搭建数据集都可以用Hadoop和Spark所提供的方式解决,也可以用传统方式解决2.hadoop中使用sqoop和Flume组件完成ETL3.hadoop中使用HDFS存储数据(或存入基于HDFS之上的HBase或Hive数据仓库)4.hadoop中使用MapReduce(java程序)或Spark(Scala程序,只有数据计算,没有数据存储)或SQL处理数据5.处理数据后使用HDFS或NoSQL:Redis存储到数据集市*.大数据的终极目标是使用SQL处理数据 三、OLTP和OLAP LTP:Online Transaction processing 联机事务处理，指：insert、update、delete —&gt; 事务 OLAP:Online Analytic Processing 联机分析处理，一般只做查询 —&gt; 数据仓库就是一种OLAP的应用系统 Hadoop、Spark看成是一种数据仓库的解决方案 数据仓库（查询）不支持事务 四、Google的基本思想:三篇论文(一)GFS（Google File System） —- HDFS（Hadoop Distributed File System）：分布式文件系统 (*) HDFS = NameNode + SecondaryNameNode + DataNode1.分布式文件系统2.大数据的存储问题3.HDFS中，记录数据的位置信息（元信息） —- 采用倒排索引（Reverted Index） (1)什么是索引？Index * CREATE INDEX创建索引 * 索引就是一个记录（Oracle中索引表保存的是有规律的行地址） * 通过索引可以找到对应的数据(2)什么是倒排索引？ * 最简单的倒排索引：单词表（wordID，word，index）(3)NameNode（主节点，名称节点）是整个HDFS的管理员，和SecondaryNameNode（第二名称节点）同处一台主机，负责管理DateNode（从节点，数据节点），并不负责存储，与DateNode（从节点，数据节点）构成HDFS环境 (二)MapReduce：分布计算模型，问题来源PageRank（网页排名） 1.PageRank（网页排名）2.MapReduce的标程模型： *. 核心：先拆分（拆分计算，Map阶段），再合并（Reduce阶段）*. MR任务：job=map+reduce*. Map的输出同时也是Reduce的输入*. 一个MR任务一共存在四对输入和输出（），Map的输入和输出，Reduce的输入和输出*. k2=k3，v2和v3数据类型一致，v3是一个集合，该集合中的每个值就是v2*. 所有的数据类型必须是Hadoop自己的数据类型（为了实现Hadoop的序列化机制）*. MR任务处理的是HDFS上的数据*. Hadoop2.X开始，通过Yarn容器编程部署MR任务（ResourceManager&lt;主节点&gt; + NodeManager&lt;从节点&gt;） 123Example:/root/training/Hadoop-2.7.3/Share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jarYarn的web console:http://192.168.226.11:8088命令://hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount /input/data.txt /out/wcl (三)BigTable：大表 —- NoSQL数据库：HBase 1.关系型数据库：以二维表的形式保存数据2.大表的基本思想：所有的数据存入一张表（通过牺牲空间，换取时间）3.常见的NoSQL数据库（一般为行式数据库，适合） * Redis：内存数据库（一般为行式数据库，适合DML操作，insert，update，delect）* MongoDB：面向文档（BSON文档：是JSON的二进制）* HBase：面向列（列式数据库，查询，select，在HBase中，rowKey不能为null，但是可以重复，相同的ey是一条记录） 1HBase = ZooKeeper + HMaster（主节点） + RegionServer（从节点） hadoop的环境一、Hadoop的目录结构 hadoop-2.7.3/ —&gt; Hadoop的HOME目录bin/ —&gt; Hadoop的操作命令etc/hadoop/ —&gt; 所有的配置文件sbin/ —&gt; Hadoop集群的命令：启动，停止等share/ —&gt; 所有共享文件share/hadoop –&gt; 所有依赖jar包 二、Hadoop的三种安装模式(一)本地模式 * 没有HDFS，只能测试MapReduce程序（不是运行在Yarn中，作为一个独立的Java程序来运行）* 配置文件：etc/Hadoop/hadoop-env.sh export JAVA_HOME={JAVA_HOME} 自己的JAVA_HOME路径* 本地模式中用到的路径都是本地路径，因为没有HDFS (二)伪分布模式 * 特点：再单击上模拟一个分布式的环境，具备Hadoop的所有功能* 具备HDFS：NameNode + DataNode + SecondaryNameNode（端口50070）* 具备Yarn：ResourceManager + NodeManager（端口8088）* 对HDFS的NameNode进行格式化（/root/training/hadoop-2.7.3/tmp）* 启动HDFS：start-dfs.sh* 启动Yarn：start-yarn.sh* 统一启动：start-all.sh* 配置文件：etc/Hadoop/hadoop-env.sh export JAVA_HOME={JAVA_HOME} 自己的JAVA_HOME路径* 配置文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//hdfs-site.xml://配置数据块的冗余度//原则冗余度跟数据节点的个数保持一致，最大不要超过3&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;//是否开启HDFS权限检查，默认值为true（使用默认值，需要再改）&lt;!-- &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;--&gt;//core-site.xml://配置HDFS主节点的地址，就是NameNode的地址//9000是RPC的通信端口&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://RedHat111:9000&lt;/value&gt;&lt;/property&gt;//HDFS数据块和元信息保存在操作系统的目录位置//默认值是Linux的tmp目录&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/root/training/hadoop-2.7.3/tmp&lt;/value&gt;&lt;/property&gt;//mapred-site.xml:（默认没有这个文件）//MR程序运行的程序或框架&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;//yarn-site.xml://配置yarn主节点的位置&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;RedHat111&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; (三)全分布模式三、主从结构的单点故障]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基础命令]]></title>
    <url>%2F2018%2F09%2F05%2F%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[文件目录操作命令ls 显示文件和目录列表 -l 列出文件的详细信息-a 列出当前目录所有文件，包含隐藏文件*设置环境变量：/root/.bash_profile pwd 显示当前目录位置mkdir 创建目录 -p 父目录不存在的情况下先生成父目录约定： mkdir /root/tools —-&gt;安装包mkdir /root/training —-&gt;安装目录 cd 切换目录touch 生成一个空文件echo 生成一个带内容文件 使用echo查看环境变量值：echo $JAVA_HOME cat、tac 显示文本文件内容 cat是从第一行开始写，tac是从最后一行开始写 cp 复制文件或目录rm 删除文件 -r 同时删除该目录下的所有文件-f 强制删除文件或目录*HDFS有回收站，默认情况下关闭 系统操作命令ps 显示瞬间的进程状态 ps -ef：使用标准格式显示每个进程信息 hostname 显示主机名kill 杀死一个进程 -9 强制杀死一个进程-3 如果针对java进程，打印java进程的线程信息Thread Dump 打包命令gzip 压缩（解压）文件或目录，压缩文件后缀为gzbzip2 压缩（解压）文件或目录，压缩文件后缀为bz2tar 文件、目录打（解）包 -zxvf 解压缩-C 解压到指定目录下 权限管理（非常类似HDFS权限管理）权限的类型 r 读w 写x 执行 ls -l、ll 查看权限 十位字符，第一位如果为d，则代表该文件为目录后九位分为三组，每一组都按读写执行的顺序排列 第一组：当前用户第二组：同组用户第三组：其他用户权限用二进制表示，有为1，没有为0（rwx -&gt; 111，rw- -&gt; 110）chmod：改变权限chmod (u/g/o/a)+(r/w/x) 文件名（括号内可选）chmod 777 文件名（每一位十进制数代表一组权限，此处为所有用户可读可写可执行） 案例分析:java的死锁或者性能瓶颈分析 JDK heap dump:分析OOM的问题JDK Thread dumo:分析性能瓶颈(线程信息)得到Thread dump: 在linux:kill -3 PID在windows下:Fn + B 或Ctrl + Break 配置IPip config：查看IP状态更改ip配置：修改/etc/sysconfig/network-scripts下的ifcfg-eth*]]></content>
      <categories>
        <category>other</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[大数据模块]]></title>
    <url>%2F2018%2F09%2F04%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[离线计算 Hadoop模块 1.数据存储：HDFS（Hadoop Distributed File System）2.数据计算：MapReduce（java程序、实现离线计算）：在Hadoop 2.X后，Yarn容器中3.Hive：基于HDFS之上的数据仓库，支持SQL语句4.HBase：基于HDFS之上的NoSQL数据库5.ZooKeeper：实现HA（High Availability高可用性、秒杀系统）的功能6.其他：Sqoop、Flume、Pig 实时计算 1.Redis内存NoSQL数据库 Redis Cluster：分布式解决方案2.Apache Storm：进行试试计算（流式计算） Spark：只有数据计算，没有数据的存储（依赖HDFS） 1.Scala变成语言：多范式的编程语言（支持多方式编程：1、面向对象 2、函数式编程）2.Spark Core：内核，相当于MapReduce； 最重要的概念：RDD（弹性分布式数据集）3.Spark SQL：类似Hive、支持SQL4.Spark Streaming：处理流式计算的模块，类似Storm]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis初识]]></title>
    <url>%2F2018%2F08%2F29%2Fredis%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Redis 高性能Key-Value服务器 多种数据结构 丰富的功能 高可用分布式支持 Redis初识 由Salvatore Sanfilippo（antirez）制作，目前服务于以色列RedisLabs，早期代码23000行，采用key-value的字典结构，GitHub、twitter、StackOverflow、阿里巴巴、百度、微博、美团、搜狐等都在使用Redis这样的技术，如今Redis已经成为必备技能。 典型使用场景 缓存系统：用户访问App server，App Server从cache（Redis）请求数据，如果有，直接返回给App Server，如果没有，cache将从Storage（持久化存储空间）中查找，查找后将从Storage中查找到的数据存入cache中以方便下次查找，而后直接由Storage返回给App Server。 计数器：微博、视频网站的转发、评论数。 消息队列系统 排行榜 社交网络 实时系统 开源基于键值的存储服务系统支持多种数据结构性能高，功能丰富 Redis特性 速度快（10W OPS(读写)） 将数据存在内存 用c语言编写 线性模型使用单线程 持久化（断电不丢数据） Redis所有数据保持在内存中，对数据的更新将异步地保存到硬盘上 多种数据结构 常规 字符串（Strings/Blobs/Bitmaps） 哈希（Hash Tables(objects!)） 列表（Linked Lists） 集合（Sets） 有序集合（Sorted Sets） 衍生 位图（BitMaps） 超小内存唯一值技术（HyperLogLog——有一定误差） 地理信息定位（GEO） 支持多种编辑语言 Java php Python Ruby Lua NodeJs 功能丰富 发布订阅 Lua脚本 事务 pipeline 简单 23000行代码 不依赖外部库（like libevent） 单线程模型（开发相对容易） 主从复制 在Redis中主服务器的数据可以同步到从服务器上，为高可用以及分布式提供一个很好的基础 高可用、分布式 高可用 ——&gt; Redis-Sentinel(v2.8)支持高可用分布式 ——&gt; Redis-Cluster(V3.0)支持分布式]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[项目笔记(ssh)]]></title>
    <url>%2F2018%2F08%2F23%2F%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0(ssh)%2F</url>
    <content type="text"><![CDATA[1.Dao层抽取1234(1)BaseDaoImpl&lt;T&gt;--&gt;BaseDao&lt;T&gt; UserDaoImpl&lt;User&gt;--&gt;UserDao&lt;User&gt;(2)UserDao&lt;User&gt; extends BaseDao&lt;T&gt; UserDaoImpl&lt;User&gt; extends BaseDaoImpl&lt;T&gt; 2.Action层抽取12(1)BaseAction&lt;T&gt; extends ActionSupport implements ModelDriven&lt;T&gt;(2)UserAction&lt;User&gt; extends BaseAction&lt;T&gt; 3.通过反射创建对象1234567891011//在构造方法中动态获取实体类，通过反射创建model对象//泛型指定user时，this为user，通过反射创建user对象赋值给model public BaseAction() throws InstantiationException, IllegalAccessException &#123; //获取父类class属性 ParameterizedType genericsSuperclass = (ParameterizedType) this.getClass().getGenericSuperclass(); //获取泛型数组 Type[] actualTypeArguments = genericsSuperclass.getActualTypeArguments(); Class&lt;T&gt; entityClass = (Class&lt;T&gt;)actualTypeArguments[0]; //通过反射创建对象 model = entityClass.newInstance(); &#125;]]></content>
      <categories>
        <category>SSH</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hibernate项目笔记(ssh)]]></title>
    <url>%2F2018%2F08%2F22%2Fhibernate%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0(ssh)%2F</url>
    <content type="text"><![CDATA[1.与spring整合中hibernate连接池的两种方式（web/applicationContext.xml）1234567方式1：&lt;bean id="dataSource" class="com.mchange.v2.c3p0.ComboPooledDataSource"&gt; &lt;property name="driverClass" value="com.mysql.jdbc.Driver"/&gt; &lt;property name="jdbcUrl" value="jdbc:mysql////bos"/&gt; &lt;property name="user" value="root"/&gt; &lt;property name="password" value="tiger"/&gt;&lt;/bean&gt; 123456789101112131415方式2：&lt;bean id="dataSource" class="org.springframework.jdbc.datasource.DriverManagerDataSource"&gt; &lt;property name="driverClassName"&gt; &lt;value&gt;$&#123;database.driverClass&#125;&lt;/value&gt; &lt;/property&gt; &lt;property name="url"&gt; &lt;value&gt;$&#123;database.jdbcUrl&#125;?characterEncoding=UTF-8&amp;amp;useSSL=false&lt;/value&gt; &lt;/property&gt; &lt;property name="username"&gt; &lt;value&gt;$&#123;database.user&#125;&lt;/value&gt; &lt;/property&gt; &lt;property name="password"&gt; &lt;value&gt;$&#123;database.password&#125;&lt;/value&gt; &lt;/property&gt; &lt;/bean&gt; 2.getHibernateTemplate()的使用方法123(1)导入org.springframework.orm.hibernate5.support.HibernateDaoSupport包(2)继承HibernateDaoSupport类 * this.getHibernateTemplate().get(entityClass,id);]]></content>
      <categories>
        <category>SSH</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[maven整合框架笔记(ssh)]]></title>
    <url>%2F2018%2F08%2F22%2Fmaven%E6%95%B4%E5%90%88%E6%A1%86%E6%9E%B6%E7%AC%94%E8%AE%B0(ssh)%2F</url>
    <content type="text"><![CDATA[1.配置阿里远程仓库（parent/pom）1234567891011121314&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;maven - ali&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 2.maven中spring与jdk 1.8的兼容问题（parent/pom）123456789&lt;!-- 属性定义指定jar版本 --&gt;&lt;properties&gt; &lt;spring.version&gt;5.0.5.RELEASE&lt;/spring.version&gt; &lt;hibernate.version&gt;5.2.17.Final&lt;/hibernate.version&gt; &lt;struts2.version&gt;2.3.24&lt;/struts2.version&gt; &lt;slf4j.version&gt;1.6.6&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.12&lt;/log4j.version&gt; &lt;shiro.version&gt;1.2.3&lt;/shiro.version&gt;&lt;/properties&gt;]]></content>
      <categories>
        <category>SSH</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[git安装与简单操作(hexo)]]></title>
    <url>%2F2018%2F08%2F22%2Fgit%E5%AE%89%E8%A3%85%E4%B8%8E%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.安装Node.js和配置好Node.js环境 2.安装Git和配置好Git环境 3.Github账户注册和新建项目，项目必须要遵守格式：账户名.github.io 在建好的项目右侧有个settings按钮，点击它，向下拉到GitHub Pages，你会看到那边有个网址，访问它，你将会惊奇的发现该项目已经被部署到网络上，能够通过外网来访问它 4.安装Hexo 创建文件夹通过命令行进入到该文件夹安装Hexo 1npm install hexo -g 5.初始化该文件夹 1hexo init 6.安装所需要的组件 1npm install 7.联系hexo与github page 终端输入： 1$ git --global user.name "XXX" 终端输入： 1$ git --global user.email "XXX@XXX.XXX" 添加密匙到ssh-agent终端输入： 1$ eval "$(ssh-agent -s)" 添加生成的ssh key到ssh-agent终端输入 1$ eval "$(ssh-agent -s)" github–&gt;setting–&gt;ssh and gpg keys添加id_rsa.pub文件中的ssh key 8.配置Deployment(_config.yml) 1234deploy: type: git repository: git@github.com:zfhzxg/zfhzxg.github.io.git branch: master 基础命令 终端：检测ssh是否配置成功 1$ ssh -T git@github.com 生成ssh密匙： 1$ ssh-keygen -t rsa -C "XXX@XXX.XXX" 生成.get： 1$ get init cmd：检测node.js是否安装成功： 12node -vnmp -v 检查hexo是否安装成功： 1hexo -v 开启本地服务器： 1hexo g 改变端口号： 1hexo server -p XXXX 新建博客： 1hexo new post "博客名" 安装拓展： 1npm install hexo-deployer-git --save 生成部署： 1hexo d -g]]></content>
      <categories>
        <category>other</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[git报错]]></title>
    <url>%2F2018%2F08%2F22%2Fgit%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[安装主题报错 报错内容： 1curl 18 transfer closed with outstanding read data remaining 解决方法： 1git config --global http.postBuffer 524288000 curl 18 transfer closed with outstanding read data remaining 解决方法： 1git config --global http.postBuffer 524288000]]></content>
      <categories>
        <category>other</category>
      </categories>
  </entry>
</search>
