<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[spring]]></title>
    <url>%2F2018%2F11%2F14%2Fspring%2F</url>
    <content type="text"><![CDATA[Spring概述 是一个开源框架，于2003年兴起的一个java轻量级开发框架，是一个分层的java se/ee开发的一站式开源框架 优点：轻量，控制反转（IOC），面向切面，容器，框架，MVC Spring三层： WEB层：SpringMVCService层：Spring的Bean管理，Spring声明式事务Dao层：Spring的JDBC模板，Spring的ORM模块 Spring优点 方便解耦合 AOP的开发：对程序进行扩展 轻量级框架 方便与其他框架整合 Spring包 Spring-framework-4.X.XRELEASE-dependencies.zip：Spring依赖库 Spring-framework-4.X.XRELEASE-dist.zip：核心包 Spring-framework-4.X.XRELEASE-dist/docs：开发规范和APISpring-framework-4.X.XRELEASE-dist/libs：核心jar包和源码Spring-framework-4.X.XRELEASE-dist/schema：配置文件的约束 Spring-framework-4.X.XRELEASE-docs.zip：文档 Spring-framework-4.X.XRELEASE-schema.zip：约束 SpringIOC的XML开发(IOC：Inversion of Control（控制反转）)包、依赖 通过xml文件，将对象的创建权反转给（交给）Spring 相关包文件： 123456spring-beans-4.2.4.RELEASE.jar（实体类包）spring-context-4.2.4.RELEASE.jar（核心扩展包）spring-core-4.2.4.RELEASE.jar（核心包）spring-expression-4.2.4.RELEASE.jar（Spring el表达式包）com.springsource.org.apache.commons.logging-1.1.1.jar（日志接口）com.springsource.org,apache.log4j-1.2.15.jar（log4j日志记录工具包） 通过BeanFactory（工厂）切换底层实现类，消除哦耦合 BeanFactory对Xml进行解析，通过xml中的id找到其实现类进行反射，返回实例 12XMl：&lt;bean i’d=“UserDao” class=“Use3rDaoImpl”&gt;&lt;/bean&gt; 引入约束 12spring-framework-4.2.4.RELEASE-dist\spring-framework-4.2.4.RELEASE\docs\spring-framework-reference\html\xsd-configuration.html：beans schema IOC和DI DI：依赖注入，前提是必须有IOC的环境，Spring管理这个类的时候将类的依赖注入（设置property属性） Spring的工厂类BeanFactory（老版本工厂类） BeanFactory：调用个体Bean的时候，才会生成类的实例ApplicationContext（新版本工厂类） ApplicationContext底层继承了BeanFactory ApplicationContext：加载配置文件的时候，就会将Spring管理的类都实例化 ApplicationContext有两个实现类 ClassPathXmlApplicationContext：加载类路径下的配置文件（src路径下）FileSystemXmlApplicationContext：加载文件系统下的配置文件（磁盘路径下） Spring Bean管理id、name id ：使用了约束中的唯一约束，不能出现特殊字符 name ：没有使用约束中的唯一约束，可以出现特殊字符（理论上可以出现重复，但实际开发不能出现） Spring和struts1框架整合的时候（struts1中action注入id名必须加上斜杠） 1&lt;bean id=&quot;/user&quot; class=&apos;&apos;&gt; class：需要生成的实例类的全路径 Bean生命周期的配置 init-method ：Bean被初始化时执行的方法 destroy-method ：Bean被销毁时执行的方法，需要从实体类中调用（Bean作为单例模式创建时）Bean作用范围的配置 scope ：Bean的作用范围 singleton ：默认的，Spring会采用单例模式创建这个对象prototype ：多例模式（struts2与Spring整合时，action采用多例模式）request ：应用在web项目中，Spring创建这个类以后，将这个类存入到request范围中session ：用用在web项目中，Spring创建这个类以后，将这个类存入到session范围中globalsession（全局session） ：应用到web项目中，必须在porlet环境下使用，如果没有这种环境，globalsession相当于session Spring属性注入（DI） 三种注入方式 构造方法方式123456public class user&#123; private String name; public user(String name)&#123; this.name = name; &#125;&#125; set方法方式123456public class user&#123; private String name; public void setName(String name)&#123; this.name = name; &#125;&#125; 接口注入方式123456789public interface user&#123; public void setName(String name);&#125;public class userImpl implements user&#123; private String name; public void setName(String name)&#123; this.name = name; &#125;&#125; Spring支持构造方法注入和set方法注入 构造方法属性注入123456789101112&lt;!-- 构造属性注入的方式 --&gt; &lt;bean id=&quot;car&quot; class=&quot;com.zfhzxg.spring.demo3.Car&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;zfhzxg&quot;/&gt; &lt;constructor-arg name=&quot;p&quot; value=&quot;BMW&quot;/&gt; &lt;/bean&gt; &lt;!-- 构造对象属性注入的方式 --&gt;&lt;!-- value用来设置普通属性值，ref用来设置其他的类的id和name --&gt; &lt;bean id=&quot;emp&quot; class=&quot;com.zfhzxg.spring.demo3.Emp&quot;&gt; &lt;constructor-arg name=&quot;name&quot; value=&quot;zfhzxg&quot;/&gt; &lt;constructor-arg name=&quot;car2&quot; ref=&quot;car2&quot;/&gt; &lt;/bean&gt; set方法属性注入123456789101112&lt;!-- set属性注入的方式 --&gt; &lt;bean id=&quot;car2&quot; class=&quot;com.zfhzxg.spring.demo3.Car2&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;zhxzgd&quot;/&gt; &lt;property name=&quot;p&quot; value=&quot;BMW&quot;/&gt; &lt;/bean&gt; &lt;!-- set对象属性注入的方式 --&gt; &lt;!-- value用来设置普通属性值，ref用来设置其他的类的id和name --&gt; &lt;bean id=&quot;emp&quot; class=&quot;com.zfhzxg.spring.demo3.Emp&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;zhxzgd&quot;/&gt; &lt;property name=&quot;car2&quot; ref=&quot;car2&quot;/&gt; &lt;/bean&gt; P名称空间属性注入（通过引入P名称空间来完成该属性的注入，Spring2.5之后）123456xmlns:p=&quot;http://www.springframework.org/schema/p&quot;&lt;!-- 不能输用构造函数--&gt;&lt;!-- 普通属性 P:属性名=&quot;值&quot;; --&gt;&lt;!-- 对象属性 P:属性名-ref=&quot;值&quot;; --&gt;&lt;!-- p名称空间注入方式 --&gt; &lt;bean id=&quot;car2&quot; class=&quot;com.zfhzxg.spring.demo3.Car2&quot; p:name=&quot;zfhzxg&quot; p:p=&quot;BMW&quot;&gt;&lt;/bean&gt; SpEL的属性注入（Spring Expression Language，Spring3.0之后）123456789101112语法： #&#123;SpEL&#125;//可以进行逻辑运算，调用对象和方法的值&lt;!-- SpEL的属性注入方式 --&gt; &lt;bean id=&quot;carInfo&quot; class=&quot;com.zfhzxg.spring.demo3.CarInfo&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;car2&quot; class=&quot;com.zfhzxg.spring.demo3.Car2&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;#&#123;carInfo.name&#125;&quot;/&gt; &lt;property name=&quot;p&quot; value=&quot;#&#123;carInfo.calculaorPrice()&#125;&quot;/&gt; &lt;/bean&gt; &lt;bean id=&quot;emp&quot; class=&quot;com.zfhzxg.spring.demo3.Emp&quot;&gt; &lt;property name=&quot;name&quot; value=&quot;#&#123;&apos;zhxzgd&apos;&#125;&quot;/&gt; &lt;property name=&quot;car2&quot; value=&quot;#&#123;car2&#125;&quot;/&gt; &lt;/bean&gt; 集合类型属性注入1234567891011121314151617181920212223242526272829303132&lt;!-- Spring集合属性注入 --&gt; &lt;bean name=&quot;collectionBean&quot; class=&quot;com.zfhzxg.spring.demo4.CollectionBean&quot;&gt; &lt;!-- 注入数组类型和list一样 --&gt; &lt;property name=&quot;arrs&quot; &gt; &lt;list&gt; &lt;!-- 对象用ref --&gt; &lt;value&gt;zfhzxg&lt;/value&gt; &lt;value&gt;zhxzgd&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;list&quot; &gt; &lt;list&gt; &lt;!-- 对象用ref --&gt; &lt;value&gt;123&lt;/value&gt; &lt;value&gt;321&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;property name=&quot;set&quot; &gt; &lt;list&gt; &lt;!-- 对象用ref --&gt; &lt;value&gt;abc&lt;/value&gt; &lt;value&gt;cba&lt;/value&gt; &lt;/list&gt; &lt;/property&gt; &lt;!-- 此处面向对象使用value-ref --&gt; &lt;property name=&quot;map&quot; &gt; &lt;map&gt; &lt;entry key=&quot;aaa&quot; value=&quot;111&quot;&gt;&lt;/entry&gt; &lt;entry key=&quot;bbb&quot; value=&quot;222&quot;&gt;&lt;/entry&gt; &lt;/map&gt; &lt;/property&gt; &lt;/bean&gt; 分模块开发配置12345一、直接加载多文件： ApplicationContext applicationContext = new ClassPathXmlApplicationContext(&quot;applicationContext1.xml&quot;,&quot;applicationContext2.xml&quot;);二、在applicationContext1.xml中引入其他配置文件： &lt;import resource=&quot;applicationContext2.xml&quot;/&gt; &lt;import resource=&quot;applicationContext3.xml&quot;/&gt; SpringIOC的注解 在Spring4的版本中，除了需要引入基本的四个开发包意外，还需要引入aop的包 Spring的web项目资源浪费（与struts2整合） web每次请求都会创建Spring的工厂，浪费服务器资源 解决： 在服务器启动的时候创建Spring的工厂将工厂保存到ServletContext中每次获取工厂都从ServletContext中获取 整合： 引入spring_web.jar配置ContextLoaderListener 1234567891011121314151617&lt;!-- 配置Spring的核心监听器 --&gt;&lt;!-- ContextLoaderListener底层实现ServletContextListener --&gt; &lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt; &lt;/listener&gt;&lt;!-- 默认加载的是/WEB-INF/applicationContext.xml --&gt;&lt;!-- 修改加载配置文件的路径 --&gt; &lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt; &lt;/context-param&gt;获取工厂：public String getApplicationContext() &#123; ServletContext serviceContext = ServletActionContext.getServletContext(); WebApplicationContext applicationContext = WebApplicationContextUtils.getWebApplicationContext(serviceContext); CustomerService customerService = (CustomerService)applicationContext.getBean(&quot;customerService&quot;);&#125;]]></content>
      <categories>
        <category>SSM</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[yarn]]></title>
    <url>%2F2018%2F10%2F19%2Fyarn%2F</url>
    <content type="text"><![CDATA[RecourceManager 处理客户端的请求 监控NodeManager 启动或者监控程序 资源的分配和调度 NodeManager RecourceManager将会给NodeManager分配container（资源）和AppMstr（任务）来处理数据 管理单个节点的资源 处理来自RecourceManager的命令 处理程序的命令]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[eclipse的Hadoop环境配置]]></title>
    <url>%2F2018%2F10%2F12%2Feclipse%E7%9A%84Hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-2%2F</url>
    <content type="text"><![CDATA[设置系统环境变量12HADOOP_HOME：hadoop-2.7.3/PATH：%HADOOP_HOME%/bin eclipse进行导包 包路径：hadoop-2.7.3/share/]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HDFS]]></title>
    <url>%2F2018%2F10%2F01%2FHDFS%2F</url>
    <content type="text"><![CDATA[HDFS的体系架构NameNode：名称节点 是HDFS的主节点、管理员 接收客户端（命令行、java程序）的请求：创建目录、上传数据、下载数据、删除数据等 管理和维护HDFS的日志和元信息 日志文件（edits文件）：记录的是客户端的所有操作，是一个二进制文件（JSON） 位置：/root/training/hadoop/tmp/dfs/name/currentedit_inprogress_00000000000000XXXXX：正在操作的日志文件hdfs oev -i edits_inprogress_00000000000000XXXXX -o ~/a.xml：通过日志查看器（edits viewer），把edits文件转换成文本（xml）格式元信息（fsimage文件）：记录的是数据块的位置信息，数据块的冗余信息，是一个二进制文件位置：/root/training/hadoop/tmp/dfs/name/currentfsimage_0000000000000XXXXX：元信息记录文件hdfs oiv -i fsimage_000000000000XXXXX -o ~/b.xml：将元信息记录文件转换成文本（xml或txt）格式 HDFS元信息保存位置配置12345hdfs-site.xml:&lt;property&gt; &lt;name&gt;dfs.name.dir&lt;/name&gt; &lt;value&gt;储存位置&lt;/value&gt;&lt;/property&gt; DataNode：数据节点 按照数据块保存数据 1.X : 64M2.X : 128M 数据块：表现形式就是一个文件（blk打头） 位置：/root/training/hadoop-2.7.3/tmp/dfs/data/current/BP-XXX-数据节点-XXX/current/finalized/subdir0/subdir0/一个数据块对应的是一对文件，‘.meta’记录的是数据块的元信息设置数据块冗余度规则：一般跟数据节点个数相同，但最大不要超过3Hadoop 3.X之前，会造成存储空间极大的浪费Hadoop 3.X之后，采用HDFS纠删码技术，使得存储空间节约一半 HDFS文件系统的保存位置配置12345hdfs-site.xml:&lt;property&gt; &lt;name&gt;dfs.data.dir&lt;/name&gt; &lt;value&gt;储存位置&lt;/value&gt;&lt;/property&gt; SecondaryNameNode：第二名称节点 职责：进行日志信息的合并 SecondaryNameNode向NameNode下载edits日志文件和fsimage元信息文件将edits中最新的信息写入fsimage文件将合并后的文件上传给NameNode当上次合并发生以后，用户进行新的操作，NameNode将产生新的edits_inprogress当HDFS发出检查点（checkpoint）的时候，会进行日志信息合并默认情况下，HDFS每隔60分钟或edits文件达到了64M产生一个检查点 由于edits文件记录了最新的状态信息，并且随着操作越多，edits就会越大 把edits中的最新信息写到fsimage中 edits文件就可以清空 配置SecondaryNameNode节点位置12345hdfs-site.xml:&lt;property&gt; &lt;name&gt;dis.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;RedHat112:50090&lt;/value&gt;&lt;/property&gt; HDFS基础命令 hdfs dfs -help ：查看帮助 hdfs dfs -ls / ：列出/目录下的文件和目录 hdfs dfs -put /本地路径 /上传路径 ：上传文件 hdfs dfs -moveFromLocal /本地路径 /上传路径 ：剪切上传文件 hdfs dfs -get /hdfs路径 /本地路径 ：下载文件 hdfs dfs -getmerge /hdfs目录（是一个文件夹） /合并后的文件 ：合并下载 hdfs dfs -mkdir /目录名 ：创建目录 hdfs dfs -mkdir -p /目录名/目录名 ：创建多级目录 hdfs dfs -mv /需要移动的目录（文件） /需要移动到的位置 ：移动文件/文件夹 hdfs dfs -copy /需要复制的目录（文件） /需要复制到的位置 ：复制文件/文件夹 hdfs dfs -rm /文件路径 ：删除文件 hdfs dfs -rm -r /目录路径 ：删除目录 hdfs dfs -cat /文件路径 ：查看文件 hdfs dfs -tail -f /文件路径 ：查看文件的最后指定行 hdfs dfs -count /目录路径（文件路径） ：查看目录（文件）文件夹数，文件数、大小 hdfs dfs -df -h / ：查看hdfs的总空间 hdfs dfs -setrep 冗余度 /文件路径 ：设置单个文件冗余度 简单的HDFS上传文件代码1234567891011121314151617181920212223242526272829303132package hdfs.demo1;import java.io.IOException;import java.net.URI;import java.net.URISyntaxException;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;public class HDFSClient &#123; public static void main(String[] args) throws IOException, InterruptedException, URISyntaxException &#123; //客户端加载配置文件 Configuration conf = new Configuration(); //指定配置（设置冗余度） conf.set(&quot;dfs.replication&quot;,&quot;2&quot;); //指定块大小 conf.set(&quot;dfs.blocksize&quot;, &quot;64m&quot;); //构造客户端 FileSystem fS = FileSystem.get(new URI(&quot;hdfs://192.168.0.102:9000/&quot;),conf,&quot;root&quot;); //上传文件 fS.copyFromLocalFile(new Path(&quot;D:/123.txt&quot;), new Path(&quot;/work.txt&quot;)); //关闭资源 fS.close(); &#125;&#125; HDFS写数据流程 请求N阿么N哦的上传文件123.txt（客户端——》NameNode） 响应可以上传的文件（NameNode——》客户端） 请求上传第一个Block（块文件）（0～120m），请求分会DataNode（客户端——》NameNode） 返回DataNode1，DataNode2，DataNode3，表示采用这两个酒店储存具体的数据（NameNode——》客户端） 请求建立一个Block传输通道（客户端——》DataNode1——》dataNode2——》dataNode3） DataNode1，DataNode2，DataNode3应答成功（DataNode——》客户端） 传输数据（客户端——》DataNode1——》dataNode2——》dataNode3） HDFS读数据流程 请求下载文件（客户端——》NameNode） 返回目标文件的元数据（NameNode——》客户端） 通过元信息请求第一块数据（客户端——》DataNode） 传输数据给客户端（DtatNode——》客户端） 继续通过元信息请求第二块数据。。。 NameNode与SecondaryNameNode工作机制 启动集群，加载edits（编辑日志）与fsimage（镜像文件） 元数据增删查改（客户端——》NameNode） SecondaryNameNode请求是否需要CheckPoint（SecondaryNameNode——》NameNode） SecondaryNameNode请求执行CheckPoint（SercondaryNameNode——》NameNode） CheckPoint触发条件：1.通过定时；2.通过edits操作记录数量 CheckPoint触发时，SecondaryNameNode向NameNode拷贝edits文件并加载到内存（SercondaryNameNode——》NameNode） 生成新的镜像文件fsimage.chkpoint 将fsimage.chkpoint拷贝到NameNode（SercondaryNameNode——》NameNode） NameNode将fsimage.chkpoint重命名为fsimage，再次发送到SecondaryNameNode当中（SercondaryNameNode——》NameNode） NodeName与SencondaryNameNode将新的fsimage加载到内存中 HDFS重点 一、工作机制 二、读写流程 三、客户端api 四、集群部署安装]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[主从结构的单点故障]]></title>
    <url>%2F2018%2F09%2F28%2F%E4%B8%BB%E4%BB%8E%E7%BB%93%E6%9E%84%E7%9A%84%E5%8D%95%E7%82%B9%E6%95%85%E9%9A%9C%2F</url>
    <content type="text"><![CDATA[主从结构的主出现故障时视为单点故障HDFS NameNode（主）+DataNode（从） Yarn ResourceManager（主）+NodeManager（从） HBase HMaster（主）+RefionServer（从） Storm nimbus（主）+supervisor Spark Master（主）+Worker（从） #单点故障的解决方法（HA） 使用zookeeper实现HA功能，当主节点（active）出现故障时，通过主节点（standby）操作HDFS]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hive]]></title>
    <url>%2F2018%2F09%2F26%2Fhive%2F</url>
    <content type="text"><![CDATA[数据分析引擎一、Hadoop中 （1）Hive：支持SQL （2）Pig：支持PigLation二、Spark中__ （*）Spark SQL：类似Hive，支持SQL、DSL三、另一个：Impala 什么是Hive一、Hive是基于HDFS之上的一个数据仓库，有助于使用SQL Hive —-&gt; HDFS表 ——&gt; 目录数据 —-&gt; 文件分区 —-&gt; 目录桶 ——&gt; 文件 二、Hive是基于Hadoop之上的一个数据分析引擎 Hive 2.X 以前：SQL —-&gt; Hive —-&gt; MapReduceHive 2.X 以后：推荐使用Spark作为SQL的执行引擎（只针对Hadoop 3.X以前）（《Hive on Spark文档》） Hive的体系架构 一、CLI（命令行）：直接由Hive Dirver翻译二、JDBC（标准接口）：1.X由Thrift Server，2.X由Hive Server翻译为SQL语句交由Hive Dirver，端口号都为10000三、HWI（Hive Web Interface）：只在Hive 2.2前提供HWI网页工具，推荐使用HUE，由Hive Dirver翻译*、在Hive的体系架构中还需要有关系型数据库用来存储Hive元信息（推荐使用MySQL） Hive优缺点 优点 1）操作接口采用了sql，简化开发，减少学习成本2）避免手写mapreduce程序3）hive执行延迟较高，适用场景大多用在实时性要求不强的场景4）处理大数据有优势5）支持自定义函数 缺点 1）hive的sql表达能力有限（hql），并不能解决所有大数据场景2）hive效率低（自动生成mapreduce作业，但力度比较粗，调优困难）3） 安装和配置Hive准备工作： 12345678910111213141516171819202122232425262728293031321、解压 tar -zxvf apache-hive-2.3.0-bin.tar.gz -C ~/training/2、设置环境变量 vi ~/.bash_profile HIVE_HOME=/root/training/hive export HIVE_HOME PATH=$HIVE_HOME/bin:$PATH export PATH3.安装配置MySQL数据库 在虚拟机上安装MySQL： yum remove mysql-libs rpm -ivh mysql-community-common-5.7.19-1.el7.x86_64.rpm rpm -ivh mysql-community-libs-5.7.19-1.el7.x86_64.rpm rpm -ivh mysql-community-client-5.7.19-1.el7.x86_64.rpm rpm -ivh mysql-community-server-5.7.19-1.el7.x86_64.rpm rpm -ivh mysql-community-devel-5.7.19-1.el7.x86_64.rpm （可选，但还是装上，后面装HUE的时候会用到。） 启动MySQL：service mysqld start 或者：systemctl start mysqld.service 查看root用户的密码：cat /var/log/mysqld.log | grep password 登录后修改密码：alter user &apos;root&apos;@&apos;localhost&apos; identified by &apos;Welcome_1&apos;; MySQL数据库的配置： 创建一个新的数据库：create database hive; 创建一个新的用户： create user &apos;hiveowner&apos;@&apos;%&apos; identified by &apos;Welcome_1&apos;; 给该用户授权 grant all on hive.* TO &apos;hiveowner&apos;@&apos;%&apos;; grant all on hive.* TO &apos;hiveowner&apos;@&apos;localhost&apos; identified by &apos;Welcome_1&apos;; 免费工具：http://www.mysqlfront.de/ 嵌入模式：不需要MySQL，使用Hive自带的Derby数据库存储Hive的元信息 12345678910111213141516171819202122232425262728293031323334353637383940414243特点： （1）使用自带的Derby （2）只支持一个连接 （3）用于开发和测试修改配置文件hive-env.sh： HADOOP_HOME=path export HIVE_CONF_DIR=/root/hd/hive/conf在HDFS集群上创建文件夹： hdfs dfs -mkdir /tmp hdfs dfs -mkdir /user/hive/warehouse/创建hive-site.xml： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:derby:;databaseName=metastore_db;create=true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;org.apache.derby.jdbc.EmbeddedDriver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.local&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;file:///root/training/apache-hive-2.3.0-bin/warehouse&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 初始化MetaStore： schematool -dbType derby -initSchema 日志： Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. 本地模式、远程模式：都需要MySQL 1234567891011121314151617181920212223242526272829303132333435363738修改配置文件hive-env.sh： HADOOP_HOME=path export HIVE_CONF_DIR=/root/hd/hive/conf在HDFS集群上创建文件夹： hdfs dfs -mkdir /tmp hdfs dfs -mkdir /user/hive/warehouse/创建hive-site.xml： &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt; &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt; &lt;configuration&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive?useSSL=false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hiveowner&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;tiger&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt;初始化MetaStore： schematool -dbType mysql -initSchema * 启动hive前先启动hadoop集群和yarn Hive的架构 提供了一系列接口：hive shell、jdbc/odbc、webui hive架构：hive（Meta元数据（默认derby数据库，可以自定义）、Client（cli、idbc）、SQL Parser解析器、Physica编译器、Query优化器、Execution执行器、MR程序） 客户端输入sql后，首先调用到元数据 通过SQL Parser解析器找到对应的MR程序 经过编译器编译代码 通过优化其选择SQL需不需要经过计算 通过执行器执行MR程序 *Hive的表类型和数据类型表类型 一、内部表（管理表）：类似MySQL、Oracle中的表 删除管理表时，hive会自动删除管理表的数据，不擅长做数据共享 二、外部表 删除外部表时，hive不认为这张表拥有这份数据，并不会删除数据，适合做数据共享外部表已被删除时，重新创建一个表明、表结构都相同的表，hive将自动关联到外部表留下的数据 三、分区表：提高性能 （*）补充：如何提高性能？（SQL执行计划） 四、桶表：类似Hash分区 五、视图：View数据类型| Java数据类型 | hive数据类型 | 数据长度 || byte | TINYINT | 1byte || short | SMALINT | 2byte || int | INT | 4byte || long | BIGINT | 8byte || float | FLOAT | 单精度浮点数 || double | DOUBLE | 双精度浮点数 || String | STRING | 字符 || | TIMESTAMP | 时间类型 || | BINARY | 字节数组 | 基础HQLDBL数据定义 库操作 alter database hive_db set dbproperties(‘dataname’=’zfhzxg的数据库’);：添加数据库描述信息desc database hive_db;：查看数据库结构desc database extended hive_db;：查看数据库拓展性drop database if exists hive_db;：检测该库存在时，删除该库 管理表操作 show databases [link ‘db*’];：显示数据库，可选通配符筛选create database hive_db location ‘path’;：在指定路径下创建数据库create database if not exists hive_db;：检测该表不存在时，创建该表desc formatted emp;：查看表类型 外部表操作 create table emp(id int,name string) row format[按行格式化] delimited fields[根据字段] terminated by “\t”[按空格切分];：创建表（默认创建管理表）create external table if not exists emp2(id int,name string) row format delimited fields terminated by “\t”;：创建外部表create table if not exists emp2 as select * from emp where name=”zfhzxg”;：emp2存在时，将emp中name为zfhzxg的字段传输到emp2中 分区表操作 create table if not exists emp_partition(id int,name string) partitioned by (day string) row format delimited fields terminated by “\t”;：创建分区表，以day为分区select * from emp_partition where day=’1112’;：查询day分区为1112的数据alter table emp_partition add partition(day=’1113’);：向emp_partition表中添加1113分区alter table emp_partition drop partition(day=’1112’);：删除1112分区 修改表 alter table emptable rename to empt;：修改表名alter table emp_partition add clumns(desc string);：向表中添加desc字段alter table emp_partition change column desc descs int;：修改表中的字段alter table emp_partition replace colums(name string,descs int);：替换表中的字段（重新定义，全部替换） DML数据操作 加载/导入、插入 load data local inpath ‘path’ into table hive_db;：导入本地数据（追加）load data inpath ‘path’ into table default.emp;：向default库中的emp表导入hdfs中的数据（追加，如果加载hdfs中的数据，源文件将会被剪切）load data inpath ‘path’ overwrite into table default.emp;：导入hdfs中的数据（覆盖）load data local inpath ‘path’ into table emp_partition partition(day=’1112’);：向1113分区中导入本地数据（追加）insert into table emp_partition partition(day=’1112’) values(1,’zfhzxg’);：向分区表中的1112分区插入数据create table if not exists emptable as select from emp_partition where day=’1112’;：新创建一张表，并导入emp_partition表中1112分区中的数据create table if not exists empta(id int,name string) row format delimited fields terminated by ‘\t’ location ‘path’;：创建表是关联数据，并不会剪切元数据insert overwrite local directory ‘path’ select from emp_partition where day=’1113’;：向本地路径（文件夹）导出empt_partition表中1113分区中的数据dfs _get ‘path1’ ‘path2’;：在hive终端可以输入hdfs命令，可以用hadoop命令进行导出[root@RedHat112 hive]# bin/hive -e “select from emp_partition where day=’1113’” &gt; /root/1113.txt ：将hive中表的数据导出成文件，用于结果导出，如果sql语句中包含单引号，需要用双引号包含sql语句 查询 基础查询 seltct * from emp;：全表查询select emp.id,emp.name from emp;：查询指定字段select emp.id [as] id,emp.name [as] name from emp;：自定义字段名 算数运算符 +（相加）-（相减）*（相乘）/（相除）%（取余）&amp;（按位取与）|（按位取或）^（异或）~（按位取反，只能用于四种整数类型） 函数 count（求行数）max（最大）min（最小）sum（求和）avg（平均值）limit（查看前几条数据）：select * from emp limit 2; where select from emp where id between 1 and 3;：查询id在1到3之间的人select from emp where id in(1,3);：查询id为1和3的人 like：选择类似的值，选择条件可以包含字母和数字 select from emp where name like “_f%”;：查找name第二个字符为f的人select from emp where name rlike “[7]”;：rlike中可以使用正则表达式 group by：分组 select avg(empt.sal) avg_sal,deptno from empt group by deptno;：查询每个部门的工资平均值（按deptno分组）select max(empt.sak) max_sal,deptno from empt group by deptno;：查询每个部门的最高薪水select deptno,avg(sal) avg_sal from emp group by deptno having avg_sal&gt;1700;：查询每个部门的平均薪水大于1700的部门（在分组后添加条件时，用having）]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[HBase开发]]></title>
    <url>%2F2018%2F09%2F21%2FHBase%E5%BC%80%E5%8F%91%2F</url>
    <content type="text"><![CDATA[###NoSQL简介 一、什么是NoSQL数据库？（not only sql） 二、常见NoSQL数据库 HBaseRedis：基于内存的NoSQL数据库，前身MemCached（不支持持久化）MongoDB：基于文档型（BSON）的NoSQL数据库Cassandra：跟HBase类似 ###HBsae的体系架构（主从结构）和表结构 一、Hadoop的生态体系圈 二、HBase 基于HDFS之上的NoSQL数据库 123HBase HDFS表 ---&gt; 目录数据 ---&gt; 文件（HFile，默认大小：128MB） 三、HBase的体系架构（主节点：HMaster + 从节点：RegionServer） 单点故障 通过Zookeeper ###HBase的搭建模式 一、本地模式 二、伪分布模式 三、全分布模式 四、实现HBsae的HA ###操作HBsae 一、]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop免密码登陆原理]]></title>
    <url>%2F2018%2F09%2F14%2FHadoop%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E9%99%86%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[###hadoop免密码登陆原理 *不对称密码 密匙对（两个文件）： 公钥–锁（.ssh/id_rsa）：给别人加密私钥–钥匙（.ssh/id_rsa.pub）：给自己，解密 *对称加密（加密和解密使用同一文件） 一、生成密钥对（请求登陆方-A） 1ssh-keygen -t rsa 二、把请求登陆方生成的公钥拷贝给被请求方（B） 1ssh-copy-id -i .ssh/id_rsa.pub root@RedHat111 三、B收到请求方发来的公钥，并自动保存在.ssh/authorized_keys文件 四、B随机产生一个字符串：hello（前三部是配置，第四步开始，是认证的过程） 五、B使用请求登陆方的公钥进行加密（*），并发回给A 六、A收到B发来的加密字符串，使用自己的私钥进行解密（hello） 七、A把解密后的字符串（hello）发回给B进行认证 八、B收到A解密的字符串（hello） 九、B对从A收到的字符串（hello）与自己生成的字符串（hello）进行对比]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[MapReduce笔记]]></title>
    <url>%2F2018%2F09%2F10%2FMapReduce%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[#MapReduce高级特性 一、序列化 核心接口：Writable 如果一个类实现了Writable该类的对象可以作为Key和Value二、排序 规则：按照Key2排序（可以是基本数据类型，也可以是对象） 基本数据类型：数字（默认升序），字符串（默认字典顺序）可以通过创建自己的比较规则改变排序（extends IntWritable.Comparator/extends Text.Comparator） 对象 SQL排序：order by 列名、表达式、别名、序号 desc/asc（desc/asc只作用于最近的一列）MapReduce排序：1.该对象必须是Key2；2.必须实现序列化接口Writable；3.对象必须是可排序的（自定义排序使用java.long 接口 Comparable） MR排序分类 部分排序全排序辅助排序二次排序 三、分区 什么是分区：partition 查询： 1.没有分区：执行全表扫描2.有分区，只扫描分区 分区的类型： Orcale： 1.范围分区；2.列表分区；3.Hash分区；4.Hash范围分区；5.Hash列表分区MR的分区：默认情况下，MR的输出只有一个分区（一个分区就是一个文件）自定义分区：按照字段进行分区（根据Map的输出&lt;Key2,Value2&gt;分区） *通过SQL的执行计划，判断效率是否提高四、合并 合并是一种特殊的Reduce 合并是在Map端执行一次合并，用于减少Mapper输出到Reduce的数据量，可以提高效率 平均值不能使用combiner 无论有没有combiner，都没不能改变Map和Reduce对应的数据类型*MapReduce核心：Shuffle（洗牌） Hadoop3.X之前会有数据落地（产生I/O操作） map()方法写入数据到环形缓冲区 环形缓冲区达到80%后，发生溢写，进行分区、排序、合并（combiner可选）、归并 归并后，拷贝到内存缓冲 当内存不够时，溢出到磁盘 进行归并排序 相同key分组 传入Reduce()*MapReduce优缺点 优点 1.易于编程2.良好的拓展性3.高容错性4.适合处理PB级别以上的离线处理 缺点 1.不擅长做实时计算2.不擅长做流式计算（MR的数据源事静态的）3.不支持DAG（有向图）计算（Spark） #Mapper，Reduce和Driver Mapper阶段： 用户自定义Mapper类，要继承父类MapperMapper的输入数据的kv对形式（kv类型可以自定义）Mapper的map方法的重写（加入业务逻辑）Mapper的数据输出kv对的形式（kv类型可以自定义）map（）方法（maptext进程）对每个&lt;k,v&gt;只调用一次 Reducer阶段； 用户自定义Reduce类，要继承父类ReducerReducer的数据输入类型对应的是Mapper九段的输出数据类型Reducer的reduce方法重写（加入业务逻辑）ReduceText进程对每组的k的&lt;k,v&gt;组只调用一次 Driver阶段 mr程序需要一个Driver来进行任务的提交，提交的任务是一个描述了各种重要信息的job maptask流程 并行度：一个job任务map阶段并行度由客户端所提交的任务决定每一个split分配一个maptask并行处理默认情况下，split大小=blocksize切片是针对每一个文件单独切片流程：准备数据wordcount创建客户端，提交任务程序driver逻辑运算向环形缓冲区写数据&lt;k,v&gt;（默认大小100M）当环形缓冲区内存占用达到80%，进行溢写（HashPratitioner分区，key.compareTo排序（索引））溢写到文件（保证分区且区内是有序的）merge归并排序 reducetask流程 reduceTask将相同分区的数据下载到reduceTask本地磁盘再次合并文件，归并排序合并过程中进行辅助排序一次读一组，进行写出，生成结果文件 #Hadoop中所提供的数据序列化类型 int &gt; IntWritable float &gt; FloatWritable long &gt; LongWritable double &gt; DoubleWritable String &gt; Text boolean &gt; BooleanWritable byte &gt; ByteWritable Map &gt; MapWritable Arry &gt; ArryWritable 为什么要序列化？存储“活的对象”什么是序列化？就是把内存中的对象，转换成字节序列以便于存储和网络传输反序列化就是将收到的字节序列或者硬盘的持久化数据，转换成内存中的对象Java中的序列化Serializable为什么不用Java提供的序列化接口？Java的序列化是一个重量级的序列化，一个对象被序列化后会附带很多额外的信息（校验信息，Header，继承体系等），不便于在网络中的高效传输，所以Hadoop开发了一套序列化鸡之（Writable），精简/高效为什么序列化在Hadoop中很重要？Hadoop通信是通过远程调用（rpc）实现的，需要进行序列化特点：1）紧凑2）快速3）可拓展4）互操作 #MapReduce常用案例 一、数据去重 相同Key名交给同一地址进行处理，处理后输出给Reduce，Key值唯一，Value形成数组 二、多表查询（笛卡尔集：列数相加，行数相乘） 等值链接的处理过程（以表作为Value1） Mapper阶段，通过分词后的列数或其他方法区分表Mapper阶段，在字段前添加标识输出给ReduceReduce进行处理 三、自连接 同一个表经Mapper输出两次非法数据要先经过处理（数据清理）Reduce进行处理 四、倒排索引 五、单元测试（MRUnit） #MapReduce重点 一、WordCount案例、流量汇总案例与涉及知识点 二、yran集群部署安装 三、job任务提交流程 四、辅助排序、分区排序 五、MapReduce整体流程 六、数据压缩]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 2.X管理与开发（一）]]></title>
    <url>%2F2018%2F09%2F05%2FHadoop-2-X%E7%AE%A1%E7%90%86%E4%B8%8E%E5%BC%80%E5%8F%91%EF%BC%88%E4%B8%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[hadoop的起源与背景知识一、什么是大数据 举例: 1.电商的推荐系统(可能会用到推荐算法:协同过滤,ALS,逻辑回归…)2.天气预报 核心的问题:数据的存储,数据的计算(不是算法) 数据的存储:采用分布式的文件系统HDFS(hadoop Distributed file system) 数据的计算:采用分布式的计算MapReduce,Spark(RDD:弹性分布式数据集) 二、数据仓库和大数据传统方式:搭建数据仓库(Data Werehouse)来解决大数据的问题 1.数据仓库就是一个数据库(Orcale,MySQL,MS)2.数据仓库和大数据一般只做查询(分析)3.搭建数据仓库的过程 (1)数据源RDBMS:关系型数据库(结构化数据)/文本数据/其他数据(2)利用ETL抽取/转化/加载数据后搭建数据仓库(保存为原始数据)(3)分析处理数据(SQL,PL/SQL,JDBC)(4)经过分析后搭建数据集市(hr,sales)(5)提供给hr系统或销售系统 Hadoop和Spark都可以看成是数据仓库的一种实现 1.从ETL到搭建数据集都可以用Hadoop和Spark所提供的方式解决,也可以用传统方式解决2.hadoop中使用sqoop和Flume组件完成ETL3.hadoop中使用HDFS存储数据(或存入基于HDFS之上的HBase或Hive数据仓库)4.hadoop中使用MapReduce(java程序)或Spark(Scala程序,只有数据计算,没有数据存储)或SQL处理数据5.处理数据后使用HDFS或NoSQL:Redis存储到数据集市*.大数据的终极目标是使用SQL处理数据 三、OLTP和OLAP LTP:Online Transaction processing 联机事务处理，指：insert、update、delete —&gt; 事务 OLAP:Online Analytic Processing 联机分析处理，一般只做查询 —&gt; 数据仓库就是一种OLAP的应用系统 Hadoop、Spark看成是一种数据仓库的解决方案 数据仓库（查询）不支持事务 四、Google的基本思想:三篇论文(一)GFS（Google File System） —- HDFS（Hadoop Distributed File System）：分布式文件系统 (*) HDFS = NameNode + SecondaryNameNode + DataNode1.分布式文件系统2.大数据的存储问题3.HDFS中，记录数据的位置信息（元信息） —- 采用倒排索引（Reverted Index） (1)什么是索引？Index * CREATE INDEX创建索引 * 索引就是一个记录（Oracle中索引表保存的是有规律的行地址） * 通过索引可以找到对应的数据(2)什么是倒排索引？ * 最简单的倒排索引：单词表（wordID，word，index）(3)NameNode（主节点，名称节点）是整个HDFS的管理员，和SecondaryNameNode（第二名称节点）同处一台主机，负责管理DateNode（从节点，数据节点），并不负责存储，与DateNode（从节点，数据节点）构成HDFS环境 (二)MapReduce：分布计算模型，问题来源PageRank（网页排名） 1.PageRank（网页排名）2.MapReduce的标程模型： *. 核心：先拆分（拆分计算，Map阶段），再合并（Reduce阶段）*. MR任务：job=map+reduce*. Map的输出同时也是Reduce的输入*. 一个MR任务一共存在四对输入和输出（），Map的输入和输出，Reduce的输入和输出*. k2=k3，v2和v3数据类型一致，v3是一个集合，该集合中的每个值就是v2*. 所有的数据类型必须是Hadoop自己的数据类型（为了实现Hadoop的序列化机制）*. MR任务处理的是HDFS上的数据*. Hadoop2.X开始，通过Yarn容器编程部署MR任务（ResourceManager&lt;主节点&gt; + NodeManager&lt;从节点&gt;） 123Example:/root/training/Hadoop-2.7.3/Share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jarYarn的web console:http://192.168.226.11:8088命令://hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount /input/data.txt /out/wcl (三)BigTable：大表 —- NoSQL数据库：HBase 1.关系型数据库：以二维表的形式保存数据2.大表的基本思想：所有的数据存入一张表（通过牺牲空间，换取时间）3.常见的NoSQL数据库（一般为行式数据库，适合） * Redis：内存数据库（一般为行式数据库，适合DML操作，insert，update，delect）* MongoDB：面向文档（BSON文档：是JSON的二进制）* HBase：面向列（列式数据库，查询，select，在HBase中，rowKey不能为null，但是可以重复，相同的ey是一条记录） 1HBase = ZooKeeper + HMaster（主节点） + RegionServer（从节点） hadoop的环境一、Hadoop的目录结构 hadoop-2.7.3/ —&gt; Hadoop的HOME目录bin/ —&gt; Hadoop的操作命令etc/hadoop/ —&gt; 所有的配置文件sbin/ —&gt; Hadoop集群的命令：启动，停止等share/ —&gt; 所有共享文件share/hadoop –&gt; 所有依赖jar包 二、Hadoop的三种安装模式(一)本地模式 * 没有HDFS，只能测试MapReduce程序（不是运行在Yarn中，作为一个独立的Java程序来运行）* 配置文件：etc/Hadoop/hadoop-env.sh export JAVA_HOME={JAVA_HOME} 自己的JAVA_HOME路径* 本地模式中用到的路径都是本地路径，因为没有HDFS (二)伪分布模式 * 特点：再单击上模拟一个分布式的环境，具备Hadoop的所有功能* 具备HDFS：NameNode + DataNode + SecondaryNameNode（端口50070）* 具备Yarn：ResourceManager + NodeManager（端口8088）* 对HDFS的NameNode进行格式化（/root/training/hadoop-2.7.3/tmp）* 启动HDFS：start-dfs.sh* 启动Yarn：start-yarn.sh* 统一启动：start-all.sh* 配置文件：etc/Hadoop/hadoop-env.sh export JAVA_HOME={JAVA_HOME} 自己的JAVA_HOME路径* 配置文件： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//hdfs-site.xml://配置数据块的冗余度//原则冗余度跟数据节点的个数保持一致，最大不要超过3&lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt;&lt;/property&gt;//是否开启HDFS权限检查，默认值为true（使用默认值，需要再改）&lt;!-- &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt;--&gt;//core-site.xml://配置HDFS主节点的地址，就是NameNode的地址//9000是RPC的通信端口&lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://RedHat111:9000&lt;/value&gt;&lt;/property&gt;//HDFS数据块和元信息保存在操作系统的目录位置//默认值是Linux的tmp目录&lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/root/training/hadoop-2.7.3/tmp&lt;/value&gt;&lt;/property&gt;//mapred-site.xml:（默认没有这个文件）//MR程序运行的程序或框架&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;//yarn-site.xml://配置yarn主节点的位置&lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;RedHat111&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt; (三)全分布模式12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061(1)hdfs-site.xml: &lt;!—配置数据块的冗余度，默认是3—&gt; &lt;!—原则冗余度跟数据节点保持一致，最大不要超过3—&gt; &lt;property&gt; &lt;name&gt;dis.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;!—是否开启HDFS权限检查，默认是true—&gt; &lt;property&gt; &lt;name&gt;dis.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; (2)core-site.xml: &lt;!—配置HDFS主节点位置，就是NameNode的位置—&gt; &lt;!—9000是RPC的通信端口—&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://RedHat112:9000&lt;/value&gt; &lt;/property&gt; &lt;!—HDFS元信息和数据块保存在操作目录的位置—&gt; &lt;!—默认是系统的tmp文件夹，会随断电而清除—&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/root/training/hadoop-2.7.3/tnp&lt;/value&gt; &lt;/porperty&gt; (3)mapped-site.xml &lt;!—MR运行容器或框架—&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; (4)yarn-site.xml &lt;!—配置yarn主节点的位置—&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;RedHat112&lt;/value&gt; &lt;/property&gt; &lt;!—NodeManager执行MR任务的方式是Shuffle洗牌—&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt;(4)slaves配置从节点地址: RedHat113 RedHat114 (5)对NameNode进行格式化(6)把RedHat上安装好的目录复制到从节点上 scp -r hadoop-2.7.3/ root@RedHat113:/root/training scp -r hadoop-2.7.3/ root@RedHat114:/root/training (7)在主节点上启动集群 start-all.sh]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hadoop 2.X管理与开发（二）]]></title>
    <url>%2F2018%2F09%2F05%2FHadoop-2-X%E7%AE%A1%E7%90%86%E4%B8%8E%E5%BC%80%E5%8F%91%EF%BC%88%E4%BA%8C%EF%BC%89%2F</url>
    <content type="text"><![CDATA[#Hadoop数据压缩 数据压缩 1）MR操作过程中进行大量数据传输，就需要对数据进行压缩 2）压缩技术能够有效减少底层存储（HDFS）读写字节数，提高的网络带宽和磁盘空间的效率 3）数据压缩能够有效节省资源 4）压缩事MR程序的优化策略 5）通过压缩编码对Mapper或者reduce数据传输进行的压缩，以减少磁盘IO 压缩的基本原则 1）运算密集型任务少用压缩 2）IO密集型的任务，多用压缩 MR支持的压缩编码 DEFAULT 是自带编码 .default 不可切分 Gzip 是自带编码 .gz 不可切分 bzip2 是自带编码 .bz2 可以切分 LZO 非自带编码 .lzo 可以切分 Snappy 非自带编码 .Snappy 不可切分 编码/解码器DEFAULT org.apache.hadoop.io.compress.DefaultCodeCGzip org.apache.hadoop.io.compress.GzipCodeCbzip2 org.apache.hadoop.io.compress.BZip2CodeCLZO com.hadoop.compression.lzo.lzoCodeCSnappy org.apache.hadoop.io.compress.SnappyCodeC 压缩性能Gzip 原大小：8.3GB 压缩后：1.8GB 压缩速度：17.5MB/s 解压速度：58MB/sbzip2 原大小：8.3GB 压缩后：1.1GB 压缩速度：2.4MB/s 解压速度：9.5MB/sLZO 原大小：8.3GB 压缩后：2.9GB 压缩速度：49.3MB/s 解压速度：74.6MB/s 设置压缩方式1234567891011121314mapper端： //开启map端的输出压缩 conf.setBoolean(&quot;mapreduce.map.outpot.compress&quot;, true); //设置压缩方式 //conf.setClass(&quot;mapreduce.map.outpot.compress.codec&quot;, DefaultCodec.class, CompressionCodec.class); conf.setClass(&quot;mapreduce.map.outpot. compress.codec&quot;, BZip2Codec.class, CompressionCodec.class);reduce端： //开启reduce端的输出压缩 FileOutputFormat.setCompressOutput(job, true); //设置压缩方式 //FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class); FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class); Hadoop优化MapReduce程序的效率瓶颈相关 MR功能：分布式离线计算 计算机性能：CPU、内存、磁盘、网络 I/O操作优化： 数据倾斜（代码优化）map和reduce的个数设置不合理map运行时间太长，导致reduce等待时间过久小文件过多（CombineTextInputFormat小文件合并）不可分快的超大文件（不断的溢写）多个溢写小文件需要多次merge 优化方法 数据输入 合并小文件：在执行MR任务前就进行小文件合并采用CombineTextInputFormat来作为输入来作为输入端大量小文件的场景 Map阶段 减少溢写次数（增加内存200MB 80%）：减少磁盘I/O 1234567891011mapred-default.xml:&lt;property&gt; &lt;name&gt;mapreduce.task.io.sort.mb&lt;/name&gt; &lt;value&gt;200&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;mapreduce.map.sort.spill.percent&lt;/name&gt; &lt;value&gt;0.80&lt;/value&gt;&lt;/property&gt; 减少合并的次数 12345mapred-default.xml:&lt;property&gt; &lt;name&gt;mapreduce.task.io.sort.factor&lt;/name&gt; &lt;value&gt;10&lt;/value&gt; //文件的个数，数值越大合并次数越少&lt;/property&gt; 在map之后，不影响业务逻辑的情况下可以使用combiner Reduce阶段 合理的设置map与reduce的个数设置map/reduce共存 12345mapred-default.xml:&lt;property&gt; &lt;name&gt;mapreduce.job.reduce.skowstart.completedmaps&lt;/name&gt; &lt;value&gt;0.05&lt;/value&gt; //设置运行一定程度的map后，启动reduce，减少等待时间&lt;/property&gt; 合理设置reduce的buffer 12345mapred-default.xml:&lt;property&gt; &lt;name&gt;mapreduce.reduce.markreset.buffer.percent&lt;/name&gt; &lt;value&gt;0.0&lt;/value&gt; &lt;/property&gt; I/O传输 进行数据压缩使用sequenceFile 数据倾斜 进行范围分区自定义分区Combine能用mapJoin的坚决不用reduceJoin 参数调优 分配map程序CPU核心数 12345mapred-default.xml:&lt;property&gt; &lt;name&gt;mapreduce.map.cpu.vcores&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; //核心数&lt;/property&gt; 分配reduce程序CPU核心数 12345mapred-default.xml:&lt;property&gt; &lt;name&gt;mapreduce.reduce.cpu.vcores&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; //核心数&lt;/property&gt; 设置maptask内存 12345mapred-default.xml:&lt;property&gt; &lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; //一个maptask能够使用的内存上限&lt;/property&gt; 设置reducetask内存 12345mapred-default.xml:&lt;property&gt; &lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; //一个maptask能够使用的内存上限&lt;/property&gt; reduce去map端并行度 12345mapred-default.xml:&lt;property&gt; &lt;name&gt;mapreduce.reduce.shuffle.parallelcopies&lt;/name&gt; &lt;value&gt;5&lt;/value&gt; //当reduce去map端拿取数据时所开的并行数是5&lt;/property&gt;]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[zookeeper]]></title>
    <url>%2F2018%2F09%2F05%2Fzookeeper%2F</url>
    <content type="text"><![CDATA[zookeeper简介 zookeeper 动物管理员 apache zookeeper致力于开发和维护开源服务器，实现高度可靠的分布式协调 什么是zookeeper zookeeper是一种集中式服务，用于维护配置信息，命名，提供分布式同步和提供组服务。所有这些类型的服务都以分布式应用程序的某种形式使用，每次实施他们都需要做很多工作来修复不可避免的错误和竞争条件。由于难以实现这些类型的服务，应用程序最初通常会吝啬他们，这使得他们在变化的情况下变得脆弱并且难以管理。即使正确完成，这些服务的不同实现也会在部署应用程序时导致管理复杂性。 zookeeper功能 1）存储数据：存储集群中每台机器都关心的数据（配置信息），需要接受服务器的注册 2）监听 zookeeper工作机制 基于观察者模式设计的分布式服务管理框架 1）启动服务器，首先向zookeeper中注册信息，创建临时节点（通过目录结构/servers/） 2）获取服务器列表并且注册监听（当服务器下线，在zookeeper中对应服务器节点将会消失） 3）当服务器宕机，zookeeper将会通过监听功能（process(){}回调方法）向zookeeper发送下线通知 4）监听功能重新获取服务器列表并再次进行监听 zookeeper的存储结构（目录树存储结构） / ——–&gt; 根目录（根目录下可以有多个节点） z1、z2 ——–&gt; 节点目录（Znode，默认存储1M数据） /z1/zz1 、 /z1/zz2 、 /z2/zz1 应用场景 1）集群同一配置管理 2）集群同一命名服务 3）集群统一管理 4）服务器的动态上下线感知 5）负载均衡 zookeeper集群安装 单节点安装 1）解压安装包2）重命名zookeeper/conf/zoo_sample.cfg 为 zoo.cfg3）修改zoo.cfg配置文件中的dataDir参数为自定义路径4）启动zk：bin/zkServer.sh start5）查看状态：bin/zkServer.sh status6）启动客户端：bin/zkCli.sh 全分布安装 1234567891011121314151617181）zoo.cfg： server.1=RedHat112:2888:3888 server.2=RedHat113:2888:3888 server.3=RedHat114:2888:38882）在自定义dataDir路径下创建myid文件，并设置idmyid： 13）配置环境变量4）将环境变量和zookeeper发送到其他服务器scp -zxvf -r zookeeper RedHat113:zookeeperscp -zxvf -r zookeeper RedHat114:zookeeper5)修改其他服务器myid6)生效环境变量 zookeeper操作 启动shell客户端：zkCli.sh zk：操作日志 ls path [watch]：查看path节点所包含的内容[监听] ls2 path [watch]：查看path节点的详细信息[监听] gat path [watch]：查看path节点所存储的数据 get path watch：监听节点值；get path watch：监听路径cZxid：事务id，每次创建都以时间戳的形式产生唯一的Zxidctime：节点创建时间mZxid：节点最后修改的时间戳idmtime：节点最后修改的时间pZxid：节点最后更新的子节点时间戳idcversion：节点修改次数dataVersion：节点数据的变化号dataLength：数据长度numChild：子节点数 stat path：查看节点的状态信息 set path acl：更改path节点中的数据 create [-s] [-e] path data acl：创建路径，data为创建目录节点所存储的数据，acl为应答类型，[-s]带序号的节点，[-e]短暂节点 delete path：删除节点（不能删除带有子节点的节点） rmr path：递归删除 zookeeper选举机制 当ZK1服务器启动时，先给自己投一票，ZK1作为follower 当ZK2服务器启动时，给自己投一票的同时，ZK1也会给ZK2投一票，票数占半数以上，ZK2作为leader 当ZK3服务器启动时，给自己投一票，此时发现已经存在leader，ZK3作为follower 注意事项 只要有半数以上的节点存活，就能够正常工作（zk集群要求配置奇数台）]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[基础命令]]></title>
    <url>%2F2018%2F09%2F05%2F%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[文件目录操作命令ls 显示文件和目录列表 -l 列出文件的详细信息-a 列出当前目录所有文件，包含隐藏文件*设置环境变量：/root/.bash_profile pwd 显示当前目录位置mkdir 创建目录 -p 父目录不存在的情况下先生成父目录约定： mkdir /root/tools —-&gt;安装包mkdir /root/training —-&gt;安装目录 cd 切换目录touch 生成一个空文件echo 生成一个带内容文件 使用echo查看环境变量值：echo $JAVA_HOME cat、tac 显示文本文件内容 cat是从第一行开始写，tac是从最后一行开始写 cp 复制文件或目录rm 删除文件 -r 同时删除该目录下的所有文件-f 强制删除文件或目录*HDFS有回收站，默认情况下关闭 系统操作命令ps 显示瞬间的进程状态 ps -ef：使用标准格式显示每个进程信息 hostname 显示主机名kill 杀死一个进程 -9 强制杀死一个进程-3 如果针对java进程，打印java进程的线程信息Thread Dump 打包命令gzip 压缩（解压）文件或目录，压缩文件后缀为gzbzip2 压缩（解压）文件或目录，压缩文件后缀为bz2tar 文件、目录打（解）包 -zxvf 解压缩-C 解压到指定目录下 权限管理（非常类似HDFS权限管理）权限的类型 r 读w 写x 执行 ls -l、ll 查看权限 十位字符，第一位如果为d，则代表该文件为目录后九位分为三组，每一组都按读写执行的顺序排列 第一组：当前用户第二组：同组用户第三组：其他用户权限用二进制表示，有为1，没有为0（rwx -&gt; 111，rw- -&gt; 110）chmod：改变权限chmod (u/g/o/a)+(r/w/x) 文件名（括号内可选）chmod 777 文件名（每一位十进制数代表一组权限，此处为所有用户可读可写可执行） 案例分析:java的死锁或者性能瓶颈分析 JDK heap dump:分析OOM的问题JDK Thread dumo:分析性能瓶颈(线程信息)得到Thread dump: 在linux:kill -3 PID在windows下:Fn + B 或Ctrl + Break 配置IPip config：查看IP状态更改ip配置：修改/etc/sysconfig/network-scripts下的ifcfg-eth*]]></content>
      <categories>
        <category>other</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[大数据模块]]></title>
    <url>%2F2018%2F09%2F04%2F%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9D%97%2F</url>
    <content type="text"><![CDATA[离线计算 Hadoop模块 1.数据存储：HDFS（Hadoop Distributed File System）2.数据计算：MapReduce（java程序、实现离线计算）：在Hadoop 2.X后，Yarn容器中3.Hive：基于HDFS之上的数据仓库，支持SQL语句4.HBase：基于HDFS之上的NoSQL数据库5.ZooKeeper：实现HA（High Availability高可用性、秒杀系统）的功能6.其他：Sqoop、Flume、Pig 实时计算 1.Redis内存NoSQL数据库 Redis Cluster：分布式解决方案2.Apache Storm：进行试试计算（流式计算） Spark：只有数据计算，没有数据的存储（依赖HDFS） 1.Scala变成语言：多范式的编程语言（支持多方式编程：1、面向对象 2、函数式编程）2.Spark Core：内核，相当于MapReduce； 最重要的概念：RDD（弹性分布式数据集）3.Spark SQL：类似Hive、支持SQL4.Spark Streaming：处理流式计算的模块，类似Storm]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Redis初识]]></title>
    <url>%2F2018%2F08%2F29%2Fredis%E5%88%9D%E8%AF%86%2F</url>
    <content type="text"><![CDATA[Redis 高性能Key-Value服务器 多种数据结构 丰富的功能 高可用分布式支持 Redis初识 由Salvatore Sanfilippo（antirez）制作，目前服务于以色列RedisLabs，早期代码23000行，采用key-value的字典结构，GitHub、twitter、StackOverflow、阿里巴巴、百度、微博、美团、搜狐等都在使用Redis这样的技术，如今Redis已经成为必备技能。 典型使用场景 缓存系统：用户访问App server，App Server从cache（Redis）请求数据，如果有，直接返回给App Server，如果没有，cache将从Storage（持久化存储空间）中查找，查找后将从Storage中查找到的数据存入cache中以方便下次查找，而后直接由Storage返回给App Server。 计数器：微博、视频网站的转发、评论数。 消息队列系统 排行榜 社交网络 实时系统 开源基于键值的存储服务系统支持多种数据结构性能高，功能丰富 Redis特性 速度快（10W OPS(读写)） 将数据存在内存 用c语言编写 线性模型使用单线程 持久化（断电不丢数据） Redis所有数据保持在内存中，对数据的更新将异步地保存到硬盘上 多种数据结构 常规 字符串（Strings/Blobs/Bitmaps） 哈希（Hash Tables(objects!)） 列表（Linked Lists） 集合（Sets） 有序集合（Sorted Sets） 衍生 位图（BitMaps） 超小内存唯一值技术（HyperLogLog——有一定误差） 地理信息定位（GEO） 支持多种编辑语言 Java php Python Ruby Lua NodeJs 功能丰富 发布订阅 Lua脚本 事务 pipeline 简单 23000行代码 不依赖外部库（like libevent） 单线程模型（开发相对容易） 主从复制 在Redis中主服务器的数据可以同步到从服务器上，为高可用以及分布式提供一个很好的基础 高可用、分布式 高可用 ——&gt; Redis-Sentinel(v2.8)支持高可用分布式 ——&gt; Redis-Cluster(V3.0)支持分布式]]></content>
      <categories>
        <category>Hadoop</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[项目笔记(ssh)]]></title>
    <url>%2F2018%2F08%2F23%2F%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0(ssh)%2F</url>
    <content type="text"><![CDATA[1.Dao层抽取1234(1)BaseDaoImpl&lt;T&gt;--&gt;BaseDao&lt;T&gt; UserDaoImpl&lt;User&gt;--&gt;UserDao&lt;User&gt;(2)UserDao&lt;User&gt; extends BaseDao&lt;T&gt; UserDaoImpl&lt;User&gt; extends BaseDaoImpl&lt;T&gt; 2.Action层抽取12(1)BaseAction&lt;T&gt; extends ActionSupport implements ModelDriven&lt;T&gt;(2)UserAction&lt;User&gt; extends BaseAction&lt;T&gt; 3.通过反射创建对象1234567891011//在构造方法中动态获取实体类，通过反射创建model对象//泛型指定user时，this为user，通过反射创建user对象赋值给model public BaseAction() throws InstantiationException, IllegalAccessException &#123; //获取父类class属性 ParameterizedType genericsSuperclass = (ParameterizedType) this.getClass().getGenericSuperclass(); //获取泛型数组 Type[] actualTypeArguments = genericsSuperclass.getActualTypeArguments(); Class&lt;T&gt; entityClass = (Class&lt;T&gt;)actualTypeArguments[0]; //通过反射创建对象 model = entityClass.newInstance(); &#125;]]></content>
      <categories>
        <category>SSH</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[hibernate项目笔记(ssh)]]></title>
    <url>%2F2018%2F08%2F22%2Fhibernate%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0(ssh)%2F</url>
    <content type="text"><![CDATA[1.与spring整合中hibernate连接池的两种方式（web/applicationContext.xml）1234567方式1：&lt;bean id="dataSource" class="com.mchange.v2.c3p0.ComboPooledDataSource"&gt; &lt;property name="driverClass" value="com.mysql.jdbc.Driver"/&gt; &lt;property name="jdbcUrl" value="jdbc:mysql////bos"/&gt; &lt;property name="user" value="root"/&gt; &lt;property name="password" value="tiger"/&gt;&lt;/bean&gt; 123456789101112131415方式2：&lt;bean id="dataSource" class="org.springframework.jdbc.datasource.DriverManagerDataSource"&gt; &lt;property name="driverClassName"&gt; &lt;value&gt;$&#123;database.driverClass&#125;&lt;/value&gt; &lt;/property&gt; &lt;property name="url"&gt; &lt;value&gt;$&#123;database.jdbcUrl&#125;?characterEncoding=UTF-8&amp;amp;useSSL=false&lt;/value&gt; &lt;/property&gt; &lt;property name="username"&gt; &lt;value&gt;$&#123;database.user&#125;&lt;/value&gt; &lt;/property&gt; &lt;property name="password"&gt; &lt;value&gt;$&#123;database.password&#125;&lt;/value&gt; &lt;/property&gt; &lt;/bean&gt; 2.getHibernateTemplate()的使用方法123(1)导入org.springframework.orm.hibernate5.support.HibernateDaoSupport包(2)继承HibernateDaoSupport类 * this.getHibernateTemplate().get(entityClass,id);]]></content>
      <categories>
        <category>SSH</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[maven整合框架笔记(ssh)]]></title>
    <url>%2F2018%2F08%2F22%2Fmaven%E6%95%B4%E5%90%88%E6%A1%86%E6%9E%B6%E7%AC%94%E8%AE%B0(ssh)%2F</url>
    <content type="text"><![CDATA[1.配置阿里远程仓库（parent/pom）1234567891011121314&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;maven - ali&lt;/id&gt; &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt; &lt;releases&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;/releases&gt; &lt;snapshots&gt; &lt;enabled&gt;true&lt;/enabled&gt; &lt;updatePolicy&gt;always&lt;/updatePolicy&gt; &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt; &lt;/snapshots&gt; &lt;/repository&gt;&lt;/repositories&gt; 2.maven中spring与jdk 1.8的兼容问题（parent/pom）123456789&lt;!-- 属性定义指定jar版本 --&gt;&lt;properties&gt; &lt;spring.version&gt;5.0.5.RELEASE&lt;/spring.version&gt; &lt;hibernate.version&gt;5.2.17.Final&lt;/hibernate.version&gt; &lt;struts2.version&gt;2.3.24&lt;/struts2.version&gt; &lt;slf4j.version&gt;1.6.6&lt;/slf4j.version&gt; &lt;log4j.version&gt;1.2.12&lt;/log4j.version&gt; &lt;shiro.version&gt;1.2.3&lt;/shiro.version&gt;&lt;/properties&gt;]]></content>
      <categories>
        <category>SSH</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[git安装与简单操作(hexo)]]></title>
    <url>%2F2018%2F08%2F22%2Fgit%E5%AE%89%E8%A3%85%E4%B8%8E%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1.安装Node.js和配置好Node.js环境 2.安装Git和配置好Git环境 3.Github账户注册和新建项目，项目必须要遵守格式：账户名.github.io 在建好的项目右侧有个settings按钮，点击它，向下拉到GitHub Pages，你会看到那边有个网址，访问它，你将会惊奇的发现该项目已经被部署到网络上，能够通过外网来访问它 4.安装Hexo 创建文件夹通过命令行进入到该文件夹安装Hexo 1npm install hexo -g 5.初始化该文件夹 1hexo init 6.安装所需要的组件 1npm install 7.联系hexo与github page 终端输入： 1$ git --global user.name "XXX" 终端输入： 1$ git --global user.email "XXX@XXX.XXX" 添加密匙到ssh-agent终端输入： 1$ eval "$(ssh-agent -s)" 添加生成的ssh key到ssh-agent终端输入 1$ eval "$(ssh-agent -s)" github–&gt;setting–&gt;ssh and gpg keys添加id_rsa.pub文件中的ssh key 8.配置Deployment(_config.yml) 1234deploy: type: git repository: git@github.com:zfhzxg/zfhzxg.github.io.git branch: master 基础命令 终端：检测ssh是否配置成功 1$ ssh -T git@github.com 生成ssh密匙： 1$ ssh-keygen -t rsa -C "XXX@XXX.XXX" 生成.get： 1$ get init cmd：检测node.js是否安装成功： 12node -vnmp -v 检查hexo是否安装成功： 1hexo -v 开启本地服务器： 1hexo g 改变端口号： 1hexo server -p XXXX 新建博客： 1hexo new post "博客名" 安装拓展： 1npm install hexo-deployer-git --save 生成部署： 1hexo d -g]]></content>
      <categories>
        <category>other</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[git报错]]></title>
    <url>%2F2018%2F08%2F22%2Fgit%E6%8A%A5%E9%94%99%2F</url>
    <content type="text"><![CDATA[安装主题报错 报错内容： 1curl 18 transfer closed with outstanding read data remaining 解决方法： 1git config --global http.postBuffer 524288000 curl 18 transfer closed with outstanding read data remaining 解决方法： 1git config --global http.postBuffer 524288000]]></content>
      <categories>
        <category>other</category>
      </categories>
  </entry>
</search>
