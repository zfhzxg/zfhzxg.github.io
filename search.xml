<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>spring</title>
      <link href="/2018/11/14/spring/"/>
      <url>/2018/11/14/spring/</url>
      
        <content type="html"><![CDATA[<h1 id="Spring概述"><a href="#Spring概述" class="headerlink" title="Spring概述"></a>Spring概述</h1><ul><li>是一个开源框架，于2003年兴起的一个java轻量级开发框架，是一个分层的java se/ee开发的一站式开源框架</li><li>优点：轻量，控制反转（IOC），面向切面，容器，框架，MVC</li><li>Spring三层：<blockquote><p>WEB层：SpringMVC<br>Service层：Spring的Bean管理，Spring声明式事务<br>Dao层：Spring的JDBC模板，Spring的ORM模块</p></blockquote></li></ul><hr><h1 id="Spring优点"><a href="#Spring优点" class="headerlink" title="Spring优点"></a>Spring优点</h1><ul><li>方便解耦合</li><li>AOP的开发：对程序进行扩展</li><li>轻量级框架</li><li>方便与其他框架整合</li></ul><hr><h1 id="Spring包"><a href="#Spring包" class="headerlink" title="Spring包"></a>Spring包</h1><ul><li>Spring-framework-4.X.XRELEASE-dependencies.zip：Spring依赖库</li><li>Spring-framework-4.X.XRELEASE-dist.zip：核心包<blockquote><p>Spring-framework-4.X.XRELEASE-dist/docs：开发规范和API<br>Spring-framework-4.X.XRELEASE-dist/libs：核心jar包和源码<br>Spring-framework-4.X.XRELEASE-dist/schema：配置文件的约束</p></blockquote></li><li>Spring-framework-4.X.XRELEASE-docs.zip：文档</li><li>Spring-framework-4.X.XRELEASE-schema.zip：约束</li></ul><hr><h1 id="SpringIOC的XML开发-IOC：Inversion-of-Control（控制反转）"><a href="#SpringIOC的XML开发-IOC：Inversion-of-Control（控制反转）" class="headerlink" title="SpringIOC的XML开发(IOC：Inversion of Control（控制反转）)"></a>SpringIOC的XML开发(IOC：Inversion of Control（控制反转）)</h1><h2 id="包、依赖"><a href="#包、依赖" class="headerlink" title="包、依赖"></a>包、依赖</h2><ul><li>通过xml文件，将对象的创建权反转给（交给）Spring</li><li><p>相关包文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spring-beans-4.2.4.RELEASE.jar（实体类包）</span><br><span class="line">spring-context-4.2.4.RELEASE.jar（核心扩展包）</span><br><span class="line">spring-core-4.2.4.RELEASE.jar（核心包）</span><br><span class="line">spring-expression-4.2.4.RELEASE.jar（Spring el表达式包）</span><br><span class="line">com.springsource.org.apache.commons.logging-1.1.1.jar（日志接口）</span><br><span class="line">com.springsource.org,apache.log4j-1.2.15.jar（log4j日志记录工具包）</span><br></pre></td></tr></table></figure></li><li><p>通过BeanFactory（工厂）切换底层实现类，消除哦耦合</p></li><li><p>BeanFactory对Xml进行解析，通过xml中的id找到其实现类进行反射，返回实例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">XMl：</span><br><span class="line">&lt;bean i’d=“UserDao” class=“UserDaoImpl”&gt;&lt;/bean&gt;</span><br></pre></td></tr></table></figure></li><li><p>引入约束</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spring-framework-4.2.4.RELEASE-dist\spring-framework-4.2.4.RELEASE\docs\spring-framework-reference\html\xsd-configuration.html：</span><br><span class="line">beans schema</span><br></pre></td></tr></table></figure></li><li><p>IOC和DI</p><blockquote><p>DI：依赖注入，前提是必须有IOC的环境，Spring管理这个类的时候将类的依赖注入（设置property属性）</p></blockquote></li></ul><h2 id="Spring的工厂类"><a href="#Spring的工厂类" class="headerlink" title="Spring的工厂类"></a>Spring的工厂类</h2><h3 id="BeanFactory（老版本工厂类）"><a href="#BeanFactory（老版本工厂类）" class="headerlink" title="BeanFactory（老版本工厂类）"></a>BeanFactory（老版本工厂类）</h3><ul><li>BeanFactory：调用个体Bean的时候，才会生成类的实例<h3 id="ApplicationContext（新版本工厂类）"><a href="#ApplicationContext（新版本工厂类）" class="headerlink" title="ApplicationContext（新版本工厂类）"></a>ApplicationContext（新版本工厂类）</h3></li><li>ApplicationContext底层继承了BeanFactory</li><li>ApplicationContext：加载配置文件的时候，就会将Spring管理的类都实例化</li><li>ApplicationContext有两个实现类<blockquote><p>ClassPathXmlApplicationContext：加载类路径下的配置文件（src路径下）<br>FileSystemXmlApplicationContext：加载文件系统下的配置文件（磁盘路径下）</p></blockquote></li></ul><h2 id="Spring-Bean管理"><a href="#Spring-Bean管理" class="headerlink" title="Spring Bean管理"></a>Spring Bean管理</h2><h3 id="id、name"><a href="#id、name" class="headerlink" title="id、name"></a>id、name</h3><ul><li>id ：使用了约束中的唯一约束，不能出现特殊字符</li><li><p>name ：没有使用约束中的唯一约束，可以出现特殊字符（理论上可以出现重复，但实际开发不能出现）</p><blockquote><p>Spring和struts1框架整合的时候（struts1中action注入id名必须加上斜杠）</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;bean id=&quot;/user&quot; class=&apos;&apos;&gt;</span><br></pre></td></tr></table></figure></blockquote></li><li><p>class：需要生成的实例类的全路径</p></li></ul><h3 id="Bean生命周期的配置"><a href="#Bean生命周期的配置" class="headerlink" title="Bean生命周期的配置"></a>Bean生命周期的配置</h3><ul><li>init-method ：Bean被初始化时执行的方法</li><li>destroy-method ：Bean被销毁时执行的方法，需要从实体类中调用（Bean作为单例模式创建时）</li></ul><h3 id="Bean作用范围的配置"><a href="#Bean作用范围的配置" class="headerlink" title="Bean作用范围的配置"></a>Bean作用范围的配置</h3><ul><li>scope ：Bean的作用范围<blockquote><p>singleton ：默认的，Spring会采用单例模式创建这个对象<br>prototype ：多例模式（struts2与Spring整合时，action采用多例模式）<br>request ：应用在web项目中，Spring创建这个类以后，将这个类存入到request范围中<br>session ：用用在web项目中，Spring创建这个类以后，将这个类存入到session范围中<br>globalsession（全局session） ：应用到web项目中，必须在porlet环境下使用，如果没有这种环境，globalsession相当于session</p></blockquote></li></ul><h3 id="Spring属性注入（DI）"><a href="#Spring属性注入（DI）" class="headerlink" title="Spring属性注入（DI）"></a>Spring属性注入（DI）</h3><ul><li>三种注入方式<blockquote><p>构造方法方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public class user&#123;</span><br><span class="line">    private String name;</span><br><span class="line">    public user(String name)&#123;</span><br><span class="line">        this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></blockquote></li></ul><blockquote><p>set方法方式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public class user&#123;</span><br><span class="line">    private String name;</span><br><span class="line">    public void setName(String name)&#123;</span><br><span class="line">        this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>接口注入方式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public interface user&#123;</span><br><span class="line">    public void setName(String name);</span><br><span class="line">&#125;</span><br><span class="line">public class userImpl implements user&#123;</span><br><span class="line">    private String name;</span><br><span class="line">    public void setName(String name)&#123;</span><br><span class="line">        this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></blockquote><ul><li>Spring支持构造方法注入和set方法注入<blockquote><p>构造方法属性注入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 构造属性注入的方式 --&gt;</span><br><span class="line">&lt;bean id=&quot;car&quot; class=&quot;com.zfhzxg.spring.demo3.Car&quot;&gt;</span><br><span class="line">&lt;constructor-arg name=&quot;name&quot; value=&quot;zfhzxg&quot;/&gt;</span><br><span class="line">&lt;constructor-arg name=&quot;p&quot; value=&quot;BMW&quot;/&gt;</span><br><span class="line">&lt;/bean&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 构造对象属性注入的方式 --&gt;</span><br><span class="line">&lt;!-- value用来设置普通属性值，ref用来设置其他的类的id和name --&gt;</span><br><span class="line">&lt;bean id=&quot;emp&quot; class=&quot;com.zfhzxg.spring.demo3.Emp&quot;&gt;</span><br><span class="line">&lt;constructor-arg name=&quot;name&quot; value=&quot;zfhzxg&quot;/&gt;</span><br><span class="line">&lt;constructor-arg name=&quot;car2&quot; ref=&quot;car2&quot;/&gt;</span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure></blockquote></li></ul><blockquote><p>set方法属性注入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- set属性注入的方式 --&gt;</span><br><span class="line">&lt;bean id=&quot;car2&quot; class=&quot;com.zfhzxg.spring.demo3.Car2&quot;&gt;</span><br><span class="line">&lt;property name=&quot;name&quot; value=&quot;zhxzgd&quot;/&gt;</span><br><span class="line">&lt;property name=&quot;p&quot; value=&quot;BMW&quot;/&gt;</span><br><span class="line">&lt;/bean&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- set对象属性注入的方式 --&gt;</span><br><span class="line">&lt;!-- value用来设置普通属性值，ref用来设置其他的类的id和name --&gt;</span><br><span class="line">&lt;bean id=&quot;emp&quot; class=&quot;com.zfhzxg.spring.demo3.Emp&quot;&gt;</span><br><span class="line">&lt;property name=&quot;name&quot; value=&quot;zhxzgd&quot;/&gt;</span><br><span class="line">&lt;property name=&quot;car2&quot; ref=&quot;car2&quot;/&gt;</span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>P名称空间属性注入（通过引入P名称空间来完成该属性的注入，Spring2.5之后）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">xmlns:p=&quot;http://www.springframework.org/schema/p&quot;</span><br><span class="line">&lt;!-- 不能输用构造函数--&gt;</span><br><span class="line">&lt;!-- 普通属性 P:属性名=&quot;值&quot;; --&gt;</span><br><span class="line">&lt;!-- 对象属性 P:属性名-ref=&quot;值&quot;; --&gt;</span><br><span class="line">&lt;!-- p名称空间注入方式 --&gt;</span><br><span class="line">&lt;bean id=&quot;car2&quot; class=&quot;com.zfhzxg.spring.demo3.Car2&quot; p:name=&quot;zfhzxg&quot; p:p=&quot;BMW&quot;&gt;&lt;/bean&gt;</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>SpEL的属性注入（Spring Expression Language，Spring3.0之后）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">语法：</span><br><span class="line">    #&#123;SpEL&#125;//可以进行逻辑运算，调用对象和方法的值</span><br><span class="line">&lt;!-- SpEL的属性注入方式 --&gt;</span><br><span class="line">&lt;bean id=&quot;carInfo&quot; class=&quot;com.zfhzxg.spring.demo3.CarInfo&quot;&gt;&lt;/bean&gt;</span><br><span class="line">&lt;bean id=&quot;car2&quot; class=&quot;com.zfhzxg.spring.demo3.Car2&quot;&gt;</span><br><span class="line">&lt;property name=&quot;name&quot; value=&quot;#&#123;carInfo.name&#125;&quot;/&gt;</span><br><span class="line">&lt;property name=&quot;p&quot; value=&quot;#&#123;carInfo.calculaorPrice()&#125;&quot;/&gt;</span><br><span class="line">&lt;/bean&gt;</span><br><span class="line">&lt;bean id=&quot;emp&quot; class=&quot;com.zfhzxg.spring.demo3.Emp&quot;&gt;</span><br><span class="line">&lt;property name=&quot;name&quot; value=&quot;#&#123;&apos;zhxzgd&apos;&#125;&quot;/&gt;</span><br><span class="line">&lt;property name=&quot;car2&quot; value=&quot;#&#123;car2&#125;&quot;/&gt;</span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>集合类型属性注入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Spring集合属性注入 --&gt;</span><br><span class="line">&lt;bean name=&quot;collectionBean&quot; class=&quot;com.zfhzxg.spring.demo4.CollectionBean&quot;&gt;</span><br><span class="line">&lt;!-- 注入数组类型和list一样 --&gt;</span><br><span class="line">&lt;property name=&quot;arrs&quot; &gt;</span><br><span class="line">&lt;list&gt;</span><br><span class="line">&lt;!-- 对象用ref --&gt;</span><br><span class="line">&lt;value&gt;zfhzxg&lt;/value&gt;</span><br><span class="line">&lt;value&gt;zhxzgd&lt;/value&gt;</span><br><span class="line">&lt;/list&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property name=&quot;list&quot; &gt;</span><br><span class="line">&lt;list&gt;</span><br><span class="line">&lt;!-- 对象用ref --&gt;</span><br><span class="line">&lt;value&gt;123&lt;/value&gt;</span><br><span class="line">&lt;value&gt;321&lt;/value&gt;</span><br><span class="line">&lt;/list&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property name=&quot;set&quot; &gt;</span><br><span class="line">&lt;list&gt;</span><br><span class="line">&lt;!-- 对象用ref --&gt;</span><br><span class="line">&lt;value&gt;abc&lt;/value&gt;</span><br><span class="line">&lt;value&gt;cba&lt;/value&gt;</span><br><span class="line">&lt;/list&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 此处面向对象使用value-ref --&gt;</span><br><span class="line">&lt;property name=&quot;map&quot; &gt;</span><br><span class="line">&lt;map&gt;</span><br><span class="line">&lt;entry key=&quot;aaa&quot; value=&quot;111&quot;&gt;&lt;/entry&gt;</span><br><span class="line">&lt;entry key=&quot;bbb&quot; value=&quot;222&quot;&gt;&lt;/entry&gt;</span><br><span class="line">&lt;/map&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>分模块开发配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">一、直接加载多文件：</span><br><span class="line">    ApplicationContext applicationContext = new ClassPathXmlApplicationContext(&quot;applicationContext1.xml&quot;,&quot;applicationContext2.xml&quot;);</span><br><span class="line">二、在applicationContext1.xml中引入其他配置文件：</span><br><span class="line">    &lt;import resource=&quot;applicationContext2.xml&quot;/&gt;</span><br><span class="line">    &lt;import resource=&quot;applicationContext3.xml&quot;/&gt;</span><br></pre></td></tr></table></figure></p></blockquote><hr><h1 id="SpringIOC的注解"><a href="#SpringIOC的注解" class="headerlink" title="SpringIOC的注解"></a>SpringIOC的注解</h1><ul><li>在Spring4的版本中，除了需要引入基本的四个开发包意外，还需要引入aop的包</li></ul><hr><h1 id="Spring的web项目资源浪费（与struts2整合）"><a href="#Spring的web项目资源浪费（与struts2整合）" class="headerlink" title="Spring的web项目资源浪费（与struts2整合）"></a>Spring的web项目资源浪费（与struts2整合）</h1><ul><li>web每次请求都会创建Spring的工厂，浪费服务器资源</li><li>解决：<blockquote><p>在服务器启动的时候创建Spring的工厂<br>将工厂保存到ServletContext中<br>每次获取工厂都从ServletContext中获取</p></blockquote></li><li>整合：<blockquote><p>引入spring_web.jar<br>配置ContextLoaderListener</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 配置Spring的核心监听器 --&gt;</span><br><span class="line">&lt;!-- ContextLoaderListener底层实现ServletContextListener --&gt;</span><br><span class="line">  &lt;listener&gt;</span><br><span class="line">  &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;</span><br><span class="line"> &lt;/listener&gt;</span><br><span class="line">&lt;!-- 默认加载的是/WEB-INF/applicationContext.xml --&gt;</span><br><span class="line">&lt;!-- 修改加载配置文件的路径 --&gt;</span><br><span class="line">&lt;context-param&gt;</span><br><span class="line">  &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;</span><br><span class="line">  &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt;</span><br><span class="line">  &lt;/context-param&gt;</span><br><span class="line">获取工厂：</span><br><span class="line">public String getApplicationContext() &#123;</span><br><span class="line">ServletContext serviceContext = ServletActionContext.getServletContext();</span><br><span class="line">WebApplicationContext applicationContext = WebApplicationContextUtils.getWebApplicationContext(serviceContext);</span><br><span class="line">CustomerService customerService = (CustomerService)applicationContext.getBean(&quot;customerService&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> SSM </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>yarn</title>
      <link href="/2018/10/19/yarn/"/>
      <url>/2018/10/19/yarn/</url>
      
        <content type="html"><![CDATA[<h1 id="RecourceManager"><a href="#RecourceManager" class="headerlink" title="RecourceManager"></a>RecourceManager</h1><ul><li>处理客户端的请求</li><li>监控NodeManager</li><li>启动或者监控程序</li><li>资源的分配和调度</li></ul><h1 id="NodeManager"><a href="#NodeManager" class="headerlink" title="NodeManager"></a>NodeManager</h1><ul><li>RecourceManager将会给NodeManager分配container（资源）和AppMstr（任务）来处理数据</li><li>管理单个节点的资源</li><li>处理来自RecourceManager的命令</li><li>处理程序的命令</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>eclipse的Hadoop环境配置</title>
      <link href="/2018/10/12/eclipse%E7%9A%84Hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-2/"/>
      <url>/2018/10/12/eclipse%E7%9A%84Hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-2/</url>
      
        <content type="html"><![CDATA[<h1 id="设置系统环境变量"><a href="#设置系统环境变量" class="headerlink" title="设置系统环境变量"></a>设置系统环境变量</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME：hadoop-2.7.3/</span><br><span class="line">PATH：%HADOOP_HOME%/bin</span><br></pre></td></tr></table></figure><h1 id="eclipse进行导包"><a href="#eclipse进行导包" class="headerlink" title="eclipse进行导包"></a>eclipse进行导包</h1><ul><li>包路径：hadoop-2.7.3/share/</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>HDFS</title>
      <link href="/2018/10/01/HDFS/"/>
      <url>/2018/10/01/HDFS/</url>
      
        <content type="html"><![CDATA[<h1 id="HDFS的体系架构"><a href="#HDFS的体系架构" class="headerlink" title="HDFS的体系架构"></a>HDFS的体系架构</h1><h2 id="NameNode：名称节点"><a href="#NameNode：名称节点" class="headerlink" title="NameNode：名称节点"></a>NameNode：名称节点</h2><ul><li>是HDFS的主节点、管理员</li><li>接收客户端（命令行、java程序）的请求：创建目录、上传数据、下载数据、删除数据等</li><li>管理和维护HDFS的日志和元信息    <blockquote><p>日志文件（edits文件）：记录的是客户端的所有操作，是一个二进制文件（JSON）</p><blockquote><p>位置：/root/training/hadoop/tmp/dfs/name/current<br>edit_inprogress_00000000000000XXXXX：正在操作的日志文件<br>hdfs oev -i edits_inprogress_00000000000000XXXXX -o ~/a.xml：通过日志查看器（edits viewer），把edits文件转换成文本（xml）格式<br>元信息（fsimage文件）：记录的是数据块的位置信息，数据块的冗余信息，是一个二进制文件<br>位置：/root/training/hadoop/tmp/dfs/name/current<br>fsimage_0000000000000XXXXX：元信息记录文件<br>hdfs oiv -i fsimage_000000000000XXXXX -o ~/b.xml：将元信息记录文件转换成文本（xml或txt）格式</p></blockquote></blockquote></li><li>HDFS元信息保存位置配置<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs-site.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.name.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;储存位置&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li></ul><h2 id="DataNode：数据节点"><a href="#DataNode：数据节点" class="headerlink" title="DataNode：数据节点"></a>DataNode：数据节点</h2><ul><li>按照数据块保存数据<blockquote><p>1.X : 64M<br>2.X : 128M</p></blockquote></li><li>数据块：表现形式就是一个文件（blk打头）<blockquote><p>位置：/root/training/hadoop-2.7.3/tmp/dfs/data/current/BP-XXX-数据节点-XXX/current/finalized/subdir0/subdir0/<br>一个数据块对应的是一对文件，‘.meta’记录的是数据块的元信息<br>设置数据块冗余度规则：一般跟数据节点个数相同，但最大不要超过3<br>Hadoop 3.X之前，会造成存储空间极大的浪费<br>Hadoop 3.X之后，采用HDFS纠删码技术，使得存储空间节约一半</p></blockquote></li><li>HDFS文件系统的保存位置配置<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs-site.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.data.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;储存位置&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li></ul><h2 id="SecondaryNameNode：第二名称节点"><a href="#SecondaryNameNode：第二名称节点" class="headerlink" title="SecondaryNameNode：第二名称节点"></a>SecondaryNameNode：第二名称节点</h2><ul><li>职责：进行日志信息的合并<blockquote><p>SecondaryNameNode向NameNode下载edits日志文件和fsimage元信息文件<br>将edits中最新的信息写入fsimage文件<br>将合并后的文件上传给NameNode<br>当上次合并发生以后，用户进行新的操作，NameNode将产生新的edits_inprogress<br>当HDFS发出检查点（checkpoint）的时候，会进行日志信息合并<br>默认情况下，HDFS每隔60分钟或edits文件达到了64M产生一个检查点</p></blockquote></li><li>由于edits文件记录了最新的状态信息，并且随着操作越多，edits就会越大</li><li>把edits中的最新信息写到fsimage中</li><li>edits文件就可以清空</li><li>配置SecondaryNameNode节点位置<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs-site.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dis.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;RedHat112:50090&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li></ul><h1 id="HDFS基础命令"><a href="#HDFS基础命令" class="headerlink" title="HDFS基础命令"></a>HDFS基础命令</h1><ul><li>hdfs dfs -help        ：查看帮助</li><li>hdfs dfs -ls /        ：列出/目录下的文件和目录</li><li>hdfs dfs -put /本地路径 /上传路径        ：上传文件</li><li>hdfs dfs -moveFromLocal /本地路径 /上传路径        ：剪切上传文件</li><li>hdfs dfs -get /hdfs路径 /本地路径        ：下载文件</li><li>hdfs dfs -getmerge /hdfs目录（是一个文件夹） /合并后的文件        ：合并下载</li><li>hdfs dfs -mkdir /目录名        ：创建目录</li><li>hdfs dfs -mkdir -p /目录名/目录名        ：创建多级目录</li><li>hdfs dfs -mv /需要移动的目录（文件） /需要移动到的位置        ：移动文件/文件夹</li><li>hdfs dfs -copy /需要复制的目录（文件） /需要复制到的位置        ：复制文件/文件夹</li><li>hdfs dfs -rm /文件路径       ：删除文件</li><li>hdfs dfs -rm -r /目录路径        ：删除目录</li><li>hdfs dfs -cat /文件路径        ：查看文件</li><li>hdfs dfs -tail -f /文件路径        ：查看文件的最后指定行</li><li>hdfs dfs -count /目录路径（文件路径）        ：查看目录（文件）文件夹数，文件数、大小</li><li>hdfs dfs -df -h /        ：查看hdfs的总空间</li><li>hdfs dfs -setrep 冗余度 /文件路径        ：设置单个文件冗余度</li></ul><h1 id="简单的HDFS上传文件代码"><a href="#简单的HDFS上传文件代码" class="headerlink" title="简单的HDFS上传文件代码"></a>简单的HDFS上传文件代码</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">package hdfs.demo1;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.net.URI;</span><br><span class="line">import java.net.URISyntaxException;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line"></span><br><span class="line">public class HDFSClient &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) throws IOException, InterruptedException, URISyntaxException &#123;</span><br><span class="line"></span><br><span class="line">//客户端加载配置文件</span><br><span class="line">Configuration conf = new Configuration();</span><br><span class="line"></span><br><span class="line">//指定配置（设置冗余度）</span><br><span class="line">conf.set(&quot;dfs.replication&quot;,&quot;2&quot;);</span><br><span class="line"></span><br><span class="line">//指定块大小</span><br><span class="line">conf.set(&quot;dfs.blocksize&quot;, &quot;64m&quot;);</span><br><span class="line"></span><br><span class="line">//构造客户端</span><br><span class="line">FileSystem fS = FileSystem.get(new URI(&quot;hdfs://192.168.0.102:9000/&quot;),conf,&quot;root&quot;);</span><br><span class="line"></span><br><span class="line">//上传文件</span><br><span class="line">fS.copyFromLocalFile(new Path(&quot;D:/123.txt&quot;), new Path(&quot;/work.txt&quot;));</span><br><span class="line"></span><br><span class="line">//关闭资源</span><br><span class="line">fS.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h1><ul><li>请求N阿么N哦的上传文件123.txt（客户端——》NameNode）</li><li>响应可以上传的文件（NameNode——》客户端）</li><li>请求上传第一个Block（块文件）（0～120m），请求分会DataNode（客户端——》NameNode）</li><li>返回DataNode1，DataNode2，DataNode3，表示采用这两个酒店储存具体的数据（NameNode——》客户端）</li><li>请求建立一个Block传输通道（客户端——》DataNode1——》dataNode2——》dataNode3）</li><li>DataNode1，DataNode2，DataNode3应答成功（DataNode——》客户端）</li><li>传输数据（客户端——》DataNode1——》dataNode2——》dataNode3）</li></ul><h1 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h1><ul><li>请求下载文件（客户端——》NameNode）</li><li>返回目标文件的元数据（NameNode——》客户端）</li><li>通过元信息请求第一块数据（客户端——》DataNode）</li><li>传输数据给客户端（DtatNode——》客户端）</li><li>继续通过元信息请求第二块数据。。。</li></ul><h1 id="NameNode与SecondaryNameNode工作机制"><a href="#NameNode与SecondaryNameNode工作机制" class="headerlink" title="NameNode与SecondaryNameNode工作机制"></a>NameNode与SecondaryNameNode工作机制</h1><ul><li>启动集群，加载edits（编辑日志）与fsimage（镜像文件）</li><li>元数据增删查改（客户端——》NameNode）</li><li>SecondaryNameNode请求是否需要CheckPoint（SecondaryNameNode——》NameNode）</li><li>SecondaryNameNode请求执行CheckPoint（SercondaryNameNode——》NameNode）</li><li>CheckPoint触发条件：1.通过定时；2.通过edits操作记录数量</li><li>CheckPoint触发时，SecondaryNameNode向NameNode拷贝edits文件并加载到内存（SercondaryNameNode——》NameNode）</li><li>生成新的镜像文件fsimage.chkpoint</li><li>将fsimage.chkpoint拷贝到NameNode（SercondaryNameNode——》NameNode）</li><li>NameNode将fsimage.chkpoint重命名为fsimage，再次发送到SecondaryNameNode当中（SercondaryNameNode——》NameNode）</li><li>NodeName与SencondaryNameNode将新的fsimage加载到内存中</li></ul><h1 id="HDFS重点"><a href="#HDFS重点" class="headerlink" title="HDFS重点"></a>HDFS重点</h1><ul><li>一、工作机制</li><li>二、读写流程</li><li>三、客户端api</li><li>四、集群部署安装</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>主从结构的单点故障</title>
      <link href="/2018/09/28/%E4%B8%BB%E4%BB%8E%E7%BB%93%E6%9E%84%E7%9A%84%E5%8D%95%E7%82%B9%E6%95%85%E9%9A%9C/"/>
      <url>/2018/09/28/%E4%B8%BB%E4%BB%8E%E7%BB%93%E6%9E%84%E7%9A%84%E5%8D%95%E7%82%B9%E6%95%85%E9%9A%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="主从结构的主出现故障时视为单点故障"><a href="#主从结构的主出现故障时视为单点故障" class="headerlink" title="主从结构的主出现故障时视为单点故障"></a><strong>主从结构的主出现故障时视为单点故障</strong></h1><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><blockquote><p>NameNode（主）+DataNode（从）</p></blockquote><h2 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h2><blockquote><p>ResourceManager（主）+NodeManager（从）</p></blockquote><h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><blockquote><p>HMaster（主）+RefionServer（从）</p></blockquote><h2 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h2><blockquote><p>nimbus（主）+supervisor</p></blockquote><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><blockquote><p>Master（主）+Worker（从）</p></blockquote><p>#<strong>单点故障的解决方法（HA）</strong></p><blockquote><p>使用zookeeper实现HA功能，当主节点（active）出现故障时，通过主节点（standby）操作HDFS</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>hive</title>
      <link href="/2018/09/26/hive/"/>
      <url>/2018/09/26/hive/</url>
      
        <content type="html"><![CDATA[<hr><h1 id="数据分析引擎"><a href="#数据分析引擎" class="headerlink" title="数据分析引擎"></a><strong>数据分析引擎</strong></h1><h2 id="一、Hadoop中"><a href="#一、Hadoop中" class="headerlink" title="一、Hadoop中"></a>一、Hadoop中</h2><ul><li>（1）Hive：支持SQL</li><li>（2）Pig：支持PigLation<h2 id="二、Spark中"><a href="#二、Spark中" class="headerlink" title="二、Spark中__"></a>二、Spark中__</h2></li><li>（*）Spark SQL：类似Hive，支持SQL、DSL<h2 id="三、另一个：Impala"><a href="#三、另一个：Impala" class="headerlink" title="三、另一个：Impala"></a>三、另一个：Impala</h2></li></ul><hr><h1 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a><strong>什么是Hive</strong></h1><h2 id="一、Hive是基于HDFS之上的一个数据仓库，有助于使用SQL"><a href="#一、Hive是基于HDFS之上的一个数据仓库，有助于使用SQL" class="headerlink" title="一、Hive是基于HDFS之上的一个数据仓库，有助于使用SQL"></a>一、Hive是基于HDFS之上的一个数据仓库，有助于使用SQL</h2><blockquote><p>Hive —-&gt; HDFS<br>表 ——&gt; 目录<br>数据 —-&gt; 文件<br>分区 —-&gt; 目录<br>桶 ——&gt; 文件</p></blockquote><h2 id="二、Hive是基于Hadoop之上的一个数据分析引擎"><a href="#二、Hive是基于Hadoop之上的一个数据分析引擎" class="headerlink" title="二、Hive是基于Hadoop之上的一个数据分析引擎"></a>二、Hive是基于Hadoop之上的一个数据分析引擎</h2><blockquote><p>Hive 2.X 以前：SQL —-&gt; Hive —-&gt; MapReduce<br>Hive 2.X 以后：推荐使用Spark作为SQL的执行引擎（只针对Hadoop 3.X以前）（《Hive on Spark文档》）</p></blockquote><hr><h1 id="Hive的体系架构"><a href="#Hive的体系架构" class="headerlink" title="Hive的体系架构"></a><strong>Hive的体系架构</strong></h1><blockquote><p>一、CLI（命令行）：直接由Hive Dirver翻译<br>二、JDBC（标准接口）：1.X由Thrift Server，2.X由Hive Server翻译为SQL语句交由Hive Dirver，端口号都为10000<br>三、HWI（Hive Web Interface）：只在Hive 2.2前提供HWI网页工具，推荐使用HUE，由Hive Dirver翻译<br>*、在Hive的体系架构中还需要有关系型数据库用来存储Hive元信息（推荐使用MySQL）</p></blockquote><hr><h1 id="Hive优缺点"><a href="#Hive优缺点" class="headerlink" title="Hive优缺点"></a><strong>Hive优缺点</strong></h1><ul><li>优点<blockquote><p>1）操作接口采用了sql，简化开发，减少学习成本<br>2）避免手写mapreduce程序<br>3）hive执行延迟较高，适用场景大多用在实时性要求不强的场景<br>4）处理大数据有优势<br>5）支持自定义函数</p></blockquote></li><li>缺点<blockquote><p>1）hive的sql表达能力有限（hql），并不能解决所有大数据场景<br>2）hive效率低（自动生成mapreduce作业，但力度比较粗，调优困难）<br>3）</p></blockquote></li></ul><hr><h1 id="安装和配置Hive"><a href="#安装和配置Hive" class="headerlink" title="安装和配置Hive"></a><strong>安装和配置Hive</strong></h1><h2 id="准备工作："><a href="#准备工作：" class="headerlink" title="准备工作："></a>准备工作：</h2>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">1、解压  tar -zxvf apache-hive-2.3.0-bin.tar.gz -C ~/training/</span><br><span class="line">2、设置环境变量 vi  ~/.bash_profile</span><br><span class="line">  HIVE_HOME=/root/training/hive</span><br><span class="line">  export HIVE_HOME</span><br><span class="line"> </span><br><span class="line">  PATH=$HIVE_HOME/bin:$PATH</span><br><span class="line">  export PATH</span><br><span class="line">3.安装配置MySQL数据库</span><br><span class="line">  在虚拟机上安装MySQL：</span><br><span class="line">    yum remove mysql-libs </span><br><span class="line">    rpm -ivh mysql-community-common-5.7.19-1.el7.x86_64.rpm</span><br><span class="line">    rpm -ivh mysql-community-libs-5.7.19-1.el7.x86_64.rpm</span><br><span class="line">    rpm -ivh mysql-community-client-5.7.19-1.el7.x86_64.rpm</span><br><span class="line">    rpm -ivh mysql-community-server-5.7.19-1.el7.x86_64.rpm</span><br><span class="line">    rpm -ivh mysql-community-devel-5.7.19-1.el7.x86_64.rpm  （可选，但还是装上，后面装HUE的时候会用到。）</span><br><span class="line">                   </span><br><span class="line">  启动MySQL：service mysqld start</span><br><span class="line">  或者：systemctl start mysqld.service</span><br><span class="line"> </span><br><span class="line">  查看root用户的密码：cat /var/log/mysqld.log | grep password</span><br><span class="line">  登录后修改密码：alter user &apos;root&apos;@&apos;localhost&apos; identified by &apos;Welcome_1&apos;;</span><br><span class="line"> </span><br><span class="line">  MySQL数据库的配置：</span><br><span class="line">    创建一个新的数据库：create database hive;</span><br><span class="line">    创建一个新的用户：</span><br><span class="line">      create user &apos;hiveowner&apos;@&apos;%&apos; identified by &apos;Welcome_1&apos;;</span><br><span class="line"></span><br><span class="line">      给该用户授权</span><br><span class="line">        grant all on hive.* TO &apos;hiveowner&apos;@&apos;%&apos;; </span><br><span class="line">        grant all on hive.* TO &apos;hiveowner&apos;@&apos;localhost&apos; identified by &apos;Welcome_1&apos;;          </span><br><span class="line">         </span><br><span class="line">        免费工具：http://www.mysqlfront.de/</span><br></pre></td></tr></table></figure><h2 id="嵌入模式：不需要MySQL，使用Hive自带的Derby数据库存储Hive的元信息"><a href="#嵌入模式：不需要MySQL，使用Hive自带的Derby数据库存储Hive的元信息" class="headerlink" title="嵌入模式：不需要MySQL，使用Hive自带的Derby数据库存储Hive的元信息"></a>嵌入模式：不需要MySQL，使用Hive自带的Derby数据库存储Hive的元信息</h2>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">特点：</span><br><span class="line">  （1）使用自带的Derby</span><br><span class="line">  （2）只支持一个连接</span><br><span class="line">  （3）用于开发和测试</span><br><span class="line"></span><br><span class="line">修改配置文件hive-env.sh：</span><br><span class="line">  HADOOP_HOME=path</span><br><span class="line">  export HIVE_CONF_DIR=/root/hd/hive/conf</span><br><span class="line"></span><br><span class="line">在HDFS集群上创建文件夹：</span><br><span class="line">  hdfs dfs -mkdir /tmp</span><br><span class="line">  hdfs dfs -mkdir /user/hive/warehouse/</span><br><span class="line"></span><br><span class="line">创建hive-site.xml：</span><br><span class="line">  &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">  &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">  &lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;jdbc:derby:;databaseName=metastore_db;create=true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">              </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;org.apache.derby.jdbc.EmbeddedDriver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;   </span><br><span class="line">             </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hive.metastore.local&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;   </span><br><span class="line"> </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;file:///root/training/apache-hive-2.3.0-bin/warehouse&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;           </span><br><span class="line">  &lt;/configuration&gt;</span><br><span class="line"> </span><br><span class="line">初始化MetaStore：</span><br><span class="line">    schematool -dbType derby -initSchema</span><br><span class="line">           </span><br><span class="line">日志：</span><br><span class="line">    Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br></pre></td></tr></table></figure><h2 id="本地模式、远程模式：都需要MySQL"><a href="#本地模式、远程模式：都需要MySQL" class="headerlink" title="本地模式、远程模式：都需要MySQL"></a>本地模式、远程模式：都需要MySQL</h2>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">修改配置文件hive-env.sh：</span><br><span class="line">  HADOOP_HOME=path</span><br><span class="line">  export HIVE_CONF_DIR=/root/hd/hive/conf</span><br><span class="line"></span><br><span class="line">在HDFS集群上创建文件夹：</span><br><span class="line">  hdfs dfs -mkdir /tmp</span><br><span class="line">  hdfs dfs -mkdir /user/hive/warehouse/</span><br><span class="line"></span><br><span class="line">创建hive-site.xml：</span><br><span class="line">  &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">  &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">  &lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;jdbc:mysql://localhost:3306/hive?useSSL=false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;               </span><br><span class="line"> </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;   </span><br><span class="line"> </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;hiveowner&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;   </span><br><span class="line"> </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;tiger&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;                   </span><br><span class="line">           </span><br><span class="line">  &lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">初始化MetaStore：</span><br><span class="line">  schematool -dbType mysql -initSchema</span><br><span class="line">  </span><br><span class="line">* 启动hive前先启动hadoop集群和yarn</span><br></pre></td></tr></table></figure><hr><h1 id="Hive的架构"><a href="#Hive的架构" class="headerlink" title="Hive的架构"></a><strong>Hive的架构</strong></h1><ul><li>提供了一系列接口：hive shell、jdbc/odbc、webui</li><li>hive架构：hive（Meta元数据（默认derby数据库，可以自定义）、Client（cli、idbc）、SQL Parser解析器、Physica编译器、Query优化器、Execution执行器、MR程序）</li><li>客户端输入sql后，首先调用到元数据</li><li>通过SQL Parser解析器找到对应的MR程序</li><li>经过编译器编译代码</li><li>通过优化其选择SQL需不需要经过计算</li><li>通过执行器执行MR程序</li></ul><hr><h1 id="Hive的表类型和数据类型"><a href="#Hive的表类型和数据类型" class="headerlink" title="*Hive的表类型和数据类型"></a><strong>*Hive的表类型和数据类型</strong></h1><h2 id="表类型"><a href="#表类型" class="headerlink" title="表类型"></a>表类型</h2><ul><li>一、内部表（管理表）：类似MySQL、Oracle中的表<blockquote><p>删除管理表时，hive会自动删除管理表的数据，不擅长做数据共享</p></blockquote></li><li>二、外部表<blockquote><p>删除外部表时，hive不认为这张表拥有这份数据，并不会删除数据，适合做数据共享<br>外部表已被删除时，重新创建一个表明、表结构都相同的表，hive将自动关联到外部表留下的数据</p></blockquote></li><li>三、分区表：提高性能<blockquote><p>（*）补充：如何提高性能？（SQL执行计划）</p></blockquote></li><li>四、桶表：类似Hash分区</li><li>五、视图：View<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2>| Java数据类型 | hive数据类型 | 数据长度 |<br>| byte        | TINYINT     | 1byte    |<br>| short       | SMALINT     | 2byte    |<br>| int         | INT         | 4byte    |<br>| long        | BIGINT      | 8byte    |<br>| float       | FLOAT       | 单精度浮点数 |<br>| double      | DOUBLE      | 双精度浮点数 |<br>| String      | STRING      | 字符      |<br>| | TIMESTAMP | 时间类型     |<br>| | BINARY    | 字节数组     |</li></ul><hr><h1 id="基础HQL"><a href="#基础HQL" class="headerlink" title="基础HQL"></a><strong>基础HQL</strong></h1><h2 id="DBL数据定义"><a href="#DBL数据定义" class="headerlink" title="DBL数据定义"></a>DBL数据定义</h2><ul><li>库操作<blockquote><p>alter database hive_db set dbproperties(‘dataname’=’zfhzxg的数据库’);：添加数据库描述信息<br>desc database hive_db;：查看数据库结构<br>desc database extended hive_db;：查看数据库拓展性<br>drop database if exists hive_db;：检测该库存在时，删除该库</p></blockquote></li><li>管理表操作<blockquote><p>show databases [link ‘db*’];：显示数据库，可选通配符筛选<br>create database hive_db location ‘path’;：在指定路径下创建数据库<br>create database if not exists hive_db;：检测该表不存在时，创建该表<br>desc formatted emp;：查看表类型</p></blockquote></li><li>外部表操作<blockquote><p>create table emp(id int,name string) row format[按行格式化] delimited fields[根据字段] terminated by “\t”[按空格切分];：创建表（默认创建管理表）<br>create external table if not exists emp2(id int,name string) row format delimited fields terminated by “\t”;：创建外部表<br>create table if not exists emp2 as select * from emp where name=”zfhzxg”;：emp2存在时，将emp中name为zfhzxg的字段传输到emp2中</p></blockquote></li><li>分区表操作<blockquote><p>create table if not exists emp_partition(id int,name string) partitioned by (day string) row format delimited fields terminated by “\t”;：创建分区表，以day为分区<br>select * from emp_partition where day=’1112’;：查询day分区为1112的数据<br>alter table emp_partition add partition(day=’1113’);：向emp_partition表中添加1113分区<br>alter table emp_partition drop partition(day=’1112’);：删除1112分区</p></blockquote></li><li>修改表<blockquote><p>alter table emptable rename to empt;：修改表名<br>alter table emp_partition add clumns(desc string);：向表中添加desc字段<br>alter table emp_partition change column desc descs int;：修改表中的字段<br>alter table emp_partition replace colums(name string,descs int);：替换表中的字段（重新定义，全部替换）</p></blockquote></li></ul><h2 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h2><ul><li>加载/导入、插入<blockquote><p>load data local inpath ‘path’ into table hive_db;：导入本地数据（追加）<br>load data inpath ‘path’ into table default.emp;：向default库中的emp表导入hdfs中的数据（追加，如果加载hdfs中的数据，源文件将会被剪切）<br>load data inpath ‘path’ overwrite into table default.emp;：导入hdfs中的数据（覆盖）<br>load data local inpath ‘path’ into table emp_partition partition(day=’1112’);：向1113分区中导入本地数据（追加）<br>insert into table emp_partition partition(day=’1112’) values(1,’zfhzxg’);：向分区表中的1112分区插入数据<br>create table if not exists emptable as select <em> from emp_partition where day=’1112’;：新创建一张表，并导入emp_partition表中1112分区中的数据<br>create table if not exists empta(id int,name string) row format delimited fields terminated by ‘\t’ location ‘path’;：创建表是关联数据，并不会剪切元数据<br>insert overwrite local directory ‘path’ select </em> from emp_partition where day=’1113’;：向本地路径（文件夹）导出empt_partition表中1113分区中的数据<br>dfs _get ‘path1’ ‘path2’;：在hive终端可以输入hdfs命令，可以用hadoop命令进行导出<br>[root@RedHat112 hive]# bin/hive -e “select from emp_partition where day=’1113’” &gt; /root/1113.txt ：将hive中表的数据导出成文件，用于结果导出，如果sql语句中包含单引号，需要用双引号包含sql语句</p></blockquote></li></ul><h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><ul><li>基础查询<blockquote><p>seltct * from emp;：全表查询<br>select emp.id,emp.name from emp;：查询指定字段<br>select emp.id [as] id,emp.name [as] name from emp;：自定义字段名</p></blockquote></li><li>算数运算符<blockquote><p>+（相加）<br>-（相减）<br>*（相乘）<br>/（相除）<br>%（取余）<br>&amp;（按位取与）<br>|（按位取或）<br>^（异或）<br>~（按位取反，只能用于四种整数类型）</p></blockquote></li><li>函数<blockquote><p>count（求行数）<br>max（最大）<br>min（最小）<br>sum（求和）<br>avg（平均值）<br>limit（查看前几条数据）：select * from emp limit 2;</p></blockquote></li><li>where<blockquote><p>select <em> from emp where id between 1 and 3;：查询id在1到3之间的人<br>select </em> from emp where id in(1,3);：查询id为1和3的人</p></blockquote></li><li>like：选择类似的值，选择条件可以包含字母和数字<blockquote><p>select <em> from emp where name like “_f%”;：查找name第二个字符为f的人<br>select </em> from emp where name rlike “[7]”;：rlike中可以使用正则表达式</p></blockquote></li><li>group by：分组<blockquote><p>select avg(empt.sal) avg_sal,deptno from empt group by deptno;：查询每个部门的工资平均值（按deptno分组）<br>select max(empt.sak) max_sal,deptno from empt group by deptno;：查询每个部门的最高薪水<br>select deptno,avg(sal) avg_sal from emp group by deptno having avg_sal&gt;1700;：查询每个部门的平均薪水大于1700的部门（在分组后添加条件时，用having）</p></blockquote></li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>HBase开发</title>
      <link href="/2018/09/21/HBase%E5%BC%80%E5%8F%91/"/>
      <url>/2018/09/21/HBase%E5%BC%80%E5%8F%91/</url>
      
        <content type="html"><![CDATA[<hr><p>###<strong>NoSQL简介</strong></p><ul><li>一、什么是NoSQL数据库？（not only sql）</li><li>二、常见NoSQL数据库<blockquote><p>HBase<br>Redis：基于内存的NoSQL数据库，前身MemCached（不支持持久化）<br>MongoDB：基于文档型（BSON）的NoSQL数据库<br>Cassandra：跟HBase类似</p></blockquote></li></ul><hr><hr><p>###<strong>HBsae的体系架构（主从结构）和表结构</strong></p><ul><li>一、Hadoop的生态体系圈</li><li><p>二、HBase</p><blockquote><p>基于HDFS之上的NoSQL数据库</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HBase      HDFS</span><br><span class="line">表    ---&gt; 目录</span><br><span class="line">数据  ---&gt; 文件（HFile，默认大小：128MB）</span><br></pre></td></tr></table></figure></blockquote></li><li><p>三、HBase的体系架构（主节点：HMaster + 从节点：RegionServer）</p><blockquote><p>单点故障</p><blockquote><p>通过Zookeeper</p></blockquote></blockquote></li></ul><hr><hr><p>###<strong>HBase的搭建模式</strong></p><ul><li>一、本地模式</li><li>二、伪分布模式</li><li>三、全分布模式</li><li>四、实现HBsae的HA</li></ul><hr><hr><p>###<strong>操作HBsae</strong></p><ul><li>一、</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Hadoop免密码登陆原理</title>
      <link href="/2018/09/14/Hadoop%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E9%99%86%E5%8E%9F%E7%90%86/"/>
      <url>/2018/09/14/Hadoop%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E9%99%86%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>###<strong>hadoop免密码登陆原理</strong></p><ul><li>*不对称密码<blockquote><p>密匙对（两个文件）：</p><blockquote><p>公钥–锁（.ssh/id_rsa）：给别人加密<br>私钥–钥匙（.ssh/id_rsa.pub）：给自己，解密</p></blockquote></blockquote></li><li><p>*对称加密（加密和解密使用同一文件）</p></li><li><p>一、生成密钥对（请求登陆方-A）</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure></li><li><p>二、把请求登陆方生成的公钥拷贝给被请求方（B）</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i .ssh/id_rsa.pub root@RedHat111</span><br></pre></td></tr></table></figure></li><li><p>三、B收到请求方发来的公钥，并自动保存在.ssh/authorized_keys文件</p></li><li><p>四、B随机产生一个字符串：hello（前三部是配置，第四步开始，是认证的过程）</p></li><li><p>五、B使用请求登陆方的公钥进行加密（<strong>*</strong>），并发回给A</p></li><li><p>六、A收到B发来的加密字符串，使用自己的私钥进行解密（hello）</p></li><li><p>七、A把解密后的字符串（hello）发回给B进行认证</p></li><li><p>八、B收到A解密的字符串（hello）</p></li><li><p>九、B对从A收到的字符串（hello）与自己生成的字符串（hello）进行对比</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>MapReduce笔记</title>
      <link href="/2018/09/10/MapReduce%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/09/10/MapReduce%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<hr><p>#<strong>MapReduce高级特性</strong></p><h2 id="一、序列化"><a href="#一、序列化" class="headerlink" title="一、序列化"></a>一、序列化</h2><ul><li>核心接口：Writable</li><li>如果一个类实现了Writable该类的对象可以作为Key和Value<h2 id="二、排序"><a href="#二、排序" class="headerlink" title="二、排序"></a>二、排序</h2></li><li>规则：按照Key2排序（可以是基本数据类型，也可以是对象）<blockquote><p>基本数据类型：数字（默认升序），字符串（默认字典顺序）<br>可以通过创建自己的比较规则改变排序（extends IntWritable.Comparator/extends Text.Comparator）</p></blockquote></li><li>对象<blockquote><p>SQL排序：order by 列名、表达式、别名、序号 desc/asc（desc/asc只作用于最近的一列）<br>MapReduce排序：1.该对象必须是Key2；2.必须实现序列化接口Writable；3.对象必须是可排序的（自定义排序使用java.long 接口 Comparable）</p></blockquote></li><li>MR排序分类<blockquote><p>部分排序<br>全排序<br>辅助排序<br>二次排序</p></blockquote><h2 id="三、分区"><a href="#三、分区" class="headerlink" title="三、分区"></a>三、分区</h2></li><li>什么是分区：partition</li><li>查询：<blockquote><p>1.没有分区：执行全表扫描<br>2.有分区，只扫描分区</p></blockquote></li><li>分区的类型：<blockquote><p>Orcale：</p><blockquote><p>1.范围分区；<br>2.列表分区；<br>3.Hash分区；<br>4.Hash范围分区；<br>5.Hash列表分区<br>MR的分区：<br>默认情况下，MR的输出只有一个分区（一个分区就是一个文件）<br>自定义分区：按照字段进行分区（根据Map的输出&lt;Key2,Value2&gt;分区）</p></blockquote></blockquote></li><li>*通过SQL的执行计划，判断效率是否提高<h2 id="四、合并"><a href="#四、合并" class="headerlink" title="四、合并"></a>四、合并</h2></li><li>合并是一种特殊的Reduce</li><li>合并是在Map端执行一次合并，用于减少Mapper输出到Reduce的数据量，可以提高效率</li><li>平均值不能使用combiner</li><li>无论有没有combiner，都没不能改变Map和Reduce对应的数据类型<h2 id="MapReduce核心：Shuffle（洗牌）"><a href="#MapReduce核心：Shuffle（洗牌）" class="headerlink" title="*MapReduce核心：Shuffle（洗牌）"></a>*MapReduce核心：Shuffle（洗牌）</h2></li><li>Hadoop3.X之前会有数据落地（产生I/O操作）</li><li>map()方法写入数据到环形缓冲区</li><li>环形缓冲区达到80%后，发生溢写，进行分区、排序、合并（combiner可选）、归并</li><li>归并后，拷贝到内存缓冲</li><li>当内存不够时，溢出到磁盘</li><li>进行归并排序</li><li>相同key分组</li><li>传入Reduce()<h2 id="MapReduce优缺点"><a href="#MapReduce优缺点" class="headerlink" title="*MapReduce优缺点"></a>*MapReduce优缺点</h2></li><li>优点<blockquote><p>1.易于编程<br>2.良好的拓展性<br>3.高容错性<br>4.适合处理PB级别以上的离线处理</p></blockquote></li><li>缺点<blockquote><p>1.不擅长做实时计算<br>2.不擅长做流式计算（MR的数据源事静态的）<br>3.不支持DAG（有向图）计算（Spark）</p></blockquote></li></ul><hr><hr><p>#<strong>Mapper，Reduce和Driver</strong></p><ul><li><p>Mapper阶段：</p><blockquote><p>用户自定义Mapper类，要继承父类Mapper<br>Mapper的输入数据的kv对形式（kv类型可以自定义）<br>Mapper的map方法的重写（加入业务逻辑）<br>Mapper的数据输出kv对的形式（kv类型可以自定义）<br>map（）方法（maptext进程）对每个&lt;k,v&gt;只调用一次</p></blockquote></li><li><p>Reducer阶段；</p><blockquote><p>用户自定义Reduce类，要继承父类Reducer<br>Reducer的数据输入类型对应的是Mapper九段的输出数据类型<br>Reducer的reduce方法重写（加入业务逻辑）<br>ReduceText进程对每组的k的&lt;k,v&gt;组只调用一次</p></blockquote></li><li><p>Driver阶段</p><blockquote><p>mr程序需要一个Driver来进行任务的提交，提交的任务是一个描述了各种重要信息的job</p></blockquote></li><li><p>maptask流程</p><blockquote><p>并行度：<br>一个job任务map阶段并行度由客户端所提交的任务决定<br>每一个split分配一个maptask并行处理<br>默认情况下，split大小=blocksize<br>切片是针对每一个文件单独切片<br>流程：<br>准备数据wordcount<br>创建客户端，提交任务程序driver<br>逻辑运算<br>向环形缓冲区写数据&lt;k,v&gt;（默认大小100M）<br>当环形缓冲区内存占用达到80%，进行溢写（HashPratitioner分区，key.compareTo排序（索引））<br>溢写到文件（保证分区且区内是有序的）<br>merge归并排序</p></blockquote></li><li><p>reducetask流程</p><blockquote><p>reduceTask将相同分区的数据下载到reduceTask本地磁盘<br>再次合并文件，归并排序<br>合并过程中进行辅助排序<br>一次读一组，进行写出，生成结果文件</p></blockquote></li></ul><hr><hr><p>#<strong>Hadoop中所提供的数据序列化类型</strong></p><ul><li>int         &gt; IntWritable</li><li>float       &gt; FloatWritable</li><li>long        &gt; LongWritable</li><li>double      &gt; DoubleWritable</li><li>String      &gt; Text</li><li>boolean     &gt; BooleanWritable</li><li>byte        &gt; ByteWritable</li><li>Map         &gt; MapWritable</li><li>Arry        &gt; ArryWritable</li></ul><blockquote><p>为什么要序列化？<br>存储“活的对象”<br>什么是序列化？<br>就是把内存中的对象，转换成字节序列以便于存储和网络传输<br>反序列化<br>就是将收到的字节序列或者硬盘的持久化数据，转换成内存中的对象<br>Java中的序列化<br>Serializable<br>为什么不用Java提供的序列化接口？<br>Java的序列化是一个重量级的序列化，一个对象被序列化后会附带很多额外的信息（校验信息，Header，继承体系等），不便于在网络中的高效传输，所以Hadoop开发了一套序列化鸡之（Writable），精简/高效<br>为什么序列化在Hadoop中很重要？<br>Hadoop通信是通过远程调用（rpc）实现的，需要进行序列化<br>特点：<br>1）紧凑<br>2）快速<br>3）可拓展<br>4）互操作</p></blockquote><hr><hr><p>#<strong>MapReduce常用案例</strong></p><ul><li>一、数据去重<blockquote><p>相同Key名交给同一地址进行处理，处理后输出给Reduce，Key值唯一，Value形成数组</p></blockquote></li><li>二、多表查询（笛卡尔集：列数相加，行数相乘）<blockquote><p>等值链接的处理过程（以表作为Value1）</p><blockquote><p>Mapper阶段，通过分词后的列数或其他方法区分表<br>Mapper阶段，在字段前添加标识输出给Reduce<br>Reduce进行处理</p></blockquote></blockquote></li><li>三、自连接<blockquote><p>同一个表经Mapper输出两次<br>非法数据要先经过处理（数据清理）<br>Reduce进行处理</p></blockquote></li><li>四、倒排索引</li><li>五、单元测试（MRUnit）</li></ul><hr><hr><p>#<strong>MapReduce重点</strong></p><ul><li>一、WordCount案例、流量汇总案例与涉及知识点</li><li>二、yran集群部署安装</li><li>三、job任务提交流程</li><li>四、辅助排序、分区排序</li><li>五、MapReduce整体流程</li><li>六、数据压缩</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Hadoop 2.X管理与开发（一）</title>
      <link href="/2018/09/05/Hadoop-2-X%E7%AE%A1%E7%90%86%E4%B8%8E%E5%BC%80%E5%8F%91%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2018/09/05/Hadoop-2-X%E7%AE%A1%E7%90%86%E4%B8%8E%E5%BC%80%E5%8F%91%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="hadoop的起源与背景知识"><a href="#hadoop的起源与背景知识" class="headerlink" title="hadoop的起源与背景知识"></a><strong>hadoop的起源与背景知识</strong></h1><h2 id="一、什么是大数据"><a href="#一、什么是大数据" class="headerlink" title="一、什么是大数据"></a>一、什么是大数据</h2><blockquote><p>举例:</p><blockquote><p>1.电商的推荐系统(可能会用到推荐算法:协同过滤,ALS,逻辑回归…)<br>2.天气预报</p></blockquote></blockquote><blockquote><p>核心的问题:数据的存储,数据的计算(不是算法)</p></blockquote><blockquote><p>数据的存储:采用分布式的文件系统HDFS(hadoop Distributed file system)</p></blockquote><blockquote><p>数据的计算:采用分布式的计算MapReduce,Spark(RDD:弹性分布式数据集)</p></blockquote><h2 id="二、数据仓库和大数据"><a href="#二、数据仓库和大数据" class="headerlink" title="二、数据仓库和大数据"></a>二、数据仓库和大数据</h2><h3 id="传统方式-搭建数据仓库-Data-Werehouse-来解决大数据的问题"><a href="#传统方式-搭建数据仓库-Data-Werehouse-来解决大数据的问题" class="headerlink" title="传统方式:搭建数据仓库(Data Werehouse)来解决大数据的问题"></a>传统方式:搭建数据仓库(Data Werehouse)来解决大数据的问题</h3><blockquote><p>1.数据仓库就是一个数据库(Orcale,MySQL,MS)<br>2.数据仓库和大数据一般只做查询(分析)<br>3.搭建数据仓库的过程</p><blockquote><p>(1)数据源RDBMS:关系型数据库(结构化数据)/文本数据/其他数据<br>(2)利用ETL抽取/转化/加载数据后搭建数据仓库(保存为原始数据)<br>(3)分析处理数据(SQL,PL/SQL,JDBC)<br>(4)经过分析后搭建数据集市(hr,sales)<br>(5)提供给hr系统或销售系统</p></blockquote></blockquote><h3 id="Hadoop和Spark都可以看成是数据仓库的一种实现"><a href="#Hadoop和Spark都可以看成是数据仓库的一种实现" class="headerlink" title="Hadoop和Spark都可以看成是数据仓库的一种实现"></a>Hadoop和Spark都可以看成是数据仓库的一种实现</h3><blockquote><p>1.从ETL到搭建数据集都可以用Hadoop和Spark所提供的方式解决,也可以用传统方式解决<br>2.hadoop中使用sqoop和Flume组件完成ETL<br>3.hadoop中使用HDFS存储数据(或存入基于HDFS之上的HBase或Hive数据仓库)<br>4.hadoop中使用MapReduce(java程序)或Spark(Scala程序,只有数据计算,没有数据存储)或SQL处理数据<br>5.处理数据后使用HDFS或NoSQL:Redis存储到数据集市<br>*.大数据的终极目标是使用SQL处理数据</p></blockquote><h2 id="三、OLTP和OLAP"><a href="#三、OLTP和OLAP" class="headerlink" title="三、OLTP和OLAP"></a>三、OLTP和OLAP</h2><blockquote><p>LTP:Online Transaction processing 联机事务处理，指：insert、update、delete —&gt; 事务</p></blockquote><blockquote><p>OLAP:Online Analytic Processing 联机分析处理，一般只做查询 —&gt; 数据仓库就是一种OLAP的应用系统</p></blockquote><blockquote><p>Hadoop、Spark看成是一种数据仓库的解决方案</p></blockquote><blockquote><p>数据仓库（查询）不支持事务</p></blockquote><h2 id="四、Google的基本思想-三篇论文"><a href="#四、Google的基本思想-三篇论文" class="headerlink" title="四、Google的基本思想:三篇论文"></a>四、<strong>Google的基本思想:三篇论文</strong></h2><h3 id="一-GFS（Google-File-System）-—-HDFS（Hadoop-Distributed-File-System）：分布式文件系统"><a href="#一-GFS（Google-File-System）-—-HDFS（Hadoop-Distributed-File-System）：分布式文件系统" class="headerlink" title="(一)GFS（Google File System）  —- HDFS（Hadoop Distributed File System）：分布式文件系统"></a>(一)GFS（Google File System）  —- HDFS（Hadoop Distributed File System）：分布式文件系统</h3><blockquote><p>(*) HDFS = NameNode + SecondaryNameNode + DataNode<br>1.分布式文件系统<br>2.大数据的存储问题<br>3.HDFS中，记录数据的位置信息（元信息） —- 采用倒排索引（Reverted Index）</p><blockquote><p>(1)什么是索引？Index<br>    * CREATE INDEX创建索引<br>    * 索引就是一个记录（Oracle中索引表保存的是有规律的行地址）<br>    * 通过索引可以找到对应的数据<br>(2)什么是倒排索引？<br>    * 最简单的倒排索引：单词表（wordID，word，index）<br>(3)NameNode（主节点，名称节点）是整个HDFS的管理员，和SecondaryNameNode（第二名称节点）同处一台主机，负责管理DateNode（从节点，数据节点），并不负责存储，与DateNode（从节点，数据节点）构成HDFS环境</p></blockquote></blockquote><h3 id="二-MapReduce：分布计算模型，问题来源PageRank（网页排名）"><a href="#二-MapReduce：分布计算模型，问题来源PageRank（网页排名）" class="headerlink" title="(二)MapReduce：分布计算模型，问题来源PageRank（网页排名）"></a>(二)MapReduce：分布计算模型，问题来源PageRank（网页排名）</h3><blockquote><p>1.PageRank（网页排名）<br>2.MapReduce的标程模型：</p><blockquote><p>*. 核心：先拆分（拆分计算，Map阶段），再合并（Reduce阶段）<br>*. MR任务：job=map+reduce<br>*. Map的输出同时也是Reduce的输入<br>*. 一个MR任务一共存在四对输入和输出（<key value="">），Map的输入和输出，Reduce的输入和输出<br>*. k2=k3，v2和v3数据类型一致，v3是一个集合，该集合中的每个值就是v2<br>*. 所有的<key value="">数据类型必须是Hadoop自己的数据类型（为了实现Hadoop的序列化机制）<br>*. MR任务处理的是HDFS上的数据<br>*. Hadoop2.X开始，通过Yarn容器编程部署MR任务（ResourceManager&lt;主节点&gt; + NodeManager&lt;从节点&gt;）<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Example:/root/training/Hadoop-2.7.3/Share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar</span><br><span class="line">Yarn的web console:http://192.168.226.11:8088</span><br><span class="line">命令://hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount /input/data.txt /out/wcl</span><br></pre></td></tr></table></figure></key></key></p></blockquote></blockquote><h3 id="三-BigTable：大表-—-NoSQL数据库：HBase"><a href="#三-BigTable：大表-—-NoSQL数据库：HBase" class="headerlink" title="(三)BigTable：大表 —- NoSQL数据库：HBase"></a>(三)BigTable：大表 —- NoSQL数据库：HBase</h3><blockquote><p>1.关系型数据库：以二维表的形式保存数据<br>2.大表的基本思想：所有的数据存入一张表（通过牺牲空间，换取时间）<br>3.常见的NoSQL数据库（一般为行式数据库，适合）</p><blockquote><p>* Redis：内存数据库（一般为行式数据库，适合DML操作，insert，update，delect）<br>* MongoDB：面向文档（BSON文档：是JSON的二进制）<br>* HBase：面向列（列式数据库，查询，select，在HBase中，rowKey不能为null，但是可以重复，相同的ey是一条记录）<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBase = ZooKeeper + HMaster（主节点） + RegionServer（从节点）</span><br></pre></td></tr></table></figure></p></blockquote></blockquote><hr><h1 id="hadoop的环境"><a href="#hadoop的环境" class="headerlink" title="hadoop的环境"></a><strong>hadoop的环境</strong></h1><h2 id="一、Hadoop的目录结构"><a href="#一、Hadoop的目录结构" class="headerlink" title="一、Hadoop的目录结构"></a>一、Hadoop的目录结构</h2><blockquote><p>hadoop-2.7.3/  —&gt;  Hadoop的HOME目录<br>bin/  —&gt;  Hadoop的操作命令<br>etc/hadoop/  —&gt;  所有的配置文件<br>sbin/  —&gt;  Hadoop集群的命令：启动，停止等<br>share/  —&gt;  所有共享文件<br>share/hadoop  –&gt;  所有依赖jar包</p></blockquote><h2 id="二、Hadoop的三种安装模式"><a href="#二、Hadoop的三种安装模式" class="headerlink" title="二、Hadoop的三种安装模式"></a>二、Hadoop的三种安装模式</h2><h3 id="一-本地模式"><a href="#一-本地模式" class="headerlink" title="(一)本地模式"></a>(一)本地模式</h3><blockquote><p>* 没有HDFS，只能测试MapReduce程序（不是运行在Yarn中，作为一个独立的Java程序来运行）<br>* 配置文件：etc/Hadoop/hadoop-env.sh export JAVA_HOME={JAVA_HOME} 自己的JAVA_HOME路径<br>* 本地模式中用到的路径都是本地路径，因为没有HDFS</p></blockquote><h3 id="二-伪分布模式"><a href="#二-伪分布模式" class="headerlink" title="(二)伪分布模式"></a>(二)伪分布模式</h3><blockquote><p>* 特点：再单击上模拟一个分布式的环境，具备Hadoop的所有功能<br>* 具备HDFS：NameNode + DataNode + SecondaryNameNode（端口50070）<br>* 具备Yarn：ResourceManager + NodeManager（端口8088）<br>* 对HDFS的NameNode进行格式化（/root/training/hadoop-2.7.3/tmp）<br>* 启动HDFS：start-dfs.sh<br>* 启动Yarn：start-yarn.sh<br>* 统一启动：start-all.sh<br>* 配置文件：etc/Hadoop/hadoop-env.sh export JAVA_HOME={JAVA_HOME} 自己的JAVA_HOME路径<br>* 配置文件： </p></blockquote>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">//hdfs-site.xml:</span><br><span class="line"></span><br><span class="line">//配置数据块的冗余度</span><br><span class="line">//原则冗余度跟数据节点的个数保持一致，最大不要超过3</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">//是否开启HDFS权限检查，默认值为true（使用默认值，需要再改）</span><br><span class="line">&lt;!--</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.permissions&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//core-site.xml:</span><br><span class="line"></span><br><span class="line">//配置HDFS主节点的地址，就是NameNode的地址</span><br><span class="line">//9000是RPC的通信端口</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://RedHat111:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">//HDFS数据块和元信息保存在操作系统的目录位置</span><br><span class="line">//默认值是Linux的tmp目录</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/root/training/hadoop-2.7.3/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//mapred-site.xml:（默认没有这个文件）</span><br><span class="line">//MR程序运行的程序或框架</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//yarn-site.xml:</span><br><span class="line">//配置yarn主节点的位置</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;RedHat111&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h3 id="三-全分布模式"><a href="#三-全分布模式" class="headerlink" title="(三)全分布模式"></a>(三)全分布模式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">(1)hdfs-site.xml:</span><br><span class="line">    &lt;!—配置数据块的冗余度，默认是3—&gt;</span><br><span class="line">    &lt;!—原则冗余度跟数据节点保持一致，最大不要超过3—&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dis.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!—是否开启HDFS权限检查，默认是true—&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dis.permissions&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt; </span><br><span class="line">    </span><br><span class="line">(2)core-site.xml:</span><br><span class="line">    &lt;!—配置HDFS主节点位置，就是NameNode的位置—&gt;</span><br><span class="line">    &lt;!—9000是RPC的通信端口—&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://RedHat112:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!—HDFS元信息和数据块保存在操作目录的位置—&gt;</span><br><span class="line">    &lt;!—默认是系统的tmp文件夹，会随断电而清除—&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/root/training/hadoop-2.7.3/tnp&lt;/value&gt;</span><br><span class="line">    &lt;/porperty&gt;</span><br><span class="line">    </span><br><span class="line">(3)mapped-site.xml</span><br><span class="line">    &lt;!—MR运行容器或框架—&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">(4)yarn-site.xml</span><br><span class="line">    &lt;!—配置yarn主节点的位置—&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;RedHat112&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!—NodeManager执行MR任务的方式是Shuffle洗牌—&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">(4)slaves配置从节点地址:</span><br><span class="line">    RedHat113</span><br><span class="line">    RedHat114</span><br><span class="line">    </span><br><span class="line">(5)对NameNode进行格式化</span><br><span class="line"></span><br><span class="line">(6)把RedHat上安装好的目录复制到从节点上</span><br><span class="line">    scp -r hadoop-2.7.3/ root@RedHat113:/root/training</span><br><span class="line">    scp -r hadoop-2.7.3/ root@RedHat114:/root/training</span><br><span class="line">    </span><br><span class="line">(7)在主节点上启动集群</span><br><span class="line">    start-all.sh</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Hadoop 2.X管理与开发（二）</title>
      <link href="/2018/09/05/Hadoop-2-X%E7%AE%A1%E7%90%86%E4%B8%8E%E5%BC%80%E5%8F%91%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2018/09/05/Hadoop-2-X%E7%AE%A1%E7%90%86%E4%B8%8E%E5%BC%80%E5%8F%91%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>#Hadoop数据压缩</p><h2 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h2><ul><li>1）MR操作过程中进行大量数据传输，就需要对数据进行压缩</li><li>2）压缩技术能够有效减少底层存储（HDFS）读写字节数，提高的网络带宽和磁盘空间的效率</li><li>3）数据压缩能够有效节省资源</li><li>4）压缩事MR程序的优化策略</li><li>5）通过压缩编码对Mapper或者reduce数据传输进行的压缩，以减少磁盘IO</li></ul><h2 id="压缩的基本原则"><a href="#压缩的基本原则" class="headerlink" title="压缩的基本原则"></a>压缩的基本原则</h2><ul><li>1）运算密集型任务少用压缩</li><li>2）IO密集型的任务，多用压缩</li></ul><h2 id="MR支持的压缩编码"><a href="#MR支持的压缩编码" class="headerlink" title="MR支持的压缩编码"></a>MR支持的压缩编码</h2><ul><li>DEFAULT    是自带编码    .default    不可切分</li><li>Gzip       是自带编码    .gz         不可切分</li><li>bzip2      是自带编码    .bz2        可以切分</li><li>LZO        非自带编码    .lzo        可以切分</li><li>Snappy     非自带编码    .Snappy     不可切分</li></ul><h2 id="编码-解码器"><a href="#编码-解码器" class="headerlink" title="编码/解码器"></a>编码/解码器</h2><p>DEFAULT       org.apache.hadoop.io.compress.DefaultCodeC<br>Gzip          org.apache.hadoop.io.compress.GzipCodeC<br>bzip2         org.apache.hadoop.io.compress.BZip2CodeC<br>LZO           com.hadoop.compression.lzo.lzoCodeC<br>Snappy        org.apache.hadoop.io.compress.SnappyCodeC</p><h2 id="压缩性能"><a href="#压缩性能" class="headerlink" title="压缩性能"></a>压缩性能</h2><p>Gzip      原大小：8.3GB      压缩后：1.8GB      压缩速度：17.5MB/s      解压速度：58MB/s<br>bzip2     原大小：8.3GB      压缩后：1.1GB      压缩速度：2.4MB/s       解压速度：9.5MB/s<br>LZO       原大小：8.3GB      压缩后：2.9GB      压缩速度：49.3MB/s      解压速度：74.6MB/s</p><h2 id="设置压缩方式"><a href="#设置压缩方式" class="headerlink" title="设置压缩方式"></a>设置压缩方式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mapper端：</span><br><span class="line">    //开启map端的输出压缩</span><br><span class="line">conf.setBoolean(&quot;mapreduce.map.outpot.compress&quot;, true);</span><br><span class="line">//设置压缩方式</span><br><span class="line">//conf.setClass(&quot;mapreduce.map.outpot.compress.codec&quot;, DefaultCodec.class, CompressionCodec.class);</span><br><span class="line">conf.setClass(&quot;mapreduce.map.outpot.</span><br><span class="line">        compress.codec&quot;, BZip2Codec.class, CompressionCodec.class);</span><br><span class="line"></span><br><span class="line">reduce端：</span><br><span class="line">    //开启reduce端的输出压缩</span><br><span class="line">FileOutputFormat.setCompressOutput(job, true);</span><br><span class="line">//设置压缩方式</span><br><span class="line">//FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);</span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);</span><br></pre></td></tr></table></figure><h1 id="Hadoop优化"><a href="#Hadoop优化" class="headerlink" title="Hadoop优化"></a>Hadoop优化</h1><h2 id="MapReduce程序的效率瓶颈"><a href="#MapReduce程序的效率瓶颈" class="headerlink" title="MapReduce程序的效率瓶颈"></a>MapReduce程序的效率瓶颈</h2><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li>MR功能：分布式离线计算</li><li>计算机性能：CPU、内存、磁盘、网络</li><li>I/O操作优化：<blockquote><p>数据倾斜（代码优化）<br>map和reduce的个数设置不合理<br>map运行时间太长，导致reduce等待时间过久<br>小文件过多（CombineTextInputFormat小文件合并）<br>不可分快的超大文件（不断的溢写）<br>多个溢写小文件需要多次merge</p></blockquote><h3 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h3></li><li>数据输入<blockquote><p>合并小文件：在执行MR任务前就进行小文件合并<br>采用CombineTextInputFormat来作为输入来作为输入端大量小文件的场景</p></blockquote></li><li><p>Map阶段</p><blockquote><p>减少溢写次数（增加内存200MB 80%）：减少磁盘I/O</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.task.io.sort.mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;200&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.map.sort.spill.percent&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0.80&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>减少合并的次数</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.task.io.sort.factor&lt;/name&gt;</span><br><span class="line">&lt;value&gt;10&lt;/value&gt; //文件的个数，数值越大合并次数越少</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>在map之后，不影响业务逻辑的情况下可以使用combiner</p></blockquote></li><li><p>Reduce阶段</p><blockquote><p>合理的设置map与reduce的个数<br>设置map/reduce共存</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.job.reduce.skowstart.completedmaps&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0.05&lt;/value&gt; //设置运行一定程度的map后，启动reduce，减少等待时间</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>合理设置reduce的buffer</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.reduce.markreset.buffer.percent&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0.0&lt;/value&gt; </span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote></li><li><p>I/O传输</p><blockquote><p>进行数据压缩<br>使用sequenceFile</p></blockquote></li><li>数据倾斜<blockquote><p>进行范围分区<br>自定义分区<br>Combine<br>能用mapJoin的坚决不用reduceJoin</p></blockquote></li><li><p>参数调优</p><blockquote><p>分配map程序CPU核心数</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.map.cpu.vcores&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1&lt;/value&gt; //核心数</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>分配reduce程序CPU核心数</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.reduce.cpu.vcores&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1&lt;/value&gt; //核心数</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>设置maptask内存</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1024&lt;/value&gt; //一个maptask能够使用的内存上限</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>设置reducetask内存</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1024&lt;/value&gt; //一个maptask能够使用的内存上限</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>reduce去map端并行度</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.reduce.shuffle.parallelcopies&lt;/name&gt;</span><br><span class="line">&lt;value&gt;5&lt;/value&gt; //当reduce去map端拿取数据时所开的并行数是5</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>zookeeper</title>
      <link href="/2018/09/05/zookeeper/"/>
      <url>/2018/09/05/zookeeper/</url>
      
        <content type="html"><![CDATA[<h1 id="zookeeper简介"><a href="#zookeeper简介" class="headerlink" title="zookeeper简介"></a>zookeeper简介</h1><ul><li>zookeeper 动物管理员<blockquote><p>apache zookeeper致力于开发和维护开源服务器，实现高度可靠的分布式协调</p></blockquote></li><li>什么是zookeeper<blockquote><p>zookeeper是一种集中式服务，用于维护配置信息，命名，提供分布式同步和提供组服务。所有这些类型的服务都以分布式应用程序的某种形式使用，每次实施他们都需要做很多工作来修复不可避免的错误和竞争条件。由于难以实现这些类型的服务，应用程序最初通常会吝啬他们，这使得他们在变化的情况下变得脆弱并且难以管理。即使正确完成，这些服务的不同实现也会在部署应用程序时导致管理复杂性。</p></blockquote></li></ul><h1 id="zookeeper功能"><a href="#zookeeper功能" class="headerlink" title="zookeeper功能"></a>zookeeper功能</h1><ul><li>1）存储数据：存储集群中每台机器都关心的数据（配置信息），需要接受服务器的注册</li><li>2）监听</li></ul><h1 id="zookeeper工作机制"><a href="#zookeeper工作机制" class="headerlink" title="zookeeper工作机制"></a>zookeeper工作机制</h1><ul><li>基于观察者模式设计的分布式服务管理框架</li><li>1）启动服务器，首先向zookeeper中注册信息，创建临时节点（通过目录结构/servers/）</li><li>2）获取服务器列表并且注册监听（当服务器下线，在zookeeper中对应服务器节点将会消失）</li><li>3）当服务器宕机，zookeeper将会通过监听功能（process(){}回调方法）向zookeeper发送下线通知</li><li>4）监听功能重新获取服务器列表并再次进行监听</li></ul><h1 id="zookeeper的存储结构（目录树存储结构）"><a href="#zookeeper的存储结构（目录树存储结构）" class="headerlink" title="zookeeper的存储结构（目录树存储结构）"></a>zookeeper的存储结构（目录树存储结构）</h1><ul><li>/ ——–&gt; 根目录（根目录下可以有多个节点）</li><li>z1、z2 ——–&gt; 节点目录（Znode，默认存储1M数据）</li><li>/z1/zz1 、 /z1/zz2 、 /z2/zz1</li></ul><h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><ul><li>1）集群同一配置管理</li><li>2）集群同一命名服务</li><li>3）集群统一管理</li><li>4）服务器的动态上下线感知</li><li>5）负载均衡</li></ul><h1 id="zookeeper集群安装"><a href="#zookeeper集群安装" class="headerlink" title="zookeeper集群安装"></a>zookeeper集群安装</h1><ul><li>单节点安装<blockquote><p>1）解压安装包<br>2）重命名zookeeper/conf/zoo_sample.cfg 为 zoo.cfg<br>3）修改zoo.cfg配置文件中的dataDir参数为自定义路径<br>4）启动zk：bin/zkServer.sh start<br>5）查看状态：bin/zkServer.sh status<br>6）启动客户端：bin/zkCli.sh</p></blockquote></li><li>全分布安装  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1）zoo.cfg：</span><br><span class="line">    server.1=RedHat112:2888:3888</span><br><span class="line">    server.2=RedHat113:2888:3888</span><br><span class="line">    server.3=RedHat114:2888:3888</span><br><span class="line"></span><br><span class="line">2）在自定义dataDir路径下创建myid文件，并设置id</span><br><span class="line">myid：</span><br><span class="line">    1</span><br><span class="line"></span><br><span class="line">3）配置环境变量</span><br><span class="line"></span><br><span class="line">4）将环境变量和zookeeper发送到其他服务器</span><br><span class="line">scp -zxvf -r zookeeper RedHat113:zookeeper</span><br><span class="line">scp -zxvf -r zookeeper RedHat114:zookeeper</span><br><span class="line"></span><br><span class="line">5)修改其他服务器myid</span><br><span class="line"></span><br><span class="line">6)生效环境变量</span><br></pre></td></tr></table></figure></li></ul><h1 id="zookeeper操作"><a href="#zookeeper操作" class="headerlink" title="zookeeper操作"></a>zookeeper操作</h1><ul><li>启动shell客户端：zkCli.sh</li><li>zk：操作日志</li><li>ls path [watch]：查看path节点所包含的内容[监听]</li><li>ls2 path [watch]：查看path节点的详细信息[监听]</li><li>gat path [watch]：查看path节点所存储的数据<blockquote><p>get path watch：监听节点值；get path watch：监听路径<br>cZxid：事务id，每次创建都以时间戳的形式产生唯一的Zxid<br>ctime：节点创建时间<br>mZxid：节点最后修改的时间戳id<br>mtime：节点最后修改的时间<br>pZxid：节点最后更新的子节点时间戳id<br>cversion：节点修改次数<br>dataVersion：节点数据的变化号<br>dataLength：数据长度<br>numChild：子节点数</p></blockquote></li><li>stat path：查看节点的状态信息</li><li>set path acl：更改path节点中的数据</li><li>create [-s] [-e] path data acl：创建路径，data为创建目录节点所存储的数据，acl为应答类型，[-s]带序号的节点，[-e]短暂节点</li><li>delete path：删除节点（不能删除带有子节点的节点）</li><li>rmr path：递归删除</li></ul><h1 id="zookeeper选举机制"><a href="#zookeeper选举机制" class="headerlink" title="zookeeper选举机制"></a>zookeeper选举机制</h1><ul><li>当ZK1服务器启动时，先给自己投一票，ZK1作为follower</li><li>当ZK2服务器启动时，给自己投一票的同时，ZK1也会给ZK2投一票，票数占半数以上，ZK2作为leader</li><li>当ZK3服务器启动时，给自己投一票，此时发现已经存在leader，ZK3作为follower</li></ul><h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><ul><li>只要有半数以上的节点存活，就能够正常工作（zk集群要求配置奇数台）</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>基础命令</title>
      <link href="/2018/09/05/%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/"/>
      <url>/2018/09/05/%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h1 id="文件目录操作命令"><a href="#文件目录操作命令" class="headerlink" title="文件目录操作命令"></a><strong>文件目录操作命令</strong></h1><h2 id="ls-显示文件和目录列表"><a href="#ls-显示文件和目录列表" class="headerlink" title="ls 显示文件和目录列表"></a>ls 显示文件和目录列表</h2><blockquote><p>-l 列出文件的详细信息<br>-a 列出当前目录所有文件，包含隐藏文件<br>*设置环境变量：/root/.bash_profile</p></blockquote><h2 id="pwd-显示当前目录位置"><a href="#pwd-显示当前目录位置" class="headerlink" title="pwd 显示当前目录位置"></a>pwd 显示当前目录位置</h2><h2 id="mkdir-创建目录"><a href="#mkdir-创建目录" class="headerlink" title="mkdir 创建目录"></a>mkdir 创建目录</h2><blockquote><p>-p 父目录不存在的情况下先生成父目录<br>约定：</p><blockquote><p>mkdir /root/tools —-&gt;安装包<br>mkdir /root/training —-&gt;安装目录</p></blockquote></blockquote><h2 id="cd-切换目录"><a href="#cd-切换目录" class="headerlink" title="cd 切换目录"></a>cd 切换目录</h2><h2 id="touch-生成一个空文件"><a href="#touch-生成一个空文件" class="headerlink" title="touch 生成一个空文件"></a>touch 生成一个空文件</h2><h2 id="echo-生成一个带内容文件"><a href="#echo-生成一个带内容文件" class="headerlink" title="echo 生成一个带内容文件"></a>echo 生成一个带内容文件</h2><blockquote><p>使用echo查看环境变量值：echo $JAVA_HOME</p></blockquote><h2 id="cat、tac-显示文本文件内容"><a href="#cat、tac-显示文本文件内容" class="headerlink" title="cat、tac 显示文本文件内容"></a>cat、tac 显示文本文件内容</h2><blockquote><p>cat是从第一行开始写，tac是从最后一行开始写</p></blockquote><h2 id="cp-复制文件或目录"><a href="#cp-复制文件或目录" class="headerlink" title="cp 复制文件或目录"></a>cp 复制文件或目录</h2><h2 id="rm-删除文件"><a href="#rm-删除文件" class="headerlink" title="rm 删除文件"></a>rm 删除文件</h2><blockquote><p>-r 同时删除该目录下的所有文件<br>-f 强制删除文件或目录<br>*HDFS有回收站，默认情况下关闭</p></blockquote><h1 id="系统操作命令"><a href="#系统操作命令" class="headerlink" title="系统操作命令"></a><strong>系统操作命令</strong></h1><h2 id="ps-显示瞬间的进程状态"><a href="#ps-显示瞬间的进程状态" class="headerlink" title="ps 显示瞬间的进程状态"></a>ps 显示瞬间的进程状态</h2><blockquote><p>ps -ef：使用标准格式显示每个进程信息</p></blockquote><h2 id="hostname-显示主机名"><a href="#hostname-显示主机名" class="headerlink" title="hostname 显示主机名"></a>hostname 显示主机名</h2><h2 id="kill-杀死一个进程"><a href="#kill-杀死一个进程" class="headerlink" title="kill 杀死一个进程"></a>kill 杀死一个进程</h2><blockquote><p>-9 强制杀死一个进程<br>-3 如果针对java进程，打印java进程的线程信息Thread Dump</p></blockquote><h1 id="打包命令"><a href="#打包命令" class="headerlink" title="打包命令"></a><strong>打包命令</strong></h1><h2 id="gzip-压缩（解压）文件或目录，压缩文件后缀为gz"><a href="#gzip-压缩（解压）文件或目录，压缩文件后缀为gz" class="headerlink" title="gzip 压缩（解压）文件或目录，压缩文件后缀为gz"></a>gzip 压缩（解压）文件或目录，压缩文件后缀为gz</h2><h2 id="bzip2-压缩（解压）文件或目录，压缩文件后缀为bz2"><a href="#bzip2-压缩（解压）文件或目录，压缩文件后缀为bz2" class="headerlink" title="bzip2 压缩（解压）文件或目录，压缩文件后缀为bz2"></a>bzip2 压缩（解压）文件或目录，压缩文件后缀为bz2</h2><h2 id="tar-文件、目录打（解）包"><a href="#tar-文件、目录打（解）包" class="headerlink" title="tar 文件、目录打（解）包"></a>tar 文件、目录打（解）包</h2><blockquote><p>-zxvf 解压缩<br>-C 解压到指定目录下</p></blockquote><h1 id="权限管理（非常类似HDFS权限管理）"><a href="#权限管理（非常类似HDFS权限管理）" class="headerlink" title="权限管理（非常类似HDFS权限管理）"></a><strong>权限管理（非常类似HDFS权限管理）</strong></h1><h2 id="权限的类型"><a href="#权限的类型" class="headerlink" title="权限的类型"></a>权限的类型</h2><blockquote><p>r 读<br>w 写<br>x 执行</p></blockquote><h2 id="ls-l、ll-查看权限"><a href="#ls-l、ll-查看权限" class="headerlink" title="ls -l、ll 查看权限"></a>ls -l、ll 查看权限</h2><blockquote><p>十位字符，第一位如果为d，则代表该文件为目录<br>后九位分为三组，每一组都按读写执行的顺序排列</p><blockquote><p>第一组：当前用户<br>第二组：同组用户<br>第三组：其他用户<br>权限用二进制表示，有为1，没有为0（rwx -&gt; 111，rw- -&gt; 110）<br>chmod：改变权限<br>chmod (u/g/o/a)+(r/w/x) 文件名（括号内可选）<br>chmod 777 文件名（每一位十进制数代表一组权限，此处为所有用户可读可写可执行）</p></blockquote></blockquote><h2 id="案例分析-java的死锁或者性能瓶颈分析"><a href="#案例分析-java的死锁或者性能瓶颈分析" class="headerlink" title="案例分析:java的死锁或者性能瓶颈分析"></a>案例分析:java的死锁或者性能瓶颈分析</h2><blockquote><p>JDK heap dump:分析OOM的问题<br>JDK Thread dumo:分析性能瓶颈(线程信息)<br>得到Thread dump:</p><blockquote><p>在linux:kill -3 PID<br>在windows下:Fn + B 或Ctrl + Break</p></blockquote></blockquote><h1 id="配置IP"><a href="#配置IP" class="headerlink" title="配置IP"></a><strong>配置IP</strong></h1><h2 id="ip-config：查看IP状态"><a href="#ip-config：查看IP状态" class="headerlink" title="ip config：查看IP状态"></a>ip config：查看IP状态</h2><h2 id="更改ip配置：修改-etc-sysconfig-network-scripts下的ifcfg-eth"><a href="#更改ip配置：修改-etc-sysconfig-network-scripts下的ifcfg-eth" class="headerlink" title="更改ip配置：修改/etc/sysconfig/network-scripts下的ifcfg-eth*"></a>更改ip配置：修改/etc/sysconfig/network-scripts下的ifcfg-eth*</h2>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>大数据模块</title>
      <link href="/2018/09/04/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9D%97/"/>
      <url>/2018/09/04/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h3 id="离线计算"><a href="#离线计算" class="headerlink" title="离线计算"></a><strong>离线计算</strong></h3><blockquote><p><strong>Hadoop模块</strong></p><blockquote><p>1.数据存储：HDFS（Hadoop Distributed File System）<br>2.数据计算：MapReduce（java程序、实现离线计算）：在Hadoop 2.X后，Yarn容器中<br>3.Hive：基于HDFS之上的数据仓库，支持SQL语句<br>4.HBase：基于HDFS之上的NoSQL数据库<br>5.ZooKeeper：实现HA（High Availability高可用性、秒杀系统）的功能<br>6.其他：Sqoop、Flume、Pig</p></blockquote></blockquote><blockquote><p><strong>实时计算</strong></p><blockquote><p>1.Redis内存NoSQL数据库<br> Redis Cluster：分布式解决方案<br>2.Apache Storm：进行试试计算（流式计算）</p></blockquote></blockquote><blockquote><p><strong>Spark：只有数据计算，没有数据的存储（依赖HDFS）</strong></p><blockquote><p>1.Scala变成语言：多范式的编程语言（支持多方式编程：1、面向对象 2、函数式编程）<br>2.Spark Core：内核，相当于MapReduce；<br>        最重要的概念：RDD（弹性分布式数据集）<br>3.Spark SQL：类似Hive、支持SQL<br>4.Spark Streaming：处理流式计算的模块，类似Storm</p></blockquote></blockquote>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Redis初识</title>
      <link href="/2018/08/29/redis%E5%88%9D%E8%AF%86/"/>
      <url>/2018/08/29/redis%E5%88%9D%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<h3 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a><strong>Redis</strong></h3><hr><blockquote><p><strong>高性能Key-Value服务器</strong></p></blockquote><blockquote><p><strong>多种数据结构</strong></p></blockquote><blockquote><p><strong>丰富的功能</strong></p></blockquote><blockquote><p><strong>高可用分布式支持</strong></p></blockquote><hr><h3 id="Redis初识"><a href="#Redis初识" class="headerlink" title="Redis初识"></a><strong>Redis初识</strong></h3><ul><li>由Salvatore Sanfilippo（antirez）制作，目前服务于以色列RedisLabs，早期代码23000行，采用key-value的字典结构，GitHub、twitter、StackOverflow、阿里巴巴、百度、微博、美团、搜狐等都在使用Redis这样的技术，如今Redis已经成为必备技能。<blockquote><ul><li>典型使用场景<blockquote><ol><li>缓存系统：用户访问App server，App Server从cache（Redis）请求数据，如果有，直接返回给App Server，如果没有，cache将从Storage（持久化存储空间）中查找，查找后将从Storage中查找到的数据存入cache中以方便下次查找，而后直接由Storage返回给App Server。</li><li>计数器：微博、视频网站的转发、评论数。</li><li>消息队列系统</li><li>排行榜</li><li>社交网络</li><li>实时系统</li></ol></blockquote></li></ul></blockquote></li></ul><hr><blockquote><p><strong>开源</strong><br><strong>基于键值的存储服务系统</strong><br><strong>支持多种数据结构</strong><br><strong>性能高，功能丰富</strong></p></blockquote><hr><h3 id="Redis特性"><a href="#Redis特性" class="headerlink" title="Redis特性"></a><strong>Redis特性</strong></h3><hr><blockquote><p><strong>速度快</strong>（10W OPS(读写)）</p><blockquote><ol><li><em>将数据存在内存</em></li><li>用c语言编写</li><li>线性模型使用单线程</li></ol></blockquote><hr><p><strong>持久化</strong>（断电不丢数据）</p><blockquote><p>Redis所有数据保持在内存中，对数据的更新将异步地保存到硬盘上</p></blockquote><hr><p><strong>多种数据结构</strong></p><blockquote><ul><li>常规</li></ul><ol><li>字符串（Strings/Blobs/Bitmaps）</li><li>哈希（Hash Tables(objects!)）</li><li>列表（Linked Lists）</li><li>集合（Sets）</li><li>有序集合（Sorted Sets）</li></ol><ul><li>衍生</li></ul><ol><li>位图（BitMaps）</li><li>超小内存唯一值技术（HyperLogLog——有一定误差）</li><li>地理信息定位（GEO）</li></ol></blockquote><hr><p><strong>支持多种编辑语言</strong></p><blockquote><ol><li>Java</li><li>php</li><li>Python</li><li>Ruby</li><li>Lua</li><li>NodeJs</li></ol></blockquote><hr><p><strong>功能丰富</strong></p><blockquote><ol><li>发布订阅</li><li>Lua脚本</li><li>事务</li><li>pipeline</li></ol></blockquote><hr><p><strong>简单</strong></p><blockquote><ol><li>23000行代码</li><li>不依赖外部库（like libevent）</li><li>单线程模型（开发相对容易）</li></ol></blockquote><hr><p><strong>主从复制</strong></p><blockquote><p>在Redis中主服务器的数据可以同步到从服务器上，为高可用以及分布式提供一个很好的基础</p></blockquote><hr><p><strong>高可用、分布式</strong></p><blockquote><p>高可用 ——&gt; Redis-Sentinel(v2.8)支持高可用<br>分布式 ——&gt; Redis-Cluster(V3.0)支持分布式</p></blockquote><hr></blockquote>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>项目笔记(ssh)</title>
      <link href="/2018/08/23/%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0(ssh)/"/>
      <url>/2018/08/23/%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0(ssh)/</url>
      
        <content type="html"><![CDATA[<h3 id="1-Dao层抽取"><a href="#1-Dao层抽取" class="headerlink" title="1.Dao层抽取"></a>1.Dao层抽取</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(1)BaseDaoImpl&lt;T&gt;--&gt;BaseDao&lt;T&gt;</span><br><span class="line">   UserDaoImpl&lt;User&gt;--&gt;UserDao&lt;User&gt;</span><br><span class="line">(2)UserDao&lt;User&gt; extends BaseDao&lt;T&gt;</span><br><span class="line">   UserDaoImpl&lt;User&gt; extends BaseDaoImpl&lt;T&gt;</span><br></pre></td></tr></table></figure><h3 id="2-Action层抽取"><a href="#2-Action层抽取" class="headerlink" title="2.Action层抽取"></a>2.Action层抽取</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(1)BaseAction&lt;T&gt; extends ActionSupport implements ModelDriven&lt;T&gt;</span><br><span class="line">(2)UserAction&lt;User&gt; extends BaseAction&lt;T&gt;</span><br></pre></td></tr></table></figure><h3 id="3-通过反射创建对象"><a href="#3-通过反射创建对象" class="headerlink" title="3.通过反射创建对象"></a>3.通过反射创建对象</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//在构造方法中动态获取实体类，通过反射创建model对象</span><br><span class="line">//泛型指定user时，this为user，通过反射创建user对象赋值给model</span><br><span class="line">public BaseAction() throws InstantiationException, IllegalAccessException &#123;</span><br><span class="line">//获取父类class属性</span><br><span class="line">ParameterizedType genericsSuperclass = (ParameterizedType) this.getClass().getGenericSuperclass();</span><br><span class="line">//获取泛型数组</span><br><span class="line">Type[] actualTypeArguments = genericsSuperclass.getActualTypeArguments();</span><br><span class="line">Class&lt;T&gt; entityClass = (Class&lt;T&gt;)actualTypeArguments[0];</span><br><span class="line">//通过反射创建对象</span><br><span class="line">model = entityClass.newInstance();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> SSH </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>hibernate项目笔记(ssh)</title>
      <link href="/2018/08/22/hibernate%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0(ssh)/"/>
      <url>/2018/08/22/hibernate%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0(ssh)/</url>
      
        <content type="html"><![CDATA[<h3 id="1-与spring整合中hibernate连接池的两种方式（web-applicationContext-xml）"><a href="#1-与spring整合中hibernate连接池的两种方式（web-applicationContext-xml）" class="headerlink" title="1.与spring整合中hibernate连接池的两种方式（web/applicationContext.xml）"></a>1.与spring整合中hibernate连接池的两种方式（web/applicationContext.xml）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">方式1：</span><br><span class="line">&lt;bean id=<span class="string">"dataSource"</span> class=<span class="string">"com.mchange.v2.c3p0.ComboPooledDataSource"</span>&gt;</span><br><span class="line">&lt;property name=<span class="string">"driverClass"</span> value=<span class="string">"com.mysql.jdbc.Driver"</span>/&gt;</span><br><span class="line">&lt;property name=<span class="string">"jdbcUrl"</span> value=<span class="string">"jdbc:mysql////bos"</span>/&gt;</span><br><span class="line">&lt;property name=<span class="string">"user"</span> value=<span class="string">"root"</span>/&gt;</span><br><span class="line">&lt;property name=<span class="string">"password"</span> value=<span class="string">"tiger"</span>/&gt;</span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">方式2：</span><br><span class="line">&lt;bean id=<span class="string">"dataSource"</span> class=<span class="string">"org.springframework.jdbc.datasource.DriverManagerDataSource"</span>&gt;  </span><br><span class="line">&lt;property name=<span class="string">"driverClassName"</span>&gt;  </span><br><span class="line">    &lt;value&gt;<span class="variable">$&#123;database.driverClass&#125;</span>&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;property name=<span class="string">"url"</span>&gt;  </span><br><span class="line">    &lt;value&gt;<span class="variable">$&#123;database.jdbcUrl&#125;</span>?characterEncoding=UTF-8&amp;amp;useSSL=<span class="literal">false</span>&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;property name=<span class="string">"username"</span>&gt;  </span><br><span class="line">    &lt;value&gt;<span class="variable">$&#123;database.user&#125;</span>&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;property name=<span class="string">"password"</span>&gt;  </span><br><span class="line">    &lt;value&gt;<span class="variable">$&#123;database.password&#125;</span>&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure><h3 id="2-getHibernateTemplate-的使用方法"><a href="#2-getHibernateTemplate-的使用方法" class="headerlink" title="2.getHibernateTemplate()的使用方法"></a>2.getHibernateTemplate()的使用方法</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(1)导入org.springframework.orm.hibernate5.support.HibernateDaoSupport包</span><br><span class="line">(2)继承HibernateDaoSupport类</span><br><span class="line">* this.getHibernateTemplate().get(entityClass,id);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> SSH </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>maven整合框架笔记(ssh)</title>
      <link href="/2018/08/22/maven%E6%95%B4%E5%90%88%E6%A1%86%E6%9E%B6%E7%AC%94%E8%AE%B0(ssh)/"/>
      <url>/2018/08/22/maven%E6%95%B4%E5%90%88%E6%A1%86%E6%9E%B6%E7%AC%94%E8%AE%B0(ssh)/</url>
      
        <content type="html"><![CDATA[<h3 id="1-配置阿里远程仓库（parent-pom）"><a href="#1-配置阿里远程仓库（parent-pom）" class="headerlink" title="1.配置阿里远程仓库（parent/pom）"></a>1.配置阿里远程仓库（parent/pom）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;maven - ali&lt;/id&gt;</span><br><span class="line">        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;</span><br><span class="line">        &lt;releases&gt;</span><br><span class="line">            &lt;enabled&gt;<span class="literal">true</span>&lt;/enabled&gt;</span><br><span class="line">        &lt;/releases&gt;</span><br><span class="line">        &lt;snapshots&gt;</span><br><span class="line">            &lt;enabled&gt;<span class="literal">true</span>&lt;/enabled&gt;</span><br><span class="line">            &lt;updatePolicy&gt;always&lt;/updatePolicy&gt;</span><br><span class="line">            &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt;</span><br><span class="line">        &lt;/snapshots&gt;</span><br><span class="line">    &lt;/repository&gt;</span><br><span class="line">&lt;/repositories&gt;</span><br></pre></td></tr></table></figure><h3 id="2-maven中spring与jdk-1-8的兼容问题（parent-pom）"><a href="#2-maven中spring与jdk-1-8的兼容问题（parent-pom）" class="headerlink" title="2.maven中spring与jdk 1.8的兼容问题（parent/pom）"></a>2.maven中spring与jdk 1.8的兼容问题（parent/pom）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 属性定义指定jar版本 --&gt;</span><br><span class="line">&lt;properties&gt;</span><br><span class="line">&lt;spring.version&gt;5.0.5.RELEASE&lt;/spring.version&gt;</span><br><span class="line">&lt;hibernate.version&gt;5.2.17.Final&lt;/hibernate.version&gt;</span><br><span class="line">&lt;struts2.version&gt;2.3.24&lt;/struts2.version&gt;</span><br><span class="line">&lt;slf4j.version&gt;1.6.6&lt;/slf4j.version&gt;</span><br><span class="line">&lt;log4j.version&gt;1.2.12&lt;/log4j.version&gt;</span><br><span class="line">&lt;shiro.version&gt;1.2.3&lt;/shiro.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> SSH </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>git安装与简单操作(hexo)</title>
      <link href="/2018/08/22/git%E5%AE%89%E8%A3%85%E4%B8%8E%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/"/>
      <url>/2018/08/22/git%E5%AE%89%E8%A3%85%E4%B8%8E%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<ul><li>1.安装Node.js和配置好Node.js环境</li><li>2.安装Git和配置好Git环境</li><li>3.Github账户注册和新建项目，项目必须要遵守格式：账户名.github.io<blockquote><p>在建好的项目右侧有个settings按钮，点击它，向下拉到GitHub Pages，你会看到那边有个网址，访问它，你将会惊奇的发现该项目已经被部署到网络上，能够通过外网来访问它</p></blockquote></li><li><p>4.安装Hexo</p><blockquote><p>创建文件夹<br>通过命令行进入到该文件夹<br>安装Hexo</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo -g</span><br></pre></td></tr></table></figure></blockquote></li><li><p>5.初始化该文件夹</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure></li><li><p>6.安装所需要的组件</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></table></figure></li><li><p>7.联系hexo与github page</p><blockquote><p>终端输入：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git --global user.name <span class="string">"XXX"</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>终端输入：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git --global user.email <span class="string">"XXX@XXX.XXX"</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>添加密匙到ssh-agent<br>终端输入：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">eval</span> <span class="string">"<span class="variable">$(ssh-agent -s)</span>"</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>添加生成的ssh key到ssh-agent<br>终端输入</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">eval</span> <span class="string">"<span class="variable">$(ssh-agent -s)</span>"</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>github–&gt;setting–&gt;ssh and gpg keys添加id_rsa.pub文件中的ssh key</p></blockquote></li><li><p>8.配置Deployment(_config.yml)</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">   <span class="built_in">type</span>: git</span><br><span class="line">   repository: git@github.com:zfhzxg/zfhzxg.github.io.git</span><br><span class="line">   branch: master</span><br></pre></td></tr></table></figure></li><li><p>基础命令</p><blockquote><p>终端：<br>检测ssh是否配置成功</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh -T git@github.com</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>生成ssh密匙：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C <span class="string">"XXX@XXX.XXX"</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>生成.get：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ get init</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>cmd：<br>检测node.js是否安装成功：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">node -v</span><br><span class="line">nmp -v</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>检查hexo是否安装成功：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo -v</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>开启本地服务器：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>改变端口号：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo server -p XXXX</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>新建博客：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new post <span class="string">"博客名"</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>安装拓展：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>生成部署：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d -g</span><br></pre></td></tr></table></figure></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>git报错</title>
      <link href="/2018/08/22/git%E6%8A%A5%E9%94%99/"/>
      <url>/2018/08/22/git%E6%8A%A5%E9%94%99/</url>
      
        <content type="html"><![CDATA[<ul><li><p>安装主题报错</p><blockquote><p>报错内容： </p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl 18 transfer closed with outstanding <span class="built_in">read</span> data remaining</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>解决方法： </p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.postBuffer 524288000</span><br></pre></td></tr></table></figure></blockquote></li><li><p>curl 18 transfer closed with outstanding read data remaining</p><blockquote><p>解决方法：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.postBuffer 524288000</span><br></pre></td></tr></table></figure></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
    </entry>
    
  
  
</search>
