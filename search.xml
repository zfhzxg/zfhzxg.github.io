<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Azkaban</title>
      <link href="/2018/11/26/Azkaban/"/>
      <url>/2018/11/26/Azkaban/</url>
      
        <content type="html"><![CDATA[<h1 id="Azkaban概述"><a href="#Azkaban概述" class="headerlink" title="Azkaban概述"></a>Azkaban概述</h1><p>Azkaban是一个分布式工作流管理器，在LinkedIn上实现，以解决Hadoop作业依赖性问题。我们有需要按顺序运行的工作，从ETL工作到数据分析产品。</p><ol><li>特点</li></ol><ul><li>给用户提供了一个非常友好的可视化界面（web界面）</li><li>非常方便的上传工作流（打成压缩包）</li><li>设置任务间的关系</li><li>权限设置</li><li>模块化</li><li>随时停止和启动任务</li><li>可以查看日志记录</li></ul><ol start="2"><li>与Oozie相比，Azkaban是一个轻量级调度工具。Oozie功能较多，企业应用的功能并非小众的功能可以选择Azkaban</li></ol><ul><li>功能：都可以调度使用MR，java，脚本工作流任务，都可以进行定时调度</li><li>使用：az直接传参，Oozie支持直接传参和EL表达式</li><li>定时：az定时执行任务基于时间，Oozie定时执行任务基于时间或基于数据</li><li>资源：az有严格的权限控制，Oozie无严格权限控制</li></ul><hr><h1 id="Azkaban的安装"><a href="#Azkaban的安装" class="headerlink" title="Azkaban的安装"></a>Azkaban的安装</h1><ol><li>准备</li></ol><ul><li>先创建一个文件夹用来盛放Azkaban</li><li>azkaban：程序主体</li><li>azkaban-executor：执行器（执行端）</li><li>azkaban-web：服务端</li><li>需要在mysql中创建一个Azkaban用来导入Azkaban脚本</li><li>source /root/traning/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.aql</li></ul><ol start="2"><li>安装部署</li></ol><ul><li>创建SSL（安全连接）<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata211 ~]# yum install keytool</span><br><span class="line">[root@bigdata211 ~]# keytool -keystore -alias jetty -genkey -keyalg RSA（仅需输入两次密码，其他可不填，需同步时间）</span><br><span class="line">[root@bigdata211 ~]# tzselect（生成时间同步文件，选择地区，国家，城市，并选择yes）</span><br><span class="line">[root@bigdata211 ~]# cp /use/share/zone info/Asia/Shanghai /etc/localtime（设置时区）</span><br><span class="line">[root@bigdata211 ~]# sudo data -s “2018-11-28 20:43:23”（设置时间）</span><br></pre></td></tr></table></figure></li></ul><ol start="3"><li>修改配置</li></ol><ul><li><p>azkaban-web：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">conf/azkaban.properties：</span><br><span class="line">azkaban.name（名称）</span><br><span class="line">azkaban.color（颜色）</span><br><span class="line">default.timezone.id=Asia/Shanghai（时区）</span><br><span class="line">mysql.user=root（数据库用户名）</span><br><span class="line">mysql.password=tiger（数据库密码）</span><br><span class="line">jetty.password=tiger</span><br><span class="line">jetty.keypassword=tiger</span><br><span class="line">jetty.trustpassword=tiger（设置密码）</span><br><span class="line">conf/azkaban-user.xml：</span><br><span class="line">&lt;user username=“root” password=“tiger” roles=“admin,metrics” /&gt;</span><br></pre></td></tr></table></figure></li><li><p>azkaban-executor：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">conf/azkaban.properties：</span><br><span class="line">default.timezone.id=Asia/Shanghai（时区）</span><br><span class="line">mysql.azkaban=azkaban（数据库实例名）</span><br><span class="line">mysql.user=root（数据库用户名）</span><br><span class="line">mysql.password=tiger（数据库密码）</span><br></pre></td></tr></table></figure></li><li><p>启动服务端：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata211 ~]# bin/azkaban-web-start.sh</span><br></pre></td></tr></table></figure></li><li><p>启动执行器：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata211 ~]# bin/azkaban-executor-start.sh</span><br></pre></td></tr></table></figure></li><li><p>访问web界面：<a href="https://192.168.247.211:8443" target="_blank" rel="noopener">https://192.168.247.211:8443</a></p></li></ul><hr><h1 id="Azkaban的操作"><a href="#Azkaban的操作" class="headerlink" title="Azkaban的操作"></a>Azkaban的操作</h1><ol><li>单任务</li></ol><ul><li>创建工程：create project</li><li>上传工程：</li><li><p>创建文件并打包成zip压缩包：command.zip</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">command.job：</span><br><span class="line">type=command</span><br><span class="line">command=echo &apos;zfhzxg&apos;</span><br></pre></td></tr></table></figure></li><li><p>执行工程：execute flow</p></li></ul><ol start="2"><li>多任务（依赖）</li></ol><ul><li>创建工程</li><li><p>上传工程：bf.zip</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">f.job：</span><br><span class="line">type=command</span><br><span class="line">command=echo &apos;zfhzxg&apos;</span><br><span class="line">b.job：</span><br><span class="line">type=command</span><br><span class="line">deprndencies=f</span><br><span class="line">command=echo &apos;henshuai&apos;</span><br></pre></td></tr></table></figure></li><li><p>执行工程</p></li></ul><ol start="3"><li>任务中使用组件</li></ol><ul><li>创建工程</li><li><p>上传工程：hdfs.zip</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hofstra.job：</span><br><span class="line">type=command</span><br><span class="line">command=/root/training/hadoop-2.7.3/bin/hdfs dis -mkdir /azkaban</span><br></pre></td></tr></table></figure></li><li><p>执行工程</p></li></ul><ol start="4"><li>任务中使用MapReduce</li></ol><ul><li><p>创建工程</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">wc.job：</span><br><span class="line">type=command</span><br><span class="line">command=/root/training/hadoop-2.7.3/bin/hadoop jar hadoop-mapreduce-examples-2.7.3.jar word count /azwc/in /azwc/out</span><br></pre></td></tr></table></figure></li><li><p>将jar包和程序打包在一个压缩包中，wc.zip：wc.job，hadoop-mapreduce-examples-2.7.3.jar</p></li><li>上传工程</li><li>执行工程</li></ul><ol start="5"><li>任务中使用hive</li></ol><ul><li><p>创建工程和sql文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">azhive.sql：</span><br><span class="line">use default;</span><br><span class="line">drop table attest;</span><br><span class="line">create table aztest(i dint,name,string) row format delimited fields terminated by &quot;,&quot;;</span><br><span class="line">lode data in path &apos;azdata/user.tat&apos; into table aztest;</span><br><span class="line">create table azres as select * from aztest;</span><br><span class="line">inter overwrite directory &apos;/azdata/userout&apos; select count(*) from aztest;</span><br><span class="line">hive.job：</span><br><span class="line">type=command</span><br><span class="line">command=/root/training/hive/bin/hive -f &apos;azhive.sql&apos;</span><br></pre></td></tr></table></figure></li><li><p>将sql文件和程序打包在一个压缩包中，hivef.zip：hivef.job，azhive.sql</p></li><li>上传工程</li><li>执行工程</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>sqoop</title>
      <link href="/2018/11/26/sqoop/"/>
      <url>/2018/11/26/sqoop/</url>
      
        <content type="html"><![CDATA[<h1 id="sqoop概述"><a href="#sqoop概述" class="headerlink" title="sqoop概述"></a>sqoop概述</h1><ul><li>官网：aqoop.apache.org</li><li>场景：解决传统型数据库缺点，分布式存储。把传统型数据库数据迁移</li></ul><h2 id="sqoop的export命令（导出）"><a href="#sqoop的export命令（导出）" class="headerlink" title="sqoop的export命令（导出）"></a>sqoop的export命令（导出）</h2><ul><li>需求：Hive/HDFS的数据导出到mysql</li></ul><ol><li><p>根据Hive中的表字段创建mysql表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">清空表数据：</span><br><span class="line">    mysql&gt; truncate table user1;</span><br></pre></td></tr></table></figure></li><li><p>编写sqoop启动命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata211 sqoop]# bin/sqoop export \</span><br><span class="line">&gt; --connect jdbc:mysql://bigdata211:3306/sq \</span><br><span class="line">&gt; --username root \</span><br><span class="line">&gt; --password tiger \</span><br><span class="line">&gt; --table user1 \</span><br><span class="line">&gt; --num-mappers 1 \</span><br><span class="line">&gt; --expoet-dir /user/hive/warehouse/user_sqoop \</span><br><span class="line">&gt; --input-fields-terminated-by &quot;\t&quot;</span><br></pre></td></tr></table></figure></li><li><p>mysql中查看数据是否导入</p></li></ol><ul><li>需求：通过脚本文件执行程序</li></ul><ol><li>创建文件夹/root/sqoopjob</li><li><p>创建sqoopjob脚本（一行命令一行值）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">hdfs2mysql.opt：</span><br><span class="line">    export</span><br><span class="line">    --connect</span><br><span class="line">    jdbc:mysql://bigdate211:3306/sq</span><br><span class="line">    --username</span><br><span class="line">    root</span><br><span class="line">    --password</span><br><span class="line">    tiger</span><br><span class="line">    --table</span><br><span class="line">    user1</span><br><span class="line">    --num-mappers</span><br><span class="line">    1</span><br><span class="line">    --exoprt-dir</span><br><span class="line">    /user/hive/warehouse/user_sqoop</span><br><span class="line">    --input-fields-terminated-by</span><br><span class="line">    &quot;\t&quot;</span><br></pre></td></tr></table></figure></li><li><p>执行脚本文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">启动脚本：</span><br><span class="line">    [root@bigdata211 sqoop]# bin hd/sqoop \</span><br><span class="line">    &gt; --options-file /root/sqoopjob/hdfs2mysql.opt</span><br></pre></td></tr></table></figure></li></ol><h2 id="sqoop常用命令"><a href="#sqoop常用命令" class="headerlink" title="sqoop常用命令"></a>sqoop常用命令</h2><ol><li>import ————– 将数据导入到集群</li><li>export ————– 将集群数据导出</li><li>codegen ————- 把某张表数据生成java bean对象并且打包为jar</li><li>eval —————- 查看sql的执行结果</li><li>create-hive-table — 创建一个Hive表</li><li>import-all-tables — 导入某个数据库中的所有表到集群中</li><li>list-tables ——— 列出某个数据库下的所有表</li><li>merge ————— 将HDFS中不同目录下的数据合并在一起</li><li>help —————- 查看帮助信息</li></ol><h2 id="sqoop常用参数"><a href="#sqoop常用参数" class="headerlink" title="sqoop常用参数"></a>sqoop常用参数</h2><ol><li>–connect ————– 连接关系型数据库URL</li><li>–connection-manager — 指定连接管理类</li><li>–diver —————- JDBC的driver class（驱动类）</li><li>–username ————- 链接数据库的用户名</li><li>–password ————- 链接数据库的密码</li><li>–verbose ————– 在控制台打印详细信息</li><li>–help —————– 查看帮助</li><li>–hive-import ———- 将关系型数据库导入到Hive表中</li><li>–hive-overwrite ——- 将关系型数据库覆盖到Hive表中</li><li>–create-hive-table —- 创建Hive表</li><li>–hive-table ———– 接入Hive表</li><li>–table —————- 指定关系型数据库的表名</li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Flume</title>
      <link href="/2018/11/21/Flume/"/>
      <url>/2018/11/21/Flume/</url>
      
        <content type="html"><![CDATA[<h1 id="Flume概述"><a href="#Flume概述" class="headerlink" title="Flume概述"></a>Flume概述</h1><ul><li><p>官网：<a href="http://flume.apache.org/" target="_blank" rel="noopener">http://flume.apache.org/</a></p></li><li><p>Flume是一种分布式，可靠且可用的服务，用于有效地收集，聚合和移动大量日志数据。它具有可靠的可靠性机制和许多故障转移和恢复机制，具有强大的容错能力。它使用简单的可扩展数据模型，允许在线分析应用程序。（多用于数据采集）</p></li><li><p>数据从哪里来</p></li></ul><ol><li>爬虫</li><li>日志数据 flume</li><li>传统型数据库 sqoop</li></ol><hr><h1 id="Flume架构"><a href="#Flume架构" class="headerlink" title="Flume架构"></a>Flume架构</h1><ol><li>source：数据源<blockquote><p>产生数据流，同时source将产生的数据流传输到channel</p></blockquote></li><li>channel：传输通道<blockquote><p>用于桥接source和sinks</p></blockquote></li><li>sinks：下沉<blockquote><p>从channel收集数据</p></blockquote></li><li>event：传输单元（事件）<blockquote><p>Flume数据传输的基本单元，以事件的形式将数据送往目的地</p></blockquote></li></ol><hr><h1 id="Flume安装配置"><a href="#Flume安装配置" class="headerlink" title="Flume安装配置"></a>Flume安装配置</h1><ul><li>修改配置文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=path</span><br></pre></td></tr></table></figure></li></ul><hr><h1 id="Flume的使用"><a href="#Flume的使用" class="headerlink" title="Flume的使用"></a>Flume的使用</h1><ul><li>单channel/sink</li></ul><ol><li><p>准备配置文件flumejob_telnet.conf</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">#smple.conf: A single-node Flume configuration</span><br><span class="line"></span><br><span class="line"># Name the components on this agent 定义变量方便调用 加s可以有多个此角色</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># Describe/configure the source 描述source角色 进行内容定制</span><br><span class="line"># 此配置属于tcp source 必须是netcat类型</span><br><span class="line">a1.sources.r1.type = netcat </span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># Describe the sink 输出日志文件</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory（file） 使用内存 总大小1000 每次传输100</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel 一个source可以绑定多个channel </span><br><span class="line"># 一个sinks可以只能绑定一个channel  使用的是图二的模型</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure></li><li><p>通过命令执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata211 flume]# bin/flume-ng agent \</span><br><span class="line">&gt; --conf/ \   === 指定配置文件所在目录</span><br><span class="line">&gt; --name a1 \   === 指定别名</span><br><span class="line">&gt; --conf-file conf/flumejob_telnet.conf \   === 指定配置文件名</span><br><span class="line">&gt; -Dflume.root.logger==INFO,console    === 指定输出日志到控制台</span><br></pre></td></tr></table></figure></li></ol><ul><li>多channel/sink</li></ul><ol><li>准备配置文件</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">flumejob_1.conf：</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2 </span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line">#将数据复制给多个channel</span><br><span class="line">a1.sources.r1.selector.type = replicating</span><br><span class="line"></span><br><span class="line">#设置sources</span><br><span class="line">a1.sources.r1.type = exec</span><br><span class="line">#设置需要监控的文件（通过command的tail -F查看日志命令）</span><br><span class="line">a1.sources.r1.command = tail -F /tmp/root/hive.log</span><br><span class="line">a1.sources.r1.shell = /bin/bash -c</span><br><span class="line"></span><br><span class="line">#设置sinks</span><br><span class="line">#设置数据不进行落地操作</span><br><span class="line">a1.sinks.k1.type = avro </span><br><span class="line">a1.sinks.k1.hostname = bigdata211 </span><br><span class="line">a1.sinks.k1.port = 4141</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = avro </span><br><span class="line">a1.sinks.k2.hostname = bigdata211</span><br><span class="line">a1.sinks.k2.port = 4142</span><br><span class="line"></span><br><span class="line">#设置channels（通过内存传输）</span><br><span class="line">a1.channels.c1.type = memory </span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = memory </span><br><span class="line">a1.channels.c2.capacity = 1000</span><br><span class="line">a1.channels.c2.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">#指定sources、sinks通道</span><br><span class="line">a1.sources.r1.channels = c1 c2 </span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">flumejob_2.conf</span><br><span class="line">a2.sources = r1</span><br><span class="line">a2.sinks = k1 </span><br><span class="line">a2.channels = c1</span><br><span class="line"></span><br><span class="line">#设置sources</span><br><span class="line">a2.sources.r1.type = avro </span><br><span class="line">#端口抓取数据</span><br><span class="line">a2.sources.r1.bind = bigdata211</span><br><span class="line">a2.sources.r1.port = 4141</span><br><span class="line"></span><br><span class="line">#设置sink，下沉到hdfs</span><br><span class="line">a2.sinks.k1.type = hdfs</span><br><span class="line">a2.sinks.k1.hdfs.path = hdfs://bigdata211:9000/flume2/%Y%m%d/%H</span><br><span class="line">#上传文件的前缀</span><br><span class="line">a2.sinks.k1.hdfs.filePrefix = flume2-</span><br><span class="line">#是否按照时间滚动文件夹</span><br><span class="line">a2.sinks.k1.hdfs.round = true</span><br><span class="line">#多少时间单位创建一个新的文件夹</span><br><span class="line">a2.sinks.k1.hdfs.roundValue = 1</span><br><span class="line">#重新定义时间单位</span><br><span class="line">a2.sinks.k1.hdfs.roundUnit = hour</span><br><span class="line">#是否使用本地时间戳</span><br><span class="line">a2.sinks.k1.hdfs.useLocalTimeStamp = true</span><br><span class="line">#积攒多少个 Event 才 flush 到 HDFS 一次</span><br><span class="line">a2.sinks.k1.hdfs.batchSize = 100</span><br><span class="line">#设置文件类型，可支持压缩</span><br><span class="line">a2.sinks.k1.hdfs.fileType = DataStream</span><br><span class="line">#多久生成一个新的文件</span><br><span class="line">a2.sinks.k1.hdfs.rollInterval = 600</span><br><span class="line">#设置每个文件的滚动大小大概是 128M </span><br><span class="line">a2.sinks.k1.hdfs.rollSize = 134217700</span><br><span class="line">#文件的滚动与 Event 数量无关</span><br><span class="line">a2.sinks.k1.hdfs.rollCount = 0</span><br><span class="line">#最小副本数</span><br><span class="line">a2.sinks.k1.hdfs.minBlockReplicas = 1</span><br><span class="line"></span><br><span class="line">#设置channels（通过内存传输）</span><br><span class="line">a2.channels.c1.type = memory </span><br><span class="line">a2.channels.c1.capacity = 1000</span><br><span class="line">a2.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">#设置传输通道</span><br><span class="line">a2.sources.r1.channels = c1</span><br><span class="line">a2.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">flumejob_3.conf</span><br><span class="line">a3.sources = r1</span><br><span class="line">a3.sinks = k1</span><br><span class="line">a3.channels = c1</span><br><span class="line"></span><br><span class="line"># 设置sources</span><br><span class="line">a3.sources.r1.type = avro</span><br><span class="line"># 端口抓取数据</span><br><span class="line">a3.sources.r1.bind = bigdata211</span><br><span class="line">a3.sources.r1.port = 4142</span><br><span class="line"></span><br><span class="line"># 设置sinks</span><br><span class="line">a3.sinks.k1.type = file_roll</span><br><span class="line">a3.sinks.k1.sink.directory = /root/flume2</span><br><span class="line"></span><br><span class="line"># 设置channel</span><br><span class="line">a3.channels.c1.type = memory</span><br><span class="line">a3.channels.c1.capacity = 1000</span><br><span class="line">a3.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># 设置传输通道</span><br><span class="line">a3.sources.r1.channels = c1</span><br><span class="line">a3.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><ol start="2"><li>通过命令执行<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata211 flume]# bin/flume-ng agent \</span><br><span class="line">&gt; --conf conf/ \</span><br><span class="line">&gt; --name a1 \</span><br><span class="line">&gt; --conf-file conf/flumejob_1.conf </span><br><span class="line"></span><br><span class="line">[root@bigdata211 flume]# bin/flume-ng agent \</span><br><span class="line">&gt; --conf conf/ \</span><br><span class="line">&gt; --name a1 \</span><br><span class="line">&gt; --conf-file conf/flumejob_2.conf </span><br><span class="line"></span><br><span class="line">[root@bigdata211 flume]# bin/flume-ng agent \</span><br><span class="line">&gt; --conf conf/ \</span><br><span class="line">&gt; --name a1 \</span><br><span class="line">&gt; --conf-file conf/flumejob_3.conf</span><br></pre></td></tr></table></figure></li></ol><hr>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>spring</title>
      <link href="/2018/11/14/spring/"/>
      <url>/2018/11/14/spring/</url>
      
        <content type="html"><![CDATA[<h1 id="Spring概述"><a href="#Spring概述" class="headerlink" title="Spring概述"></a>Spring概述</h1><ul><li>是一个开源框架，于2003年兴起的一个java轻量级开发框架，是一个分层的java se/ee开发的一站式开源框架</li><li>优点：轻量，控制反转（IOC），面向切面，容器，框架，MVC</li><li>Spring三层：<blockquote><p>WEB层：SpringMVC<br>Service层：Spring的Bean管理，Spring声明式事务<br>Dao层：Spring的JDBC模板，Spring的ORM模块</p></blockquote></li></ul><hr><h1 id="Spring优点"><a href="#Spring优点" class="headerlink" title="Spring优点"></a>Spring优点</h1><ul><li>方便解耦合</li><li>AOP的开发：对程序进行扩展</li><li>轻量级框架</li><li>方便与其他框架整合</li></ul><hr><h1 id="Spring包"><a href="#Spring包" class="headerlink" title="Spring包"></a>Spring包</h1><ul><li>Spring-framework-4.X.XRELEASE-dependencies.zip：Spring依赖库</li><li>Spring-framework-4.X.XRELEASE-dist.zip：核心包<blockquote><p>Spring-framework-4.X.XRELEASE-dist/docs：开发规范和API<br>Spring-framework-4.X.XRELEASE-dist/libs：核心jar包和源码<br>Spring-framework-4.X.XRELEASE-dist/schema：配置文件的约束</p></blockquote></li><li>Spring-framework-4.X.XRELEASE-docs.zip：文档</li><li>Spring-framework-4.X.XRELEASE-schema.zip：约束</li></ul><hr><h1 id="SpringIOC的XML开发-IOC：Inversion-of-Control（控制反转）"><a href="#SpringIOC的XML开发-IOC：Inversion-of-Control（控制反转）" class="headerlink" title="SpringIOC的XML开发(IOC：Inversion of Control（控制反转）)"></a>SpringIOC的XML开发(IOC：Inversion of Control（控制反转）)</h1><h2 id="包、依赖"><a href="#包、依赖" class="headerlink" title="包、依赖"></a>包、依赖</h2><ul><li>通过xml文件，将对象的创建权反转给（交给）Spring</li><li><p>相关包文件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">spring-beans-4.2.4.RELEASE.jar（实体类包）</span><br><span class="line">spring-context-4.2.4.RELEASE.jar（核心扩展包）</span><br><span class="line">spring-core-4.2.4.RELEASE.jar（核心包）</span><br><span class="line">spring-expression-4.2.4.RELEASE.jar（Spring el表达式包）</span><br><span class="line">com.springsource.org.apache.commons.logging-1.1.1.jar（日志接口）</span><br><span class="line">com.springsource.org,apache.log4j-1.2.15.jar（log4j日志记录工具包）</span><br></pre></td></tr></table></figure></li><li><p>通过BeanFactory（工厂）切换底层实现类，消除哦耦合</p></li><li><p>BeanFactory对Xml进行解析，通过xml中的id找到其实现类进行反射，返回实例</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">XMl：</span><br><span class="line">&lt;bean i’d=“UserDao” class=“Use3rDaoImpl”&gt;&lt;/bean&gt;</span><br></pre></td></tr></table></figure></li><li><p>引入约束</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">spring-framework-4.2.4.RELEASE-dist\spring-framework-4.2.4.RELEASE\docs\spring-framework-reference\html\xsd-configuration.html：</span><br><span class="line">beans schema</span><br></pre></td></tr></table></figure></li><li><p>IOC和DI</p><blockquote><p>DI：依赖注入，前提是必须有IOC的环境，Spring管理这个类的时候将类的依赖注入（设置property属性）</p></blockquote></li></ul><h2 id="Spring的工厂类"><a href="#Spring的工厂类" class="headerlink" title="Spring的工厂类"></a>Spring的工厂类</h2><h3 id="BeanFactory（老版本工厂类）"><a href="#BeanFactory（老版本工厂类）" class="headerlink" title="BeanFactory（老版本工厂类）"></a>BeanFactory（老版本工厂类）</h3><ul><li>BeanFactory：调用个体Bean的时候，才会生成类的实例<h3 id="ApplicationContext（新版本工厂类）"><a href="#ApplicationContext（新版本工厂类）" class="headerlink" title="ApplicationContext（新版本工厂类）"></a>ApplicationContext（新版本工厂类）</h3></li><li>ApplicationContext底层继承了BeanFactory</li><li>ApplicationContext：加载配置文件的时候，就会将Spring管理的类都实例化</li><li>ApplicationContext有两个实现类<blockquote><p>ClassPathXmlApplicationContext：加载类路径下的配置文件（src路径下）<br>FileSystemXmlApplicationContext：加载文件系统下的配置文件（磁盘路径下）</p></blockquote></li></ul><h2 id="Spring-Bean管理"><a href="#Spring-Bean管理" class="headerlink" title="Spring Bean管理"></a>Spring Bean管理</h2><h3 id="id、name"><a href="#id、name" class="headerlink" title="id、name"></a>id、name</h3><ul><li>id ：使用了约束中的唯一约束，不能出现特殊字符</li><li><p>name ：没有使用约束中的唯一约束，可以出现特殊字符（理论上可以出现重复，但实际开发不能出现）</p><blockquote><p>Spring和struts1框架整合的时候（struts1中action注入id名必须加上斜杠）</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;bean id=&quot;/user&quot; class=&apos;&apos;&gt;</span><br></pre></td></tr></table></figure></blockquote></li><li><p>class：需要生成的实例类的全路径</p><h3 id="Bean生命周期的配置"><a href="#Bean生命周期的配置" class="headerlink" title="Bean生命周期的配置"></a>Bean生命周期的配置</h3></li><li>init-method ：Bean被初始化时执行的方法</li><li>destroy-method ：Bean被销毁时执行的方法，需要从实体类中调用（Bean作为单例模式创建时）<h3 id="Bean作用范围的配置"><a href="#Bean作用范围的配置" class="headerlink" title="Bean作用范围的配置"></a>Bean作用范围的配置</h3></li><li>scope ：Bean的作用范围<blockquote><p>singleton ：默认的，Spring会采用单例模式创建这个对象<br>prototype ：多例模式（struts2与Spring整合时，action采用多例模式）<br>request ：应用在web项目中，Spring创建这个类以后，将这个类存入到request范围中<br>session ：用用在web项目中，Spring创建这个类以后，将这个类存入到session范围中<br>globalsession（全局session） ：应用到web项目中，必须在porlet环境下使用，如果没有这种环境，globalsession相当于session</p></blockquote><h3 id="Spring属性注入（DI）"><a href="#Spring属性注入（DI）" class="headerlink" title="Spring属性注入（DI）"></a>Spring属性注入（DI）</h3></li><li>三种注入方式</li></ul><blockquote><p>构造方法方式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public class user&#123;</span><br><span class="line">    private String name;</span><br><span class="line">    public user(String name)&#123;</span><br><span class="line">        this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>set方法方式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">public class user&#123;</span><br><span class="line">    private String name;</span><br><span class="line">    public void setName(String name)&#123;</span><br><span class="line">        this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>接口注入方式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">public interface user&#123;</span><br><span class="line">    public void setName(String name);</span><br><span class="line">&#125;</span><br><span class="line">public class userImpl implements user&#123;</span><br><span class="line">    private String name;</span><br><span class="line">    public void setName(String name)&#123;</span><br><span class="line">        this.name = name;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p></blockquote><ul><li>Spring支持构造方法注入和set方法注入</li></ul><blockquote><p>构造方法属性注入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 构造属性注入的方式 --&gt;</span><br><span class="line">&lt;bean id=&quot;car&quot; class=&quot;com.zfhzxg.spring.demo3.Car&quot;&gt;</span><br><span class="line">&lt;constructor-arg name=&quot;name&quot; value=&quot;zfhzxg&quot;/&gt;</span><br><span class="line">&lt;constructor-arg name=&quot;p&quot; value=&quot;BMW&quot;/&gt;</span><br><span class="line">&lt;/bean&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 构造对象属性注入的方式 --&gt;</span><br><span class="line">&lt;!-- value用来设置普通属性值，ref用来设置其他的类的id和name --&gt;</span><br><span class="line">&lt;bean id=&quot;emp&quot; class=&quot;com.zfhzxg.spring.demo3.Emp&quot;&gt;</span><br><span class="line">&lt;constructor-arg name=&quot;name&quot; value=&quot;zfhzxg&quot;/&gt;</span><br><span class="line">&lt;constructor-arg name=&quot;car2&quot; ref=&quot;car2&quot;/&gt;</span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>set方法属性注入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- set属性注入的方式 --&gt;</span><br><span class="line">&lt;bean id=&quot;car2&quot; class=&quot;com.zfhzxg.spring.demo3.Car2&quot;&gt;</span><br><span class="line">&lt;property name=&quot;name&quot; value=&quot;zhxzgd&quot;/&gt;</span><br><span class="line">&lt;property name=&quot;p&quot; value=&quot;BMW&quot;/&gt;</span><br><span class="line">&lt;/bean&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- set对象属性注入的方式 --&gt;</span><br><span class="line">&lt;!-- value用来设置普通属性值，ref用来设置其他的类的id和name --&gt;</span><br><span class="line">&lt;bean id=&quot;emp&quot; class=&quot;com.zfhzxg.spring.demo3.Emp&quot;&gt;</span><br><span class="line">&lt;property name=&quot;name&quot; value=&quot;zhxzgd&quot;/&gt;</span><br><span class="line">&lt;property name=&quot;car2&quot; ref=&quot;car2&quot;/&gt;</span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>P名称空间属性注入（通过引入P名称空间来完成该属性的注入，Spring2.5之后）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">xmlns:p=&quot;http://www.springframework.org/schema/p&quot;</span><br><span class="line">&lt;!-- 不能输用构造函数--&gt;</span><br><span class="line">&lt;!-- 普通属性 P:属性名=&quot;值&quot;; --&gt;</span><br><span class="line">&lt;!-- 对象属性 P:属性名-ref=&quot;值&quot;; --&gt;</span><br><span class="line">&lt;!-- p名称空间注入方式 --&gt;</span><br><span class="line">&lt;bean id=&quot;car2&quot; class=&quot;com.zfhzxg.spring.demo3.Car2&quot; p:name=&quot;zfhzxg&quot; p:p=&quot;BMW&quot;&gt;&lt;/bean&gt;</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>SpEL的属性注入（Spring Expression Language，Spring3.0之后）<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">语法：</span><br><span class="line">    #&#123;SpEL&#125;//可以进行逻辑运算，调用对象和方法的值</span><br><span class="line">&lt;!-- SpEL的属性注入方式 --&gt;</span><br><span class="line">&lt;bean id=&quot;carInfo&quot; class=&quot;com.zfhzxg.spring.demo3.CarInfo&quot;&gt;&lt;/bean&gt;</span><br><span class="line">&lt;bean id=&quot;car2&quot; class=&quot;com.zfhzxg.spring.demo3.Car2&quot;&gt;</span><br><span class="line">&lt;property name=&quot;name&quot; value=&quot;#&#123;carInfo.name&#125;&quot;/&gt;</span><br><span class="line">&lt;property name=&quot;p&quot; value=&quot;#&#123;carInfo.calculaorPrice()&#125;&quot;/&gt;</span><br><span class="line">&lt;/bean&gt;</span><br><span class="line">&lt;bean id=&quot;emp&quot; class=&quot;com.zfhzxg.spring.demo3.Emp&quot;&gt;</span><br><span class="line">&lt;property name=&quot;name&quot; value=&quot;#&#123;&apos;zhxzgd&apos;&#125;&quot;/&gt;</span><br><span class="line">&lt;property name=&quot;car2&quot; value=&quot;#&#123;car2&#125;&quot;/&gt;</span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>集合类型属性注入<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- Spring集合属性注入 --&gt;</span><br><span class="line">&lt;bean name=&quot;collectionBean&quot; class=&quot;com.zfhzxg.spring.demo4.CollectionBean&quot;&gt;</span><br><span class="line">&lt;!-- 注入数组类型和list一样 --&gt;</span><br><span class="line">&lt;property name=&quot;arrs&quot; &gt;</span><br><span class="line">&lt;list&gt;</span><br><span class="line">&lt;!-- 对象用ref --&gt;</span><br><span class="line">&lt;value&gt;zfhzxg&lt;/value&gt;</span><br><span class="line">&lt;value&gt;zhxzgd&lt;/value&gt;</span><br><span class="line">&lt;/list&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property name=&quot;list&quot; &gt;</span><br><span class="line">&lt;list&gt;</span><br><span class="line">&lt;!-- 对象用ref --&gt;</span><br><span class="line">&lt;value&gt;123&lt;/value&gt;</span><br><span class="line">&lt;value&gt;321&lt;/value&gt;</span><br><span class="line">&lt;/list&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property name=&quot;set&quot; &gt;</span><br><span class="line">&lt;list&gt;</span><br><span class="line">&lt;!-- 对象用ref --&gt;</span><br><span class="line">&lt;value&gt;abc&lt;/value&gt;</span><br><span class="line">&lt;value&gt;cba&lt;/value&gt;</span><br><span class="line">&lt;/list&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;!-- 此处面向对象使用value-ref --&gt;</span><br><span class="line">&lt;property name=&quot;map&quot; &gt;</span><br><span class="line">&lt;map&gt;</span><br><span class="line">&lt;entry key=&quot;aaa&quot; value=&quot;111&quot;&gt;&lt;/entry&gt;</span><br><span class="line">&lt;entry key=&quot;bbb&quot; value=&quot;222&quot;&gt;&lt;/entry&gt;</span><br><span class="line">&lt;/map&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure></p></blockquote><blockquote><p>分模块开发配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">一、直接加载多文件：</span><br><span class="line">    ApplicationContext applicationContext = new ClassPathXmlApplicationContext(&quot;applicationContext1.xml&quot;,&quot;applicationContext2.xml&quot;);</span><br><span class="line">二、在applicationContext1.xml中引入其他配置文件：</span><br><span class="line">    &lt;import resource=&quot;applicationContext2.xml&quot;/&gt;</span><br><span class="line">    &lt;import resource=&quot;applicationContext3.xml&quot;/&gt;</span><br></pre></td></tr></table></figure></p></blockquote><hr><h1 id="SpringIOC的注解"><a href="#SpringIOC的注解" class="headerlink" title="SpringIOC的注解"></a>SpringIOC的注解</h1><ul><li>在Spring4的版本中，除了需要引入基本的四个开发包意外，还需要引入aop的包</li></ul><hr><h1 id="Spring的web项目资源浪费（与struts2整合）"><a href="#Spring的web项目资源浪费（与struts2整合）" class="headerlink" title="Spring的web项目资源浪费（与struts2整合）"></a>Spring的web项目资源浪费（与struts2整合）</h1><ul><li>web每次请求都会创建Spring的工厂，浪费服务器资源</li><li>解决：<blockquote><p>在服务器启动的时候创建Spring的工厂<br>将工厂保存到ServletContext中<br>每次获取工厂都从ServletContext中获取</p></blockquote></li><li>整合：<blockquote><p>引入spring_web.jar<br>配置ContextLoaderListener</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 配置Spring的核心监听器 --&gt;</span><br><span class="line">&lt;!-- ContextLoaderListener底层实现ServletContextListener --&gt;</span><br><span class="line">  &lt;listener&gt;</span><br><span class="line">  &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;</span><br><span class="line"> &lt;/listener&gt;</span><br><span class="line">&lt;!-- 默认加载的是/WEB-INF/applicationContext.xml --&gt;</span><br><span class="line">&lt;!-- 修改加载配置文件的路径 --&gt;</span><br><span class="line">&lt;context-param&gt;</span><br><span class="line">  &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt;</span><br><span class="line">  &lt;param-value&gt;classpath:applicationContext.xml&lt;/param-value&gt;</span><br><span class="line">  &lt;/context-param&gt;</span><br><span class="line">获取工厂：</span><br><span class="line">public String getApplicationContext() &#123;</span><br><span class="line">ServletContext serviceContext = ServletActionContext.getServletContext();</span><br><span class="line">WebApplicationContext applicationContext = WebApplicationContextUtils.getWebApplicationContext(serviceContext);</span><br><span class="line">CustomerService customerService = (CustomerService)applicationContext.getBean(&quot;customerService&quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> SSM </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>yarn</title>
      <link href="/2018/10/19/yarn/"/>
      <url>/2018/10/19/yarn/</url>
      
        <content type="html"><![CDATA[<h1 id="RecourceManager"><a href="#RecourceManager" class="headerlink" title="RecourceManager"></a>RecourceManager</h1><ul><li>处理客户端的请求</li><li>监控NodeManager</li><li>启动或者监控程序</li><li>资源的分配和调度</li></ul><h1 id="NodeManager"><a href="#NodeManager" class="headerlink" title="NodeManager"></a>NodeManager</h1><ul><li>RecourceManager将会给NodeManager分配container（资源）和AppMstr（任务）来处理数据</li><li>管理单个节点的资源</li><li>处理来自RecourceManager的命令</li><li>处理程序的命令</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>eclipse的Hadoop环境配置</title>
      <link href="/2018/10/12/eclipse%E7%9A%84Hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-2/"/>
      <url>/2018/10/12/eclipse%E7%9A%84Hadoop%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE-2/</url>
      
        <content type="html"><![CDATA[<h1 id="设置系统环境变量"><a href="#设置系统环境变量" class="headerlink" title="设置系统环境变量"></a>设置系统环境变量</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_HOME：hadoop-2.7.3/</span><br><span class="line">PATH：%HADOOP_HOME%/bin</span><br></pre></td></tr></table></figure><h1 id="eclipse进行导包"><a href="#eclipse进行导包" class="headerlink" title="eclipse进行导包"></a>eclipse进行导包</h1><ul><li>包路径：hadoop-2.7.3/share/</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>HDFS</title>
      <link href="/2018/10/01/HDFS/"/>
      <url>/2018/10/01/HDFS/</url>
      
        <content type="html"><![CDATA[<h1 id="HDFS的体系架构"><a href="#HDFS的体系架构" class="headerlink" title="HDFS的体系架构"></a>HDFS的体系架构</h1><h2 id="NameNode：名称节点"><a href="#NameNode：名称节点" class="headerlink" title="NameNode：名称节点"></a>NameNode：名称节点</h2><ul><li>是HDFS的主节点、管理员</li><li>接收客户端（命令行、java程序）的请求：创建目录、上传数据、下载数据、删除数据等</li><li>管理和维护HDFS的日志和元信息    <blockquote><p>日志文件（edits文件）：记录的是客户端的所有操作，是一个二进制文件（JSON）</p><blockquote><p>位置：/root/training/hadoop/tmp/dfs/name/current<br>edit_inprogress_00000000000000XXXXX：正在操作的日志文件<br>hdfs oev -i edits_inprogress_00000000000000XXXXX -o ~/a.xml：通过日志查看器（edits viewer），把edits文件转换成文本（xml）格式<br>元信息（fsimage文件）：记录的是数据块的位置信息，数据块的冗余信息，是一个二进制文件<br>位置：/root/training/hadoop/tmp/dfs/name/current<br>fsimage_0000000000000XXXXX：元信息记录文件<br>hdfs oiv -i fsimage_000000000000XXXXX -o ~/b.xml：将元信息记录文件转换成文本（xml或txt）格式</p></blockquote></blockquote></li><li>HDFS元信息保存位置配置<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs-site.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.name.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;储存位置&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li></ul><h2 id="DataNode：数据节点"><a href="#DataNode：数据节点" class="headerlink" title="DataNode：数据节点"></a>DataNode：数据节点</h2><ul><li>按照数据块保存数据<blockquote><p>1.X : 64M<br>2.X : 128M</p></blockquote></li><li>数据块：表现形式就是一个文件（blk打头）<blockquote><p>位置：/root/training/hadoop-2.7.3/tmp/dfs/data/current/BP-XXX-数据节点-XXX/current/finalized/subdir0/subdir0/<br>一个数据块对应的是一对文件，‘.meta’记录的是数据块的元信息<br>设置数据块冗余度规则：一般跟数据节点个数相同，但最大不要超过3<br>Hadoop 3.X之前，会造成存储空间极大的浪费<br>Hadoop 3.X之后，采用HDFS纠删码技术，使得存储空间节约一半</p></blockquote></li><li>HDFS文件系统的保存位置配置<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs-site.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.data.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;储存位置&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li></ul><h2 id="SecondaryNameNode：第二名称节点"><a href="#SecondaryNameNode：第二名称节点" class="headerlink" title="SecondaryNameNode：第二名称节点"></a>SecondaryNameNode：第二名称节点</h2><ul><li>职责：进行日志信息的合并<blockquote><p>SecondaryNameNode向NameNode下载edits日志文件和fsimage元信息文件<br>将edits中最新的信息写入fsimage文件<br>将合并后的文件上传给NameNode<br>当上次合并发生以后，用户进行新的操作，NameNode将产生新的edits_inprogress<br>当HDFS发出检查点（checkpoint）的时候，会进行日志信息合并<br>默认情况下，HDFS每隔60分钟或edits文件达到了64M产生一个检查点</p></blockquote></li><li>由于edits文件记录了最新的状态信息，并且随着操作越多，edits就会越大</li><li>把edits中的最新信息写到fsimage中</li><li>edits文件就可以清空</li><li>配置SecondaryNameNode节点位置<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hdfs-site.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dis.namenode.secondary.http-address&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;RedHat112:50090&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li></ul><h1 id="HDFS基础命令"><a href="#HDFS基础命令" class="headerlink" title="HDFS基础命令"></a>HDFS基础命令</h1><ul><li>hdfs dfs -help        ：查看帮助</li><li>hdfs dfs -ls /        ：列出/目录下的文件和目录</li><li>hdfs dfs -put /本地路径 /上传路径        ：上传文件</li><li>hdfs dfs -moveFromLocal /本地路径 /上传路径        ：剪切上传文件</li><li>hdfs dfs -get /hdfs路径 /本地路径        ：下载文件</li><li>hdfs dfs -getmerge /hdfs目录（是一个文件夹） /合并后的文件        ：合并下载</li><li>hdfs dfs -mkdir /目录名        ：创建目录</li><li>hdfs dfs -mkdir -p /目录名/目录名        ：创建多级目录</li><li>hdfs dfs -mv /需要移动的目录（文件） /需要移动到的位置        ：移动文件/文件夹</li><li>hdfs dfs -copy /需要复制的目录（文件） /需要复制到的位置        ：复制文件/文件夹</li><li>hdfs dfs -rm /文件路径       ：删除文件</li><li>hdfs dfs -rm -r /目录路径        ：删除目录</li><li>hdfs dfs -cat /文件路径        ：查看文件</li><li>hdfs dfs -tail -f /文件路径        ：查看文件的最后指定行</li><li>hdfs dfs -count /目录路径（文件路径）        ：查看目录（文件）文件夹数，文件数、大小</li><li>hdfs dfs -df -h /        ：查看hdfs的总空间</li><li>hdfs dfs -setrep 冗余度 /文件路径        ：设置单个文件冗余度</li></ul><h1 id="简单的HDFS上传文件代码"><a href="#简单的HDFS上传文件代码" class="headerlink" title="简单的HDFS上传文件代码"></a>简单的HDFS上传文件代码</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">package hdfs.demo1;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.net.URI;</span><br><span class="line">import java.net.URISyntaxException;</span><br><span class="line">import org.apache.hadoop.conf.Configuration;</span><br><span class="line">import org.apache.hadoop.fs.FileSystem;</span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line"></span><br><span class="line">public class HDFSClient &#123;</span><br><span class="line"></span><br><span class="line">public static void main(String[] args) throws IOException, InterruptedException, URISyntaxException &#123;</span><br><span class="line"></span><br><span class="line">//客户端加载配置文件</span><br><span class="line">Configuration conf = new Configuration();</span><br><span class="line"></span><br><span class="line">//指定配置（设置冗余度）</span><br><span class="line">conf.set(&quot;dfs.replication&quot;,&quot;2&quot;);</span><br><span class="line"></span><br><span class="line">//指定块大小</span><br><span class="line">conf.set(&quot;dfs.blocksize&quot;, &quot;64m&quot;);</span><br><span class="line"></span><br><span class="line">//构造客户端</span><br><span class="line">FileSystem fS = FileSystem.get(new URI(&quot;hdfs://192.168.0.102:9000/&quot;),conf,&quot;root&quot;);</span><br><span class="line"></span><br><span class="line">//上传文件</span><br><span class="line">fS.copyFromLocalFile(new Path(&quot;D:/123.txt&quot;), new Path(&quot;/work.txt&quot;));</span><br><span class="line"></span><br><span class="line">//关闭资源</span><br><span class="line">fS.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h1 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h1><ul><li>请求N阿么N哦的上传文件123.txt（客户端——》NameNode）</li><li>响应可以上传的文件（NameNode——》客户端）</li><li>请求上传第一个Block（块文件）（0～120m），请求分会DataNode（客户端——》NameNode）</li><li>返回DataNode1，DataNode2，DataNode3，表示采用这两个酒店储存具体的数据（NameNode——》客户端）</li><li>请求建立一个Block传输通道（客户端——》DataNode1——》dataNode2——》dataNode3）</li><li>DataNode1，DataNode2，DataNode3应答成功（DataNode——》客户端）</li><li>传输数据（客户端——》DataNode1——》dataNode2——》dataNode3）</li></ul><h1 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h1><ul><li>请求下载文件（客户端——》NameNode）</li><li>返回目标文件的元数据（NameNode——》客户端）</li><li>通过元信息请求第一块数据（客户端——》DataNode）</li><li>传输数据给客户端（DtatNode——》客户端）</li><li>继续通过元信息请求第二块数据。。。</li></ul><h1 id="NameNode与SecondaryNameNode工作机制"><a href="#NameNode与SecondaryNameNode工作机制" class="headerlink" title="NameNode与SecondaryNameNode工作机制"></a>NameNode与SecondaryNameNode工作机制</h1><ul><li>启动集群，加载edits（编辑日志）与fsimage（镜像文件）</li><li>元数据增删查改（客户端——》NameNode）</li><li>SecondaryNameNode请求是否需要CheckPoint（SecondaryNameNode——》NameNode）</li><li>SecondaryNameNode请求执行CheckPoint（SercondaryNameNode——》NameNode）</li><li>CheckPoint触发条件：1.通过定时；2.通过edits操作记录数量</li><li>CheckPoint触发时，SecondaryNameNode向NameNode拷贝edits文件并加载到内存（SercondaryNameNode——》NameNode）</li><li>生成新的镜像文件fsimage.chkpoint</li><li>将fsimage.chkpoint拷贝到NameNode（SercondaryNameNode——》NameNode）</li><li>NameNode将fsimage.chkpoint重命名为fsimage，再次发送到SecondaryNameNode当中（SercondaryNameNode——》NameNode）</li><li>NodeName与SencondaryNameNode将新的fsimage加载到内存中</li></ul><h1 id="HDFS重点"><a href="#HDFS重点" class="headerlink" title="HDFS重点"></a>HDFS重点</h1><ul><li>一、工作机制</li><li>二、读写流程</li><li>三、客户端api</li><li>四、集群部署安装</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>主从结构的单点故障</title>
      <link href="/2018/09/28/%E4%B8%BB%E4%BB%8E%E7%BB%93%E6%9E%84%E7%9A%84%E5%8D%95%E7%82%B9%E6%95%85%E9%9A%9C/"/>
      <url>/2018/09/28/%E4%B8%BB%E4%BB%8E%E7%BB%93%E6%9E%84%E7%9A%84%E5%8D%95%E7%82%B9%E6%95%85%E9%9A%9C/</url>
      
        <content type="html"><![CDATA[<h1 id="主从结构的主出现故障时视为单点故障"><a href="#主从结构的主出现故障时视为单点故障" class="headerlink" title="主从结构的主出现故障时视为单点故障"></a><strong>主从结构的主出现故障时视为单点故障</strong></h1><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><blockquote><p>NameNode（主）+DataNode（从）</p></blockquote><h2 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h2><blockquote><p>ResourceManager（主）+NodeManager（从）</p></blockquote><h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><blockquote><p>HMaster（主）+RefionServer（从）</p></blockquote><h2 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h2><blockquote><p>nimbus（主）+supervisor</p></blockquote><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><blockquote><p>Master（主）+Worker（从）</p></blockquote><p>#<strong>单点故障的解决方法（HA）</strong></p><blockquote><p>使用zookeeper实现HA功能，当主节点（active）出现故障时，通过主节点（standby）操作HDFS</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>hive</title>
      <link href="/2018/09/26/hive/"/>
      <url>/2018/09/26/hive/</url>
      
        <content type="html"><![CDATA[<hr><h1 id="数据分析引擎"><a href="#数据分析引擎" class="headerlink" title="数据分析引擎"></a><strong>数据分析引擎</strong></h1><h2 id="一、Hadoop中"><a href="#一、Hadoop中" class="headerlink" title="一、Hadoop中"></a>一、Hadoop中</h2><blockquote><p>（1）Hive：支持SQL<br>（2）Pig：支持PigLation</p></blockquote><h2 id="二、Spark中"><a href="#二、Spark中" class="headerlink" title="二、Spark中__"></a>二、Spark中__</h2><blockquote><p>（*）Spark SQL：类似Hive，支持SQL、DSL</p></blockquote><h2 id="三、另一个：Impala"><a href="#三、另一个：Impala" class="headerlink" title="三、另一个：Impala"></a>三、另一个：Impala</h2><hr><h1 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a><strong>什么是Hive</strong></h1><h2 id="一、Hive是基于HDFS之上的一个数据仓库，有助于使用SQL"><a href="#一、Hive是基于HDFS之上的一个数据仓库，有助于使用SQL" class="headerlink" title="一、Hive是基于HDFS之上的一个数据仓库，有助于使用SQL"></a>一、Hive是基于HDFS之上的一个数据仓库，有助于使用SQL</h2><ul><li>Hive —-&gt; HDFS</li><li>表 ——&gt; 目录</li><li>数据 —-&gt; 文件</li><li>分区 —-&gt; 目录</li><li>桶 ——&gt; 文件<h2 id="二、Hive是基于Hadoop之上的一个数据分析引擎"><a href="#二、Hive是基于Hadoop之上的一个数据分析引擎" class="headerlink" title="二、Hive是基于Hadoop之上的一个数据分析引擎"></a>二、Hive是基于Hadoop之上的一个数据分析引擎</h2></li><li>Hive 2.X 以前：SQL —-&gt; Hive —-&gt; MapReduce</li><li>Hive 2.X 以后：推荐使用Spark作为SQL的执行引擎（只针对Hadoop 3.X以前）（《Hive on Spark文档》）</li></ul><hr><h1 id="Hive的体系架构"><a href="#Hive的体系架构" class="headerlink" title="Hive的体系架构"></a><strong>Hive的体系架构</strong></h1><ul><li>一、CLI（命令行）：直接由Hive Dirver翻译</li><li>二、JDBC（标准接口）：1.X由Thrift Server，2.X由Hive Server翻译为SQL语句交由Hive Dirver，端口号都为10000</li><li>三、HWI（Hive Web Interface）：只在Hive 2.2前提供HWI网页工具，推荐使用HUE，由Hive Dirver翻译</li><li>*、在Hive的体系架构中还需要有关系型数据库用来存储Hive元信息（推荐使用MySQL）</li></ul><hr><h1 id="Hive优缺点"><a href="#Hive优缺点" class="headerlink" title="Hive优缺点"></a><strong>Hive优缺点</strong></h1><ol><li>优点</li></ol><ul><li>1）操作接口采用了sql，简化开发，减少学习成本</li><li>2）避免手写mapreduce程序</li><li>3）hive执行延迟较高，适用场景大多用在实时性要求不强的场景</li><li>4）处理大数据有优势</li><li>5）支持自定义函数</li></ul><ol start="2"><li>缺点</li></ol><ul><li>1）hive的sql表达能力有限（hql），并不能解决所有大数据场景</li><li>2）hive效率低（自动生成mapreduce作业，但力度比较粗，调优困难）</li><li>3）</li></ul><hr><h1 id="安装和配置Hive"><a href="#安装和配置Hive" class="headerlink" title="安装和配置Hive"></a><strong>安装和配置Hive</strong></h1><h2 id="准备工作："><a href="#准备工作：" class="headerlink" title="准备工作："></a>准备工作：</h2>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">1、解压  tar -zxvf apache-hive-2.3.0-bin.tar.gz -C ~/training/</span><br><span class="line">2、设置环境变量 vi  ~/.bash_profile</span><br><span class="line">  HIVE_HOME=/root/training/hive</span><br><span class="line">  export HIVE_HOME</span><br><span class="line"> </span><br><span class="line">  PATH=$HIVE_HOME/bin:$PATH</span><br><span class="line">  export PATH</span><br><span class="line">3.安装配置MySQL数据库</span><br><span class="line">  在虚拟机上安装MySQL：</span><br><span class="line">    yum remove mysql-libs </span><br><span class="line">    rpm -ivh mysql-community-common-5.7.19-1.el7.x86_64.rpm</span><br><span class="line">    rpm -ivh mysql-community-libs-5.7.19-1.el7.x86_64.rpm</span><br><span class="line">    rpm -ivh mysql-community-client-5.7.19-1.el7.x86_64.rpm</span><br><span class="line">    rpm -ivh mysql-community-server-5.7.19-1.el7.x86_64.rpm</span><br><span class="line">    rpm -ivh mysql-community-devel-5.7.19-1.el7.x86_64.rpm  （可选，但还是装上，后面装HUE的时候会用到。）</span><br><span class="line">                   </span><br><span class="line">  启动MySQL：service mysqld start</span><br><span class="line">  或者：systemctl start mysqld.service</span><br><span class="line"> </span><br><span class="line">  查看root用户的密码：cat /var/log/mysqld.log | grep password</span><br><span class="line">  登录后修改密码：alter user &apos;root&apos;@&apos;localhost&apos; identified by &apos;Welcome_1&apos;;</span><br><span class="line"> </span><br><span class="line">  MySQL数据库的配置：</span><br><span class="line">    创建一个新的数据库：create database hive;</span><br><span class="line">    创建一个新的用户：</span><br><span class="line">      create user &apos;hiveowner&apos;@&apos;%&apos; identified by &apos;Welcome_1&apos;;</span><br><span class="line"></span><br><span class="line">      给该用户授权</span><br><span class="line">        grant all on hive.* TO &apos;hiveowner&apos;@&apos;%&apos;; </span><br><span class="line">        grant all on hive.* TO &apos;hiveowner&apos;@&apos;localhost&apos; identified by &apos;Welcome_1&apos;;          </span><br><span class="line">         </span><br><span class="line">        免费工具：http://www.mysqlfront.de/</span><br></pre></td></tr></table></figure><h2 id="嵌入模式：不需要MySQL，使用Hive自带的Derby数据库存储Hive的元信息"><a href="#嵌入模式：不需要MySQL，使用Hive自带的Derby数据库存储Hive的元信息" class="headerlink" title="嵌入模式：不需要MySQL，使用Hive自带的Derby数据库存储Hive的元信息"></a>嵌入模式：不需要MySQL，使用Hive自带的Derby数据库存储Hive的元信息</h2>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">特点：</span><br><span class="line">  （1）使用自带的Derby</span><br><span class="line">  （2）只支持一个连接</span><br><span class="line">  （3）用于开发和测试</span><br><span class="line"></span><br><span class="line">修改配置文件hive-env.sh：</span><br><span class="line">  HADOOP_HOME=path</span><br><span class="line">  export HIVE_CONF_DIR=/root/hd/hive/conf</span><br><span class="line"></span><br><span class="line">在HDFS集群上创建文件夹：</span><br><span class="line">  hdfs dfs -mkdir /tmp</span><br><span class="line">  hdfs dfs -mkdir /user/hive/warehouse/</span><br><span class="line"></span><br><span class="line">创建hive-site.xml：</span><br><span class="line">  &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">  &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">  &lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;jdbc:derby:;databaseName=metastore_db;create=true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">              </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;org.apache.derby.jdbc.EmbeddedDriver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;   </span><br><span class="line">             </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hive.metastore.local&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;   </span><br><span class="line"> </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;file:///root/training/apache-hive-2.3.0-bin/warehouse&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;           </span><br><span class="line">  &lt;/configuration&gt;</span><br><span class="line"> </span><br><span class="line">初始化MetaStore：</span><br><span class="line">    schematool -dbType derby -initSchema</span><br><span class="line">           </span><br><span class="line">日志：</span><br><span class="line">    Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases.</span><br></pre></td></tr></table></figure><h2 id="本地模式、远程模式：都需要MySQL"><a href="#本地模式、远程模式：都需要MySQL" class="headerlink" title="本地模式、远程模式：都需要MySQL"></a>本地模式、远程模式：都需要MySQL</h2>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">修改配置文件hive-env.sh：</span><br><span class="line">  HADOOP_HOME=path</span><br><span class="line">  export HIVE_CONF_DIR=/root/hd/hive/conf</span><br><span class="line"></span><br><span class="line">在HDFS集群上创建文件夹：</span><br><span class="line">  hdfs dfs -mkdir /tmp</span><br><span class="line">  hdfs dfs -mkdir /user/hive/warehouse/</span><br><span class="line"></span><br><span class="line">创建hive-site.xml：</span><br><span class="line">  &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot; standalone=&quot;no&quot;?&gt;</span><br><span class="line">  &lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;</span><br><span class="line">  &lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;jdbc:mysql://localhost:3306/hive?useSSL=false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;               </span><br><span class="line"> </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;   </span><br><span class="line"> </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;hiveowner&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;   </span><br><span class="line"> </span><br><span class="line">    &lt;property&gt;</span><br><span class="line">      &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">      &lt;value&gt;tiger&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;                   </span><br><span class="line">           </span><br><span class="line">  &lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">初始化MetaStore：</span><br><span class="line">  schematool -dbType mysql -initSchema</span><br><span class="line">  </span><br><span class="line">* 启动hive前先启动hadoop集群和yarn</span><br></pre></td></tr></table></figure><hr><h1 id="Hive的架构"><a href="#Hive的架构" class="headerlink" title="Hive的架构"></a><strong>Hive的架构</strong></h1><ol><li>提供了一系列接口：hive shell、jdbc/odbc、webui</li><li>hive架构：hive（Meta元数据（默认derby数据库，可以自定义）、Client（cli、idbc）、SQL Parser解析器、Physica编译器、Query优化器、Execution执行器、MR程序）</li><li>客户端输入sql后，首先调用到元数据</li><li>通过SQL Parser解析器找到对应的MR程序</li><li>经过编译器编译代码</li><li>通过优化其选择SQL需不需要经过计算</li><li>通过执行器执行MR程序</li></ol><hr><h1 id="Hive的表类型和数据类型"><a href="#Hive的表类型和数据类型" class="headerlink" title="*Hive的表类型和数据类型"></a><strong>*Hive的表类型和数据类型</strong></h1><h2 id="表类型"><a href="#表类型" class="headerlink" title="表类型"></a>表类型</h2><ol><li>一、内部表（管理表）：类似MySQL、Oracle中的表</li></ol><ul><li>删除管理表时，hive会自动删除管理表的数据，不擅长做数据共享</li></ul><ol start="2"><li>二、外部表</li></ol><ul><li>删除外部表时，hive不认为这张表拥有这份数据，并不会删除数据，适合做数据共享</li><li>外部表已被删除时，重新创建一个表明、表结构都相同的表，hive将自动关联到外部表留下的数据</li></ul><ol start="3"><li>三、分区表：提高性能</li></ol><ul><li>（*）补充：如何提高性能？（SQL执行计划）</li></ul><ol start="4"><li>四、桶表：类似Hash分区</li><li>五、视图：View<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2>| Java数据类型 | hive数据类型 | 数据长度 |<br>| byte        | TINYINT     | 1byte    |<br>| short       | SMALINT     | 2byte    |<br>| int         | INT         | 4byte    |<br>| long        | BIGINT      | 8byte    |<br>| float       | FLOAT       | 单精度浮点数 |<br>| double      | DOUBLE      | 双精度浮点数 |<br>| String      | STRING      | 字符      |<br>| | TIMESTAMP | 时间类型     |<br>| | BINARY    | 字节数组     |</li></ol><hr><h1 id="基础HQL"><a href="#基础HQL" class="headerlink" title="基础HQL"></a><strong>基础HQL</strong></h1><h2 id="DBL数据定义"><a href="#DBL数据定义" class="headerlink" title="DBL数据定义"></a>DBL数据定义</h2><ol><li>库操作</li></ol><ul><li>alter database hive_db set dbproperties(‘dataname’=’zfhzxg的数据库’);：添加数据库描述信息</li><li>desc database hive_db;：查看数据库结构</li><li>desc database extended hive_db;：查看数据库拓展性</li><li>drop database if exists hive_db;：检测该库存在时，删除该库</li></ul><ol start="2"><li>管理表操作</li></ol><ul><li>show databases [link ‘db*’];：显示数据库，可选通配符筛选</li><li>create database hive_db location ‘path’;：在指定路径下创建数据库</li><li>create database if not exists hive_db;：检测该表不存在时，创建该表</li><li>desc formatted emp;：查看表类型</li></ul><ol start="3"><li>外部表操作</li></ol><ul><li>create table emp(id int,name string) row format[按行格式化] delimited fields[根据字段] terminated by “\t”[按空格切分];：创建表（默认创建管理表）</li><li>create external table if not exists emp2(id int,name string) row format delimited fields terminated by “\t”;：创建外部表</li><li>create table if not exists emp2 as select * from emp where name=”zfhzxg”;：emp2存在时，将emp中name为zfhzxg的字段传输到emp2中</li></ul><ol start="4"><li>分区表操作</li></ol><ul><li>create table if not exists emp_partition(id int,name string) partitioned by (day string) row format delimited fields terminated by “\t”;：创建分区表，以day为分区</li><li>select * from emp_partition where day=’1112’;：查询day分区为1112的数据</li><li>alter table emp_partition add partition(day=’1113’);：向emp_partition表中添加1113分区</li><li>alter table emp_partition drop partition(day=’1112’);：删除1112分区</li></ul><ol start="5"><li>修改表</li></ol><ul><li>alter table emptable rename to empt;：修改表名</li><li>alter table emp_partition add clumns(desc string);：向表中添加desc字段</li><li>alter table emp_partition change column desc descs int;：修改表中的字段</li><li>alter table emp_partition replace colums(name string,descs int);：替换表中的字段（重新定义，全部替换）</li></ul><h2 id="DML数据操作"><a href="#DML数据操作" class="headerlink" title="DML数据操作"></a>DML数据操作</h2><ol><li>加载/导入、插入</li></ol><ul><li>load data local inpath ‘path’ into table hive_db;：导入本地数据（追加）</li><li>load data inpath ‘path’ into table default.emp;：向default库中的emp表导入hdfs中的数据（追加，如果加载hdfs中的数据，源文件将会被剪切）</li><li>load data inpath ‘path’ overwrite into table default.emp;：导入hdfs中的数据（覆盖）</li><li>load data local inpath ‘path’ into table emp_partition partition(day=’1112’);：向1113分区中导入本地数据（追加）</li><li>insert into table emp_partition partition(day=’1112’) values(1,’zfhzxg’);：向分区表中的1112分区插入数据</li><li>create table if not exists emptable as select * from emp_partition where day=’1112’;：新创建一张表，并导入emp_partition表中1112分区中的数据</li><li>create table if not exists empta(id int,name string) row format delimited fields terminated by ‘\t’ location ‘path’;：创建表是关联数据，并不会剪切元数据</li><li>insert overwrite local directory ‘path’ select * from emp_partition where day=’1113’;：向本地路径（文件夹）导出empt_partition表中1113分区中的数据</li><li>dfs _get ‘path1’ ‘path2’;：在hive终端可以输入hdfs命令，可以用hadoop命令进行导出</li><li>[root@RedHat112 hive]# bin/hive -e “select from emp_partition where day=’1113’” &gt; /root/1113.txt ：将hive中表的数据导出成文件，用于结果导出，如果sql语句中包含单引号，需要用双引号包含sql语句</li><li>truncate table emp;：清空表中的数据</li></ul><h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><ol><li>基础查询</li></ol><ul><li>seltct * from emp;：全表查询</li><li>select emp.id,emp.name from emp;：查询指定字段</li><li>select emp.id [as] id,emp.name [as] name from emp;：自定义字段名</li></ul><ol start="2"><li>算数运算符</li></ol><ul><li>+（相加）</li><li>-（相减）</li><li>*（相乘）</li><li>/（相除）</li><li>%（取余）</li><li>&amp;（按位取与）</li><li>|（按位取或）</li><li>^（异或）</li><li>~（按位取反，只能用于四种整数类型）</li></ul><ol start="3"><li>函数</li></ol><ul><li>count（求行数）</li><li>max（最大）</li><li>min（最小）</li><li>sum（求和）</li><li>avg（平均值）</li><li>limit（查看前几条数据）：select * from emp limit 2;</li></ul><ol start="4"><li>where</li></ol><ul><li>select * from emp where id between 1 and 3;：查询id在1到3之间的人</li><li>select * from emp where id in(1,3);：查询id为1和3的人</li></ul><ol start="5"><li>like：选择类似的值，选择条件可以包含字母和数字</li></ol><ul><li>select * from emp where name like “_f%”;：查找name第二个字符为f的人</li><li>select * from emp where name rlike “[7]”;：rlike中可以使用正则表达式</li></ul><ol start="6"><li>group by：分组</li></ol><ul><li>select avg(empt.sal) avg_sal,deptno from empt group by deptno;：查询每个部门的工资平均值（按deptno分组）</li><li>select max(empt.sak) max_sal,deptno from empt group by deptno;：查询每个部门的最高薪水</li><li>select deptno,avg(sal) avg_sal from emp group by deptno having avg_sal&gt;1700;：查询每个部门的平均薪水大于1700的部门（在分组后添加条件时，用having）</li></ul><h2 id="自定义函数"><a href="#自定义函数" class="headerlink" title="自定义函数"></a>自定义函数</h2><ol><li>UDF（user-defined-function）：一进一出</li></ol><ul><li>导入hive依赖包（hive/lib下）</li><li>上传自定义函数包</li><li>add jar /root/tools/lower.jar;：添加到hive中</li><li>create temporary function my_lower as “com.zfhzxg.com.Lower”;：关联</li><li>正常语法即可使用自定义函数</li></ul><ol start="2"><li>UDAF：多进一出（count、max、min）</li><li>UDTF：一进多出</li></ol><h2 id="join"><a href="#join" class="headerlink" title="join"></a>join</h2><ol><li>等值join</li></ol><ul><li>select e.empno,e.ename,d.dept from empt e join dept d on e.deptno = d.deptno;</li></ul><ol start="2"><li>左外连接 left join（默认）</li></ol><ul><li>select e.empno,e.ename,d.dept from empt e left join dept d on e.deptno = d.deptno;</li></ul><ol start="3"><li>右外连接 right join（右边中符合where条件的所有记录都会被返回）</li></ol><ul><li>select e.empno,e.ename,d.dept form dept e right join empt e on e.deptno = d.deptno;</li></ul><ol start="4"><li>多表连接查询</li></ol><ul><li>select e.ename,d.dept,l.loc_name from empt e join dept d on e.deptno = d.deptno join location l on d.loc = l.loc_no;：查询员工的姓名，所在部门和地址</li></ul><h2 id="笛卡尔积（省略连接条件时）设置严格模式"><a href="#笛卡尔积（省略连接条件时）设置严格模式" class="headerlink" title="笛卡尔积（省略连接条件时）设置严格模式"></a>笛卡尔积（省略连接条件时）设置严格模式</h2><ul><li>set hive.mapred.mode;：查看模式</li><li>set hive.mapred.mode=strict;：设置严格模式（仅在本次设置终端内）</li></ul><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><ol><li>全局排序 order by</li></ol><ul><li>select * from empt order by sal;：按照工资查询，升序（默认为升序）</li><li>select * from empt order by sal asc;：按照工资查询，升序</li><li>select * from empt order by sal desc;：按照工资查询，降序</li><li>select <em> empt.empno,empt.sal</em>2 two2sal from empt order by two2sal;：查询员工编号，按照双倍工资显示</li></ul><ol start="2"><li>分区排序</li></ol><ul><li>select * from empt distribute by month sort by empno desc;：查询月份分区表按照员工编号做降序排序</li></ul><h2 id="分桶"><a href="#分桶" class="headerlink" title="分桶"></a>分桶</h2><ol><li>分区表分的时数据的存储路径</li><li>分桶是针对数据文件，将数据集整理成了若干个便于管理的部分（抽样测试）</li><li>分桶操作</li></ol><ul><li>create table emp_buck(id int,name string) clustered by (id) into 4 buckets row format delimited  fields terminated by “\t”;：创建一个拥有四个分桶的分桶表</li><li>insert into table emp_buck select * from emp_b;：使用子查询的方法向分桶表中导入数据（使用元数据导入将不会产生分桶）</li><li>set hive.enforce.bucketing=true;：设置不让reduce的个数影响子查询导入的结果</li><li>select * from emp_buck tablesample(bucket x out of y on id);</li><li>y必须是table总bucket数的倍数或者因子。hive根据y的大小，决定抽样的比例。例如，table总共分了64份，当y=32时，抽取(64/32=)2个bucket的数据，当y=128时，抽取(64/128=)1/2个bucket的数据。x表示从哪个bucket开始抽取。例如，table总bucket数为32，tablesample(bucket 3 out of 16)，表示总共抽取（32/16=）2个bucket的数据，分别为第3个bucket和第（3+16=）19个bucket的数据。</li></ul><hr><h1 id="Hive优化"><a href="#Hive优化" class="headerlink" title="Hive优化"></a><strong>Hive优化</strong></h1><h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><ol><li>开启Map阶段输出压缩</li></ol><ul><li>set hive.exec.compress.intermediate=true;：开启输出压缩功能</li><li>set mapreduce.map.output.compress=true;：开启Map输出压缩功能</li><li>set mapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;：设置压缩方式为SnappyCodec</li></ul><ol start="2"><li>开启Reduce阶段输出压缩</li></ol><ul><li>set hive.exec.compress.output=true;：开启最终输出压缩功能</li><li>set mapreduce.output.fileoutputformat.compress=true;：开启Reduce最终数据压缩功能</li><li>set mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.SnappyCodec;：设置Reduce最终输出压缩方式为SnappyCodec</li><li>set mapreduce.output.fileoutputformat.compress.type=BLOCK;：开启块压缩<h2 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h2></li></ul><ol><li>hive存储格式：TextFile、SequenceFile、orc（列存储）、Parquet（列存储）</li></ol><ul><li>orc：Index Data，轻量级索引，默认每一万行做一个索引，rew Data，存储具体数据，stripe Footer，存储流的类型</li></ul><ol start="2"><li>create table emp_stored(in int,name string) row format delimited fieleds terminated by “\t” stored as orc;：创建存储格式为orc的表</li><li>压缩比：orc &gt; parquet &gt; textFile<h2 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h2></li><li>set hive.map.aggr=true;：在Map端进行聚合</li><li>set hive.groupby.skewindata=true;：负载均衡</li><li>jvm 重用：<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop:mapred-site.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;mapreduce.job.jvm.numtasks&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;10~20&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>HBase开发</title>
      <link href="/2018/09/21/HBase%E5%BC%80%E5%8F%91/"/>
      <url>/2018/09/21/HBase%E5%BC%80%E5%8F%91/</url>
      
        <content type="html"><![CDATA[<hr><h1 id="NoSQL简介"><a href="#NoSQL简介" class="headerlink" title="NoSQL简介"></a>NoSQL简介</h1><ul><li>一、Apache Hbase是Hadoop数据库，是一个分布式，可扩展的大数据存储。当您需要对大数据进行随机，实时读/写访问时，请使用Apache Hbase。该项目的目标是托管非常大的表————数十亿行X百万列在商品硬件集群上，Apache Hbase是一个开源的，分布式的版本化的非关系化数据库。<br>2006年-google发表了bigdata白皮书<br>2006年-同年开始开发Hbase<br>2008年-Hbase正式成为apache子项目<br>2010年-正式成为apache顶级项目</li><li>二、常见NoSQL数据库<blockquote><p>HBase<br>Redis：基于内存的NoSQL数据库，前身MemCached（不支持持久化）<br>MongoDB：基于文档型（BSON）的NoSQL数据库<br>Cassandra：跟HBase类似</p></blockquote></li></ul><hr><h1 id="HBsae的体系架构（主从结构）和表结构"><a href="#HBsae的体系架构（主从结构）和表结构" class="headerlink" title="HBsae的体系架构（主从结构）和表结构"></a>HBsae的体系架构（主从结构）和表结构</h1><ul><li>一、Hadoop的生态体系圈</li><li><p>二、HBase（基于HDFS之上的NoSQL数据库）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HBase      HDFS</span><br><span class="line">表    ---&gt; 目录</span><br><span class="line">数据  ---&gt; 文件（HFile，默认大小：128MB）</span><br></pre></td></tr></table></figure></li><li><p>三、HBase的体系架构：主节点HMaster + 从节点HregionServer（通过zookeeper解决单点故障）</p></li></ul><hr><h1 id="HBase的搭建模式"><a href="#HBase的搭建模式" class="headerlink" title="HBase的搭建模式"></a>HBase的搭建模式</h1><ul><li>一、需要准备Hadoop集群和Zookeeper集群</li><li>二、上传并解压软件包</li><li><p>三、修改配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">hbase-env.sh：</span><br><span class="line">export JAVA_HOME=/root/training/jdk1.8.0_144</span><br><span class="line">export HBASE_MANAGES_ZK=false</span><br><span class="line"></span><br><span class="line">hbase-site.xml：</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;hdfs://bigdata211:9000/hbase&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hbase.cluster.distributed&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;true&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hbase.master.port&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;16000&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;bigdata211:2181,bigdata212:2181,bigdata213:2181&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">        &lt;property&gt;</span><br><span class="line">                &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt;</span><br><span class="line">                &lt;value&gt;/root/training/zookeeper/zkData&lt;/value&gt;</span><br><span class="line">        &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br><span class="line"></span><br><span class="line">regionserver：</span><br><span class="line">bigdata211</span><br><span class="line">bigdata212</span><br><span class="line">bigdata213</span><br></pre></td></tr></table></figure></li><li><p>四、解决依赖问题，把相关版本的zookeeper和hadoop依赖包导入到hbase/lib下</p></li><li>五、软连接hadoop配置<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@bigdata211 flume]# ln -s /root/training/hadoop-2.7.3/etc/hadoop/hdfs-site.xml /root/training/hbase/conf</span><br><span class="line">[root@bigdata211 flume]# ln -s /root/training/hadoop-2.7.3/etc/hadoop/core-site.xml /root/training/hbase/conf</span><br></pre></td></tr></table></figure></li></ul><hr><h1 id="操作HBsae"><a href="#操作HBsae" class="headerlink" title="操作HBsae"></a>操作HBsae</h1><ul><li>一、启动：</li></ul><ol><li>启动dfs</li><li>启动yarn</li><li>启动zookeeper</li><li><p>启动hbase：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bin/hbase-daemon.sh start master</span><br><span class="line">bin/hbase-daemon.sh start regionserver</span><br></pre></td></tr></table></figure></li><li><p>webconlose：<a href="http://192.168.247.211:16010" target="_blank" rel="noopener">http://192.168.247.211:16010</a></p></li><li>启动客户端：bin/hbase shell</li></ol><ul><li>二、Hbase shell</li></ul><ol><li>帮助：help</li><li>显示服务器当前状态：hbase(main):002:0&gt; status ‘bigdata211’</li><li>查看表：list === timestamp时间戳，column列族</li><li>查看表内容：scan ‘user’</li><li>创建表：create ‘表名’,’columnFamily’ === columnFamily列族，创建时指定几个列族，插入式就可使用几个列族</li><li>插入记录：put ‘表名’,’rowkey’,’info:name’,’zfhzxg’ === rowkey行键，唯一，不重复</li><li>覆盖数据：put ‘表名’,’rowkey’,’info:name’,’zhxzgd’ === 保证表名，行键，列族，字段相同</li><li>查看第几条记录之后的数据：scan ‘user’,{STARTROW =&gt; ‘102’}</li><li>查看第几条记录到第几条记录的数据：scan ‘user’,{STARTROW =&gt; ‘102’，STOPROW =&gt; ‘103’}</li><li>查看表结构：describe ‘user’</li><li>变更表信息：alter ‘user’,{NAME =&gt; ‘info’,VERSIONS =&gt; ‘3’}</li><li>删除数据：delete ‘user’,’101’,’info:name’</li><li>删除rowkey为101的数据：deleteall ‘user’,’101’</li><li>清空表数据：truncate ‘user’</li><li>设置表为不可用状态：disable ‘user’</li><li>删除表：drop ‘user’ === 必须设置表状态为不可用</li><li>查看表记录数（以rowkey为基准）：count ‘emp’</li><li>查询指定数据：get ‘emp’,’102’,’info:high’或get ‘emp’,’102’</li></ol><hr><h1 id="Hbase数据读写流程"><a href="#Hbase数据读写流程" class="headerlink" title="Hbase数据读写流程"></a>Hbase数据读写流程</h1><ul><li>整体流程</li></ul><ol><li>client发出读写请求访问到zookeeper（元数据）</li><li>zookeeper返回元数据信息给client（root表所在位置信息，root表中存储了.META.表的元数据信息，.META.表存储了region的元数据信息）</li><li>根据元数据信息找到查询的数据所在的HregionServer，zookeeper-&gt;regionServer（每一个root表只对应一个region，不能切分，通过root表可以读取到.META.表的元数据信息）</li><li>根据root表信息找到.META.表（.META.可以存储在多个region中，访问的是存储业务数据的元数据信息）</li><li>根据.META.表元数据信息找到region</li><li>数据返回到client</li></ol><ul><li>HregionServer流程</li></ul><ol><li>客户端会通过region直接去Memstore（内存）中读取数据</li><li>如果数据存在，会直接返回数据</li><li>当Memstore中找不到时blockcache（与Memstore同属于一块内存的逻辑区域，Memstore会实时把数据落到blockcache中，blockcache再把数据落到Hfile中）</li><li>当blockcache中找不到是，将会再HDFS中查找Hfile</li><li>缓存返回（通过blockcache，Memstore层层返回）</li><li>Memstore将数据返回客户端（Memstore负责写入数据的缓存，blockcache负责读取数据的缓存，读写分离）</li></ol>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Hadoop免密码登陆原理</title>
      <link href="/2018/09/14/Hadoop%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E9%99%86%E5%8E%9F%E7%90%86/"/>
      <url>/2018/09/14/Hadoop%E5%85%8D%E5%AF%86%E7%A0%81%E7%99%BB%E9%99%86%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>###<strong>hadoop免密码登陆原理</strong></p><ul><li>*不对称密码<blockquote><p>密匙对（两个文件）：</p><blockquote><p>公钥–锁（.ssh/id_rsa）：给别人加密<br>私钥–钥匙（.ssh/id_rsa.pub）：给自己，解密</p></blockquote></blockquote></li><li><p>*对称加密（加密和解密使用同一文件）</p></li><li><p>一、生成密钥对（请求登陆方-A）</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure></li><li><p>二、把请求登陆方生成的公钥拷贝给被请求方（B）</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id -i .ssh/id_rsa.pub root@RedHat111</span><br></pre></td></tr></table></figure></li><li><p>三、B收到请求方发来的公钥，并自动保存在.ssh/authorized_keys文件</p></li><li><p>四、B随机产生一个字符串：hello（前三部是配置，第四步开始，是认证的过程）</p></li><li><p>五、B使用请求登陆方的公钥进行加密（<strong>*</strong>），并发回给A</p></li><li><p>六、A收到B发来的加密字符串，使用自己的私钥进行解密（hello）</p></li><li><p>七、A把解密后的字符串（hello）发回给B进行认证</p></li><li><p>八、B收到A解密的字符串（hello）</p></li><li><p>九、B对从A收到的字符串（hello）与自己生成的字符串（hello）进行对比</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>MapReduce笔记</title>
      <link href="/2018/09/10/MapReduce%E7%AC%94%E8%AE%B0/"/>
      <url>/2018/09/10/MapReduce%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<hr><p>#<strong>MapReduce高级特性</strong></p><h2 id="一、序列化"><a href="#一、序列化" class="headerlink" title="一、序列化"></a>一、序列化</h2><ul><li>核心接口：Writable</li><li>如果一个类实现了Writable该类的对象可以作为Key和Value<h2 id="二、排序"><a href="#二、排序" class="headerlink" title="二、排序"></a>二、排序</h2></li><li>规则：按照Key2排序（可以是基本数据类型，也可以是对象）<blockquote><p>基本数据类型：数字（默认升序），字符串（默认字典顺序）<br>可以通过创建自己的比较规则改变排序（extends IntWritable.Comparator/extends Text.Comparator）</p></blockquote></li><li>对象<blockquote><p>SQL排序：order by 列名、表达式、别名、序号 desc/asc（desc/asc只作用于最近的一列）<br>MapReduce排序：1.该对象必须是Key2；2.必须实现序列化接口Writable；3.对象必须是可排序的（自定义排序使用java.long 接口 Comparable）</p></blockquote></li><li>MR排序分类<blockquote><p>部分排序<br>全排序<br>辅助排序<br>二次排序</p></blockquote><h2 id="三、分区"><a href="#三、分区" class="headerlink" title="三、分区"></a>三、分区</h2></li><li>什么是分区：partition</li><li>查询：<blockquote><p>1.没有分区：执行全表扫描<br>2.有分区，只扫描分区</p></blockquote></li><li>分区的类型：<blockquote><p>Orcale：</p><blockquote><p>1.范围分区；<br>2.列表分区；<br>3.Hash分区；<br>4.Hash范围分区；<br>5.Hash列表分区<br>MR的分区：<br>默认情况下，MR的输出只有一个分区（一个分区就是一个文件）<br>自定义分区：按照字段进行分区（根据Map的输出&lt;Key2,Value2&gt;分区）</p></blockquote></blockquote></li><li>*通过SQL的执行计划，判断效率是否提高<h2 id="四、合并"><a href="#四、合并" class="headerlink" title="四、合并"></a>四、合并</h2></li><li>合并是一种特殊的Reduce</li><li>合并是在Map端执行一次合并，用于减少Mapper输出到Reduce的数据量，可以提高效率</li><li>平均值不能使用combiner</li><li>无论有没有combiner，都没不能改变Map和Reduce对应的数据类型<h2 id="MapReduce核心：Shuffle（洗牌）"><a href="#MapReduce核心：Shuffle（洗牌）" class="headerlink" title="*MapReduce核心：Shuffle（洗牌）"></a>*MapReduce核心：Shuffle（洗牌）</h2></li><li>Hadoop3.X之前会有数据落地（产生I/O操作）</li><li>map()方法写入数据到环形缓冲区</li><li>环形缓冲区达到80%后，发生溢写，进行分区、排序、合并（combiner可选）、归并</li><li>归并后，拷贝到内存缓冲</li><li>当内存不够时，溢出到磁盘</li><li>进行归并排序</li><li>相同key分组</li><li>传入Reduce()<h2 id="MapReduce优缺点"><a href="#MapReduce优缺点" class="headerlink" title="*MapReduce优缺点"></a>*MapReduce优缺点</h2></li><li>优点<blockquote><p>1.易于编程<br>2.良好的拓展性<br>3.高容错性<br>4.适合处理PB级别以上的离线处理</p></blockquote></li><li>缺点<blockquote><p>1.不擅长做实时计算<br>2.不擅长做流式计算（MR的数据源事静态的）<br>3.不支持DAG（有向图）计算（Spark）</p></blockquote></li></ul><hr><hr><p>#<strong>Mapper，Reduce和Driver</strong></p><ul><li><p>Mapper阶段：</p><blockquote><p>用户自定义Mapper类，要继承父类Mapper<br>Mapper的输入数据的kv对形式（kv类型可以自定义）<br>Mapper的map方法的重写（加入业务逻辑）<br>Mapper的数据输出kv对的形式（kv类型可以自定义）<br>map（）方法（maptext进程）对每个&lt;k,v&gt;只调用一次</p></blockquote></li><li><p>Reducer阶段；</p><blockquote><p>用户自定义Reduce类，要继承父类Reducer<br>Reducer的数据输入类型对应的是Mapper九段的输出数据类型<br>Reducer的reduce方法重写（加入业务逻辑）<br>ReduceText进程对每组的k的&lt;k,v&gt;组只调用一次</p></blockquote></li><li><p>Driver阶段</p><blockquote><p>mr程序需要一个Driver来进行任务的提交，提交的任务是一个描述了各种重要信息的job</p></blockquote></li><li><p>maptask流程</p><blockquote><p>并行度：<br>一个job任务map阶段并行度由客户端所提交的任务决定<br>每一个split分配一个maptask并行处理<br>默认情况下，split大小=blocksize<br>切片是针对每一个文件单独切片<br>流程：<br>准备数据wordcount<br>创建客户端，提交任务程序driver<br>逻辑运算<br>向环形缓冲区写数据&lt;k,v&gt;（默认大小100M）<br>当环形缓冲区内存占用达到80%，进行溢写（HashPratitioner分区，key.compareTo排序（索引））<br>溢写到文件（保证分区且区内是有序的）<br>merge归并排序</p></blockquote></li><li><p>reducetask流程</p><blockquote><p>reduceTask将相同分区的数据下载到reduceTask本地磁盘<br>再次合并文件，归并排序<br>合并过程中进行辅助排序<br>一次读一组，进行写出，生成结果文件</p></blockquote></li></ul><hr><hr><p>#<strong>Hadoop中所提供的数据序列化类型</strong></p><ul><li>int         &gt; IntWritable</li><li>float       &gt; FloatWritable</li><li>long        &gt; LongWritable</li><li>double      &gt; DoubleWritable</li><li>String      &gt; Text</li><li>boolean     &gt; BooleanWritable</li><li>byte        &gt; ByteWritable</li><li>Map         &gt; MapWritable</li><li>Arry        &gt; ArryWritable</li></ul><blockquote><p>为什么要序列化？<br>存储“活的对象”<br>什么是序列化？<br>就是把内存中的对象，转换成字节序列以便于存储和网络传输<br>反序列化<br>就是将收到的字节序列或者硬盘的持久化数据，转换成内存中的对象<br>Java中的序列化<br>Serializable<br>为什么不用Java提供的序列化接口？<br>Java的序列化是一个重量级的序列化，一个对象被序列化后会附带很多额外的信息（校验信息，Header，继承体系等），不便于在网络中的高效传输，所以Hadoop开发了一套序列化鸡之（Writable），精简/高效<br>为什么序列化在Hadoop中很重要？<br>Hadoop通信是通过远程调用（rpc）实现的，需要进行序列化<br>特点：<br>1）紧凑<br>2）快速<br>3）可拓展<br>4）互操作</p></blockquote><hr><hr><p>#<strong>MapReduce常用案例</strong></p><ul><li>一、数据去重<blockquote><p>相同Key名交给同一地址进行处理，处理后输出给Reduce，Key值唯一，Value形成数组</p></blockquote></li><li>二、多表查询（笛卡尔集：列数相加，行数相乘）<blockquote><p>等值链接的处理过程（以表作为Value1）</p><blockquote><p>Mapper阶段，通过分词后的列数或其他方法区分表<br>Mapper阶段，在字段前添加标识输出给Reduce<br>Reduce进行处理</p></blockquote></blockquote></li><li>三、自连接<blockquote><p>同一个表经Mapper输出两次<br>非法数据要先经过处理（数据清理）<br>Reduce进行处理</p></blockquote></li><li>四、倒排索引</li><li>五、单元测试（MRUnit）</li></ul><hr><hr><p>#<strong>MapReduce重点</strong></p><ul><li>一、WordCount案例、流量汇总案例与涉及知识点</li><li>二、yran集群部署安装</li><li>三、job任务提交流程</li><li>四、辅助排序、分区排序</li><li>五、MapReduce整体流程</li><li>六、数据压缩</li></ul><hr>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>zookeeper</title>
      <link href="/2018/09/05/zookeeper/"/>
      <url>/2018/09/05/zookeeper/</url>
      
        <content type="html"><![CDATA[<h1 id="zookeeper简介"><a href="#zookeeper简介" class="headerlink" title="zookeeper简介"></a>zookeeper简介</h1><ul><li>zookeeper 动物管理员<blockquote><p>apache zookeeper致力于开发和维护开源服务器，实现高度可靠的分布式协调</p></blockquote></li><li>什么是zookeeper<blockquote><p>zookeeper是一种集中式服务，用于维护配置信息，命名，提供分布式同步和提供组服务。所有这些类型的服务都以分布式应用程序的某种形式使用，每次实施他们都需要做很多工作来修复不可避免的错误和竞争条件。由于难以实现这些类型的服务，应用程序最初通常会吝啬他们，这使得他们在变化的情况下变得脆弱并且难以管理。即使正确完成，这些服务的不同实现也会在部署应用程序时导致管理复杂性。</p></blockquote></li></ul><h1 id="zookeeper功能"><a href="#zookeeper功能" class="headerlink" title="zookeeper功能"></a>zookeeper功能</h1><ul><li>1）存储数据：存储集群中每台机器都关心的数据（配置信息），需要接受服务器的注册</li><li>2）监听</li></ul><h1 id="zookeeper工作机制"><a href="#zookeeper工作机制" class="headerlink" title="zookeeper工作机制"></a>zookeeper工作机制</h1><ul><li>基于观察者模式设计的分布式服务管理框架</li><li>1）启动服务器，首先向zookeeper中注册信息，创建临时节点（通过目录结构/servers/）</li><li>2）获取服务器列表并且注册监听（当服务器下线，在zookeeper中对应服务器节点将会消失）</li><li>3）当服务器宕机，zookeeper将会通过监听功能（process(){}回调方法）向zookeeper发送下线通知</li><li>4）监听功能重新获取服务器列表并再次进行监听</li></ul><h1 id="zookeeper的存储结构（目录树存储结构）"><a href="#zookeeper的存储结构（目录树存储结构）" class="headerlink" title="zookeeper的存储结构（目录树存储结构）"></a>zookeeper的存储结构（目录树存储结构）</h1><ul><li>/ ——–&gt; 根目录（根目录下可以有多个节点）</li><li>z1、z2 ——–&gt; 节点目录（Znode，默认存储1M数据）</li><li>/z1/zz1 、 /z1/zz2 、 /z2/zz1</li></ul><h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1><ul><li>1）集群同一配置管理</li><li>2）集群同一命名服务</li><li>3）集群统一管理</li><li>4）服务器的动态上下线感知</li><li>5）负载均衡</li></ul><h1 id="zookeeper集群安装"><a href="#zookeeper集群安装" class="headerlink" title="zookeeper集群安装"></a>zookeeper集群安装</h1><ul><li>单节点安装<blockquote><p>1）解压安装包<br>2）重命名zookeeper/conf/zoo_sample.cfg 为 zoo.cfg<br>3）修改zoo.cfg配置文件中的dataDir参数为自定义路径<br>4）启动zk：bin/zkServer.sh start<br>5）查看状态：bin/zkServer.sh status<br>6）启动客户端：bin/zkCli.sh</p></blockquote></li><li>全分布安装  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1）zoo.cfg：</span><br><span class="line">    server.1=RedHat112:2888:3888</span><br><span class="line">    server.2=RedHat113:2888:3888</span><br><span class="line">    server.3=RedHat114:2888:3888</span><br><span class="line"></span><br><span class="line">2）在自定义dataDir路径下创建myid文件，并设置id</span><br><span class="line">myid：</span><br><span class="line">    1</span><br><span class="line"></span><br><span class="line">3）配置环境变量</span><br><span class="line"></span><br><span class="line">4）将环境变量和zookeeper发送到其他服务器</span><br><span class="line">scp -zxvf -r zookeeper RedHat113:zookeeper</span><br><span class="line">scp -zxvf -r zookeeper RedHat114:zookeeper</span><br><span class="line"></span><br><span class="line">5)修改其他服务器myid</span><br><span class="line"></span><br><span class="line">6)生效环境变量</span><br></pre></td></tr></table></figure></li></ul><h1 id="zookeeper操作"><a href="#zookeeper操作" class="headerlink" title="zookeeper操作"></a>zookeeper操作</h1><ul><li>启动shell客户端：zkCli.sh</li><li>zk：操作日志</li><li>ls path [watch]：查看path节点所包含的内容[监听]</li><li>ls2 path [watch]：查看path节点的详细信息[监听]</li><li>gat path [watch]：查看path节点所存储的数据<blockquote><p>get path watch：监听节点值；get path watch：监听路径<br>cZxid：事务id，每次创建都以时间戳的形式产生唯一的Zxid<br>ctime：节点创建时间<br>mZxid：节点最后修改的时间戳id<br>mtime：节点最后修改的时间<br>pZxid：节点最后更新的子节点时间戳id<br>cversion：节点修改次数<br>dataVersion：节点数据的变化号<br>dataLength：数据长度<br>numChild：子节点数</p></blockquote></li><li>stat path：查看节点的状态信息</li><li>set path acl：更改path节点中的数据</li><li>create [-s] [-e] path data acl：创建路径，data为创建目录节点所存储的数据，acl为应答类型，[-s]带序号的节点，[-e]短暂节点</li><li>delete path：删除节点（不能删除带有子节点的节点）</li><li>rmr path：递归删除</li></ul><h1 id="zookeeper选举机制"><a href="#zookeeper选举机制" class="headerlink" title="zookeeper选举机制"></a>zookeeper选举机制</h1><ul><li>当ZK1服务器启动时，先给自己投一票，ZK1作为follower</li><li>当ZK2服务器启动时，给自己投一票的同时，ZK1也会给ZK2投一票，票数占半数以上，ZK2作为leader</li><li>当ZK3服务器启动时，给自己投一票，此时发现已经存在leader，ZK3作为follower</li></ul><h1 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h1><ul><li>只要有半数以上的节点存活，就能够正常工作（zk集群要求配置奇数台）</li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Hadoop 2.X管理与开发（二）</title>
      <link href="/2018/09/05/Hadoop-2-X%E7%AE%A1%E7%90%86%E4%B8%8E%E5%BC%80%E5%8F%91%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2018/09/05/Hadoop-2-X%E7%AE%A1%E7%90%86%E4%B8%8E%E5%BC%80%E5%8F%91%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>#Hadoop数据压缩</p><h2 id="数据压缩"><a href="#数据压缩" class="headerlink" title="数据压缩"></a>数据压缩</h2><ul><li>1）MR操作过程中进行大量数据传输，就需要对数据进行压缩</li><li>2）压缩技术能够有效减少底层存储（HDFS）读写字节数，提高的网络带宽和磁盘空间的效率</li><li>3）数据压缩能够有效节省资源</li><li>4）压缩事MR程序的优化策略</li><li>5）通过压缩编码对Mapper或者reduce数据传输进行的压缩，以减少磁盘IO</li></ul><h2 id="压缩的基本原则"><a href="#压缩的基本原则" class="headerlink" title="压缩的基本原则"></a>压缩的基本原则</h2><ul><li>1）运算密集型任务少用压缩</li><li>2）IO密集型的任务，多用压缩</li></ul><h2 id="MR支持的压缩编码"><a href="#MR支持的压缩编码" class="headerlink" title="MR支持的压缩编码"></a>MR支持的压缩编码</h2><ul><li>DEFAULT    是自带编码    .default    不可切分</li><li>Gzip       是自带编码    .gz         不可切分</li><li>bzip2      是自带编码    .bz2        可以切分</li><li>LZO        非自带编码    .lzo        可以切分</li><li>Snappy     非自带编码    .Snappy     不可切分</li></ul><h2 id="编码-解码器"><a href="#编码-解码器" class="headerlink" title="编码/解码器"></a>编码/解码器</h2><p>DEFAULT       org.apache.hadoop.io.compress.DefaultCodec<br>Gzip          org.apache.hadoop.io.compress.GzipCodec<br>bzip2         org.apache.hadoop.io.compress.BZip2Codec<br>LZO           com.hadoop.compression.lzo.lzoCodec<br>Snappy        org.apache.hadoop.io.compress.SnappyCodec</p><h2 id="压缩性能"><a href="#压缩性能" class="headerlink" title="压缩性能"></a>压缩性能</h2><p>Gzip      原大小：8.3GB      压缩后：1.8GB      压缩速度：17.5MB/s      解压速度：58MB/s<br>bzip2     原大小：8.3GB      压缩后：1.1GB      压缩速度：2.4MB/s       解压速度：9.5MB/s<br>LZO       原大小：8.3GB      压缩后：2.9GB      压缩速度：49.3MB/s      解压速度：74.6MB/s</p><h2 id="设置压缩方式"><a href="#设置压缩方式" class="headerlink" title="设置压缩方式"></a>设置压缩方式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mapper端：</span><br><span class="line">    //开启map端的输出压缩</span><br><span class="line">conf.setBoolean(&quot;mapreduce.map.outpot.compress&quot;, true);</span><br><span class="line">//设置压缩方式</span><br><span class="line">//conf.setClass(&quot;mapreduce.map.outpot.compress.codec&quot;, DefaultCodec.class, CompressionCodec.class);</span><br><span class="line">conf.setClass(&quot;mapreduce.map.outpot.</span><br><span class="line">        compress.codec&quot;, BZip2Codec.class, CompressionCodec.class);</span><br><span class="line"></span><br><span class="line">reduce端：</span><br><span class="line">    //开启reduce端的输出压缩</span><br><span class="line">FileOutputFormat.setCompressOutput(job, true);</span><br><span class="line">//设置压缩方式</span><br><span class="line">//FileOutputFormat.setOutputCompressorClass(job, DefaultCodec.class);</span><br><span class="line">FileOutputFormat.setOutputCompressorClass(job, BZip2Codec.class);</span><br></pre></td></tr></table></figure><h1 id="Hadoop优化"><a href="#Hadoop优化" class="headerlink" title="Hadoop优化"></a>Hadoop优化</h1><h2 id="MapReduce程序的效率瓶颈"><a href="#MapReduce程序的效率瓶颈" class="headerlink" title="MapReduce程序的效率瓶颈"></a>MapReduce程序的效率瓶颈</h2><h3 id="相关"><a href="#相关" class="headerlink" title="相关"></a>相关</h3><ul><li>MR功能：分布式离线计算</li><li>计算机性能：CPU、内存、磁盘、网络</li><li>I/O操作优化：<blockquote><p>数据倾斜（代码优化）<br>map和reduce的个数设置不合理<br>map运行时间太长，导致reduce等待时间过久<br>小文件过多（CombineTextInputFormat小文件合并）<br>不可分快的超大文件（不断的溢写）<br>多个溢写小文件需要多次merge</p></blockquote><h3 id="优化方法"><a href="#优化方法" class="headerlink" title="优化方法"></a>优化方法</h3></li><li>数据输入<blockquote><p>合并小文件：在执行MR任务前就进行小文件合并<br>采用CombineTextInputFormat来作为输入来作为输入端大量小文件的场景</p></blockquote></li><li><p>Map阶段</p><blockquote><p>减少溢写次数（增加内存200MB 80%）：减少磁盘I/O</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.task.io.sort.mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;200&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.map.sort.spill.percent&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0.80&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>减少合并的次数</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.task.io.sort.factor&lt;/name&gt;</span><br><span class="line">&lt;value&gt;10&lt;/value&gt; //文件的个数，数值越大合并次数越少</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>在map之后，不影响业务逻辑的情况下可以使用combiner</p></blockquote></li><li><p>Reduce阶段</p><blockquote><p>合理的设置map与reduce的个数<br>设置map/reduce共存</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.job.reduce.skowstart.completedmaps&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0.05&lt;/value&gt; //设置运行一定程度的map后，启动reduce，减少等待时间</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>合理设置reduce的buffer</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.reduce.markreset.buffer.percent&lt;/name&gt;</span><br><span class="line">&lt;value&gt;0.0&lt;/value&gt; </span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote></li><li><p>I/O传输</p><blockquote><p>进行数据压缩<br>使用sequenceFile</p></blockquote></li><li>数据倾斜<blockquote><p>进行范围分区<br>自定义分区<br>Combine<br>能用mapJoin的坚决不用reduceJoin</p></blockquote></li><li><p>参数调优</p><blockquote><p>分配map程序CPU核心数</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.map.cpu.vcores&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1&lt;/value&gt; //核心数</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>分配reduce程序CPU核心数</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.reduce.cpu.vcores&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1&lt;/value&gt; //核心数</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>设置maptask内存</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.map.memory.mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1024&lt;/value&gt; //一个maptask能够使用的内存上限</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>设置reducetask内存</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.reduce.memory.mb&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1024&lt;/value&gt; //一个maptask能够使用的内存上限</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>reduce去map端并行度</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mapred-default.xml:</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;mapreduce.reduce.shuffle.parallelcopies&lt;/name&gt;</span><br><span class="line">&lt;value&gt;5&lt;/value&gt; //当reduce去map端拿取数据时所开的并行数是5</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Hadoop 2.X管理与开发（一）</title>
      <link href="/2018/09/05/Hadoop-2-X%E7%AE%A1%E7%90%86%E4%B8%8E%E5%BC%80%E5%8F%91%EF%BC%88%E4%B8%80%EF%BC%89/"/>
      <url>/2018/09/05/Hadoop-2-X%E7%AE%A1%E7%90%86%E4%B8%8E%E5%BC%80%E5%8F%91%EF%BC%88%E4%B8%80%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="hadoop的起源与背景知识"><a href="#hadoop的起源与背景知识" class="headerlink" title="hadoop的起源与背景知识"></a><strong>hadoop的起源与背景知识</strong></h1><h2 id="一、什么是大数据"><a href="#一、什么是大数据" class="headerlink" title="一、什么是大数据"></a>一、什么是大数据</h2><blockquote><p>举例:</p><blockquote><p>1.电商的推荐系统(可能会用到推荐算法:协同过滤,ALS,逻辑回归…)<br>2.天气预报</p></blockquote></blockquote><blockquote><p>核心的问题:数据的存储,数据的计算(不是算法)</p></blockquote><blockquote><p>数据的存储:采用分布式的文件系统HDFS(hadoop Distributed file system)</p></blockquote><blockquote><p>数据的计算:采用分布式的计算MapReduce,Spark(RDD:弹性分布式数据集)</p></blockquote><h2 id="二、数据仓库和大数据"><a href="#二、数据仓库和大数据" class="headerlink" title="二、数据仓库和大数据"></a>二、数据仓库和大数据</h2><h3 id="传统方式-搭建数据仓库-Data-Werehouse-来解决大数据的问题"><a href="#传统方式-搭建数据仓库-Data-Werehouse-来解决大数据的问题" class="headerlink" title="传统方式:搭建数据仓库(Data Werehouse)来解决大数据的问题"></a>传统方式:搭建数据仓库(Data Werehouse)来解决大数据的问题</h3><blockquote><p>1.数据仓库就是一个数据库(Orcale,MySQL,MS)<br>2.数据仓库和大数据一般只做查询(分析)<br>3.搭建数据仓库的过程</p><blockquote><p>(1)数据源RDBMS:关系型数据库(结构化数据)/文本数据/其他数据<br>(2)利用ETL抽取/转化/加载数据后搭建数据仓库(保存为原始数据)<br>(3)分析处理数据(SQL,PL/SQL,JDBC)<br>(4)经过分析后搭建数据集市(hr,sales)<br>(5)提供给hr系统或销售系统</p></blockquote></blockquote><h3 id="Hadoop和Spark都可以看成是数据仓库的一种实现"><a href="#Hadoop和Spark都可以看成是数据仓库的一种实现" class="headerlink" title="Hadoop和Spark都可以看成是数据仓库的一种实现"></a>Hadoop和Spark都可以看成是数据仓库的一种实现</h3><blockquote><p>1.从ETL到搭建数据集都可以用Hadoop和Spark所提供的方式解决,也可以用传统方式解决<br>2.hadoop中使用sqoop和Flume组件完成ETL<br>3.hadoop中使用HDFS存储数据(或存入基于HDFS之上的HBase或Hive数据仓库)<br>4.hadoop中使用MapReduce(java程序)或Spark(Scala程序,只有数据计算,没有数据存储)或SQL处理数据<br>5.处理数据后使用HDFS或NoSQL:Redis存储到数据集市<br>*.大数据的终极目标是使用SQL处理数据</p></blockquote><h2 id="三、OLTP和OLAP"><a href="#三、OLTP和OLAP" class="headerlink" title="三、OLTP和OLAP"></a>三、OLTP和OLAP</h2><blockquote><p>LTP:Online Transaction processing 联机事务处理，指：insert、update、delete —&gt; 事务</p></blockquote><blockquote><p>OLAP:Online Analytic Processing 联机分析处理，一般只做查询 —&gt; 数据仓库就是一种OLAP的应用系统</p></blockquote><blockquote><p>Hadoop、Spark看成是一种数据仓库的解决方案</p></blockquote><blockquote><p>数据仓库（查询）不支持事务</p></blockquote><h2 id="四、Google的基本思想-三篇论文"><a href="#四、Google的基本思想-三篇论文" class="headerlink" title="四、Google的基本思想:三篇论文"></a>四、<strong>Google的基本思想:三篇论文</strong></h2><h3 id="一-GFS（Google-File-System）-—-HDFS（Hadoop-Distributed-File-System）：分布式文件系统"><a href="#一-GFS（Google-File-System）-—-HDFS（Hadoop-Distributed-File-System）：分布式文件系统" class="headerlink" title="(一)GFS（Google File System）  —- HDFS（Hadoop Distributed File System）：分布式文件系统"></a>(一)GFS（Google File System）  —- HDFS（Hadoop Distributed File System）：分布式文件系统</h3><blockquote><p>(*) HDFS = NameNode + SecondaryNameNode + DataNode<br>1.分布式文件系统<br>2.大数据的存储问题<br>3.HDFS中，记录数据的位置信息（元信息） —- 采用倒排索引（Reverted Index）</p><blockquote><p>(1)什么是索引？Index<br>    * CREATE INDEX创建索引<br>    * 索引就是一个记录（Oracle中索引表保存的是有规律的行地址）<br>    * 通过索引可以找到对应的数据<br>(2)什么是倒排索引？<br>    * 最简单的倒排索引：单词表（wordID，word，index）<br>(3)NameNode（主节点，名称节点）是整个HDFS的管理员，和SecondaryNameNode（第二名称节点）同处一台主机，负责管理DateNode（从节点，数据节点），并不负责存储，与DateNode（从节点，数据节点）构成HDFS环境</p></blockquote></blockquote><h3 id="二-MapReduce：分布计算模型，问题来源PageRank（网页排名）"><a href="#二-MapReduce：分布计算模型，问题来源PageRank（网页排名）" class="headerlink" title="(二)MapReduce：分布计算模型，问题来源PageRank（网页排名）"></a>(二)MapReduce：分布计算模型，问题来源PageRank（网页排名）</h3><blockquote><p>1.PageRank（网页排名）<br>2.MapReduce的标程模型：</p><blockquote><p>*. 核心：先拆分（拆分计算，Map阶段），再合并（Reduce阶段）<br>*. MR任务：job=map+reduce<br>*. Map的输出同时也是Reduce的输入<br>*. 一个MR任务一共存在四对输入和输出（<key value="">），Map的输入和输出，Reduce的输入和输出<br>*. k2=k3，v2和v3数据类型一致，v3是一个集合，该集合中的每个值就是v2<br>*. 所有的<key value="">数据类型必须是Hadoop自己的数据类型（为了实现Hadoop的序列化机制）<br>*. MR任务处理的是HDFS上的数据<br>*. Hadoop2.X开始，通过Yarn容器编程部署MR任务（ResourceManager&lt;主节点&gt; + NodeManager&lt;从节点&gt;）<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Example:/root/training/Hadoop-2.7.3/Share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar</span><br><span class="line">Yarn的web console:http://192.168.226.11:8088</span><br><span class="line">命令://hadoop jar hadoop-mapreduce-examples-2.7.3.jar wordcount /input/data.txt /out/wcl</span><br></pre></td></tr></table></figure></key></key></p></blockquote></blockquote><h3 id="三-BigTable：大表-—-NoSQL数据库：HBase"><a href="#三-BigTable：大表-—-NoSQL数据库：HBase" class="headerlink" title="(三)BigTable：大表 —- NoSQL数据库：HBase"></a>(三)BigTable：大表 —- NoSQL数据库：HBase</h3><blockquote><p>1.关系型数据库：以二维表的形式保存数据<br>2.大表的基本思想：所有的数据存入一张表（通过牺牲空间，换取时间）<br>3.常见的NoSQL数据库（一般为行式数据库，适合）</p><blockquote><p>* Redis：内存数据库（一般为行式数据库，适合DML操作，insert，update，delect）<br>* MongoDB：面向文档（BSON文档：是JSON的二进制）<br>* HBase：面向列（列式数据库，查询，select，在HBase中，rowKey不能为null，但是可以重复，相同的ey是一条记录）<br>    <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HBase = ZooKeeper + HMaster（主节点） + RegionServer（从节点）</span><br></pre></td></tr></table></figure></p></blockquote></blockquote><hr><h1 id="hadoop的环境"><a href="#hadoop的环境" class="headerlink" title="hadoop的环境"></a><strong>hadoop的环境</strong></h1><h2 id="一、Hadoop的目录结构"><a href="#一、Hadoop的目录结构" class="headerlink" title="一、Hadoop的目录结构"></a>一、Hadoop的目录结构</h2><blockquote><p>hadoop-2.7.3/  —&gt;  Hadoop的HOME目录<br>bin/  —&gt;  Hadoop的操作命令<br>etc/hadoop/  —&gt;  所有的配置文件<br>sbin/  —&gt;  Hadoop集群的命令：启动，停止等<br>share/  —&gt;  所有共享文件<br>share/hadoop  –&gt;  所有依赖jar包</p></blockquote><h2 id="二、Hadoop的三种安装模式"><a href="#二、Hadoop的三种安装模式" class="headerlink" title="二、Hadoop的三种安装模式"></a>二、Hadoop的三种安装模式</h2><h3 id="一-本地模式"><a href="#一-本地模式" class="headerlink" title="(一)本地模式"></a>(一)本地模式</h3><blockquote><p>* 没有HDFS，只能测试MapReduce程序（不是运行在Yarn中，作为一个独立的Java程序来运行）<br>* 配置文件：etc/Hadoop/hadoop-env.sh export JAVA_HOME={JAVA_HOME} 自己的JAVA_HOME路径<br>* 本地模式中用到的路径都是本地路径，因为没有HDFS</p></blockquote><h3 id="二-伪分布模式"><a href="#二-伪分布模式" class="headerlink" title="(二)伪分布模式"></a>(二)伪分布模式</h3><blockquote><p>* 特点：再单击上模拟一个分布式的环境，具备Hadoop的所有功能<br>* 具备HDFS：NameNode + DataNode + SecondaryNameNode（端口50070）<br>* 具备Yarn：ResourceManager + NodeManager（端口8088）<br>* 对HDFS的NameNode进行格式化（/root/training/hadoop-2.7.3/tmp）<br>* 启动HDFS：start-dfs.sh<br>* 启动Yarn：start-yarn.sh<br>* 统一启动：start-all.sh<br>* 配置文件：etc/Hadoop/hadoop-env.sh export JAVA_HOME={JAVA_HOME} 自己的JAVA_HOME路径<br>* 配置文件： </p></blockquote>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">//hdfs-site.xml:</span><br><span class="line"></span><br><span class="line">//配置数据块的冗余度</span><br><span class="line">//原则冗余度跟数据节点的个数保持一致，最大不要超过3</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.replication&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;1&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">//是否开启HDFS权限检查，默认值为true（使用默认值，需要再改）</span><br><span class="line">&lt;!--</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.permissions&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">--&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//core-site.xml:</span><br><span class="line"></span><br><span class="line">//配置HDFS主节点的地址，就是NameNode的地址</span><br><span class="line">//9000是RPC的通信端口</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://RedHat111:9000&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">//HDFS数据块和元信息保存在操作系统的目录位置</span><br><span class="line">//默认值是Linux的tmp目录</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;/root/training/hadoop-2.7.3/tmp&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//mapred-site.xml:（默认没有这个文件）</span><br><span class="line">//MR程序运行的程序或框架</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">//yarn-site.xml:</span><br><span class="line">//配置yarn主节点的位置</span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;RedHat111&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><h3 id="三-全分布模式"><a href="#三-全分布模式" class="headerlink" title="(三)全分布模式"></a>(三)全分布模式</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">(1)hdfs-site.xml:</span><br><span class="line">    &lt;!—配置数据块的冗余度，默认是3—&gt;</span><br><span class="line">    &lt;!—原则冗余度跟数据节点保持一致，最大不要超过3—&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dis.replication&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;2&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!—是否开启HDFS权限检查，默认是true—&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dis.permissions&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;false&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt; </span><br><span class="line">    </span><br><span class="line">(2)core-site.xml:</span><br><span class="line">    &lt;!—配置HDFS主节点位置，就是NameNode的位置—&gt;</span><br><span class="line">    &lt;!—9000是RPC的通信端口—&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hdfs://RedHat112:9000&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!—HDFS元信息和数据块保存在操作目录的位置—&gt;</span><br><span class="line">    &lt;!—默认是系统的tmp文件夹，会随断电而清除—&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;/root/training/hadoop-2.7.3/tnp&lt;/value&gt;</span><br><span class="line">    &lt;/porperty&gt;</span><br><span class="line">    </span><br><span class="line">(3)mapped-site.xml</span><br><span class="line">    &lt;!—MR运行容器或框架—&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">(4)yarn-site.xml</span><br><span class="line">    &lt;!—配置yarn主节点的位置—&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;RedHat112&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!—NodeManager执行MR任务的方式是Shuffle洗牌—&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line"></span><br><span class="line">(4)slaves配置从节点地址:</span><br><span class="line">    RedHat113</span><br><span class="line">    RedHat114</span><br><span class="line">    </span><br><span class="line">(5)对NameNode进行格式化</span><br><span class="line"></span><br><span class="line">(6)把RedHat上安装好的目录复制到从节点上</span><br><span class="line">    scp -r hadoop-2.7.3/ root@RedHat113:/root/training</span><br><span class="line">    scp -r hadoop-2.7.3/ root@RedHat114:/root/training</span><br><span class="line">    </span><br><span class="line">(7)在主节点上启动集群</span><br><span class="line">    start-all.sh</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>基础命令</title>
      <link href="/2018/09/05/%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/"/>
      <url>/2018/09/05/%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h1 id="文件目录操作命令"><a href="#文件目录操作命令" class="headerlink" title="文件目录操作命令"></a><strong>文件目录操作命令</strong></h1><h2 id="ls-显示文件和目录列表"><a href="#ls-显示文件和目录列表" class="headerlink" title="ls 显示文件和目录列表"></a>ls 显示文件和目录列表</h2><blockquote><p>-l 列出文件的详细信息<br>-a 列出当前目录所有文件，包含隐藏文件<br>*设置环境变量：/root/.bash_profile</p></blockquote><h2 id="pwd-显示当前目录位置"><a href="#pwd-显示当前目录位置" class="headerlink" title="pwd 显示当前目录位置"></a>pwd 显示当前目录位置</h2><h2 id="mkdir-创建目录"><a href="#mkdir-创建目录" class="headerlink" title="mkdir 创建目录"></a>mkdir 创建目录</h2><blockquote><p>-p 父目录不存在的情况下先生成父目录<br>约定：</p><blockquote><p>mkdir /root/tools —-&gt;安装包<br>mkdir /root/training —-&gt;安装目录</p></blockquote></blockquote><h2 id="cd-切换目录"><a href="#cd-切换目录" class="headerlink" title="cd 切换目录"></a>cd 切换目录</h2><h2 id="touch-生成一个空文件"><a href="#touch-生成一个空文件" class="headerlink" title="touch 生成一个空文件"></a>touch 生成一个空文件</h2><h2 id="echo-生成一个带内容文件"><a href="#echo-生成一个带内容文件" class="headerlink" title="echo 生成一个带内容文件"></a>echo 生成一个带内容文件</h2><blockquote><p>使用echo查看环境变量值：echo $JAVA_HOME</p></blockquote><h2 id="cat、tac-显示文本文件内容"><a href="#cat、tac-显示文本文件内容" class="headerlink" title="cat、tac 显示文本文件内容"></a>cat、tac 显示文本文件内容</h2><blockquote><p>cat是从第一行开始写，tac是从最后一行开始写</p></blockquote><h2 id="cp-复制文件或目录"><a href="#cp-复制文件或目录" class="headerlink" title="cp 复制文件或目录"></a>cp 复制文件或目录</h2><h2 id="rm-删除文件"><a href="#rm-删除文件" class="headerlink" title="rm 删除文件"></a>rm 删除文件</h2><blockquote><p>-r 同时删除该目录下的所有文件<br>-f 强制删除文件或目录<br>*HDFS有回收站，默认情况下关闭</p></blockquote><h1 id="系统操作命令"><a href="#系统操作命令" class="headerlink" title="系统操作命令"></a><strong>系统操作命令</strong></h1><h2 id="ps-显示瞬间的进程状态"><a href="#ps-显示瞬间的进程状态" class="headerlink" title="ps 显示瞬间的进程状态"></a>ps 显示瞬间的进程状态</h2><blockquote><p>ps -ef：使用标准格式显示每个进程信息</p></blockquote><h2 id="hostname-显示主机名"><a href="#hostname-显示主机名" class="headerlink" title="hostname 显示主机名"></a>hostname 显示主机名</h2><h2 id="kill-杀死一个进程"><a href="#kill-杀死一个进程" class="headerlink" title="kill 杀死一个进程"></a>kill 杀死一个进程</h2><blockquote><p>-9 强制杀死一个进程<br>-3 如果针对java进程，打印java进程的线程信息Thread Dump</p></blockquote><h1 id="打包命令"><a href="#打包命令" class="headerlink" title="打包命令"></a><strong>打包命令</strong></h1><h2 id="gzip-压缩（解压）文件或目录，压缩文件后缀为gz"><a href="#gzip-压缩（解压）文件或目录，压缩文件后缀为gz" class="headerlink" title="gzip 压缩（解压）文件或目录，压缩文件后缀为gz"></a>gzip 压缩（解压）文件或目录，压缩文件后缀为gz</h2><h2 id="bzip2-压缩（解压）文件或目录，压缩文件后缀为bz2"><a href="#bzip2-压缩（解压）文件或目录，压缩文件后缀为bz2" class="headerlink" title="bzip2 压缩（解压）文件或目录，压缩文件后缀为bz2"></a>bzip2 压缩（解压）文件或目录，压缩文件后缀为bz2</h2><h2 id="tar-文件、目录打（解）包"><a href="#tar-文件、目录打（解）包" class="headerlink" title="tar 文件、目录打（解）包"></a>tar 文件、目录打（解）包</h2><blockquote><p>-zxvf 解压缩<br>-C 解压到指定目录下</p></blockquote><h1 id="权限管理（非常类似HDFS权限管理）"><a href="#权限管理（非常类似HDFS权限管理）" class="headerlink" title="权限管理（非常类似HDFS权限管理）"></a><strong>权限管理（非常类似HDFS权限管理）</strong></h1><h2 id="权限的类型"><a href="#权限的类型" class="headerlink" title="权限的类型"></a>权限的类型</h2><blockquote><p>r 读<br>w 写<br>x 执行</p></blockquote><h2 id="ls-l、ll-查看权限"><a href="#ls-l、ll-查看权限" class="headerlink" title="ls -l、ll 查看权限"></a>ls -l、ll 查看权限</h2><blockquote><p>十位字符，第一位如果为d，则代表该文件为目录<br>后九位分为三组，每一组都按读写执行的顺序排列</p><blockquote><p>第一组：当前用户<br>第二组：同组用户<br>第三组：其他用户<br>权限用二进制表示，有为1，没有为0（rwx -&gt; 111，rw- -&gt; 110）<br>chmod：改变权限<br>chmod (u/g/o/a)+(r/w/x) 文件名（括号内可选）<br>chmod 777 文件名（每一位十进制数代表一组权限，此处为所有用户可读可写可执行）</p></blockquote></blockquote><h2 id="案例分析-java的死锁或者性能瓶颈分析"><a href="#案例分析-java的死锁或者性能瓶颈分析" class="headerlink" title="案例分析:java的死锁或者性能瓶颈分析"></a>案例分析:java的死锁或者性能瓶颈分析</h2><blockquote><p>JDK heap dump:分析OOM的问题<br>JDK Thread dumo:分析性能瓶颈(线程信息)<br>得到Thread dump:</p><blockquote><p>在linux:kill -3 PID<br>在windows下:Fn + B 或Ctrl + Break</p></blockquote></blockquote><h1 id="配置IP"><a href="#配置IP" class="headerlink" title="配置IP"></a><strong>配置IP</strong></h1><h2 id="ip-config：查看IP状态"><a href="#ip-config：查看IP状态" class="headerlink" title="ip config：查看IP状态"></a>ip config：查看IP状态</h2><h2 id="更改ip配置：修改-etc-sysconfig-network-scripts下的ifcfg-eth"><a href="#更改ip配置：修改-etc-sysconfig-network-scripts下的ifcfg-eth" class="headerlink" title="更改ip配置：修改/etc/sysconfig/network-scripts下的ifcfg-eth*"></a>更改ip配置：修改/etc/sysconfig/network-scripts下的ifcfg-eth*</h2>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>大数据模块</title>
      <link href="/2018/09/04/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9D%97/"/>
      <url>/2018/09/04/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h3 id="离线计算"><a href="#离线计算" class="headerlink" title="离线计算"></a><strong>离线计算</strong></h3><blockquote><p><strong>Hadoop模块</strong></p><blockquote><p>1.数据存储：HDFS（Hadoop Distributed File System）<br>2.数据计算：MapReduce（java程序、实现离线计算）：在Hadoop 2.X后，Yarn容器中<br>3.Hive：基于HDFS之上的数据仓库，支持SQL语句<br>4.HBase：基于HDFS之上的NoSQL数据库<br>5.ZooKeeper：实现HA（High Availability高可用性、秒杀系统）的功能<br>6.其他：Sqoop、Flume、Pig</p></blockquote></blockquote><blockquote><p><strong>实时计算</strong></p><blockquote><p>1.Redis内存NoSQL数据库<br> Redis Cluster：分布式解决方案<br>2.Apache Storm：进行试试计算（流式计算）</p></blockquote></blockquote><blockquote><p><strong>Spark：只有数据计算，没有数据的存储（依赖HDFS）</strong></p><blockquote><p>1.Scala变成语言：多范式的编程语言（支持多方式编程：1、面向对象 2、函数式编程）<br>2.Spark Core：内核，相当于MapReduce；<br>        最重要的概念：RDD（弹性分布式数据集）<br>3.Spark SQL：类似Hive、支持SQL<br>4.Spark Streaming：处理流式计算的模块，类似Storm</p></blockquote></blockquote>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>Redis初识</title>
      <link href="/2018/08/29/redis%E5%88%9D%E8%AF%86/"/>
      <url>/2018/08/29/redis%E5%88%9D%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<h3 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a><strong>Redis</strong></h3><hr><blockquote><p><strong>高性能Key-Value服务器</strong></p></blockquote><blockquote><p><strong>多种数据结构</strong></p></blockquote><blockquote><p><strong>丰富的功能</strong></p></blockquote><blockquote><p><strong>高可用分布式支持</strong></p></blockquote><hr><h3 id="Redis初识"><a href="#Redis初识" class="headerlink" title="Redis初识"></a><strong>Redis初识</strong></h3><ul><li>由Salvatore Sanfilippo（antirez）制作，目前服务于以色列RedisLabs，早期代码23000行，采用key-value的字典结构，GitHub、twitter、StackOverflow、阿里巴巴、百度、微博、美团、搜狐等都在使用Redis这样的技术，如今Redis已经成为必备技能。<blockquote><ul><li>典型使用场景<blockquote><ol><li>缓存系统：用户访问App server，App Server从cache（Redis）请求数据，如果有，直接返回给App Server，如果没有，cache将从Storage（持久化存储空间）中查找，查找后将从Storage中查找到的数据存入cache中以方便下次查找，而后直接由Storage返回给App Server。</li><li>计数器：微博、视频网站的转发、评论数。</li><li>消息队列系统</li><li>排行榜</li><li>社交网络</li><li>实时系统</li></ol></blockquote></li></ul></blockquote></li></ul><hr><blockquote><p><strong>开源</strong><br><strong>基于键值的存储服务系统</strong><br><strong>支持多种数据结构</strong><br><strong>性能高，功能丰富</strong></p></blockquote><hr><h3 id="Redis特性"><a href="#Redis特性" class="headerlink" title="Redis特性"></a><strong>Redis特性</strong></h3><hr><blockquote><p><strong>速度快</strong>（10W OPS(读写)）</p><blockquote><ol><li><em>将数据存在内存</em></li><li>用c语言编写</li><li>线性模型使用单线程</li></ol></blockquote><hr><p><strong>持久化</strong>（断电不丢数据）</p><blockquote><p>Redis所有数据保持在内存中，对数据的更新将异步地保存到硬盘上</p></blockquote><hr><p><strong>多种数据结构</strong></p><blockquote><ul><li>常规</li></ul><ol><li>字符串（Strings/Blobs/Bitmaps）</li><li>哈希（Hash Tables(objects!)）</li><li>列表（Linked Lists）</li><li>集合（Sets）</li><li>有序集合（Sorted Sets）</li></ol><ul><li>衍生</li></ul><ol><li>位图（BitMaps）</li><li>超小内存唯一值技术（HyperLogLog——有一定误差）</li><li>地理信息定位（GEO）</li></ol></blockquote><hr><p><strong>支持多种编辑语言</strong></p><blockquote><ol><li>Java</li><li>php</li><li>Python</li><li>Ruby</li><li>Lua</li><li>NodeJs</li></ol></blockquote><hr><p><strong>功能丰富</strong></p><blockquote><ol><li>发布订阅</li><li>Lua脚本</li><li>事务</li><li>pipeline</li></ol></blockquote><hr><p><strong>简单</strong></p><blockquote><ol><li>23000行代码</li><li>不依赖外部库（like libevent）</li><li>单线程模型（开发相对容易）</li></ol></blockquote><hr><p><strong>主从复制</strong></p><blockquote><p>在Redis中主服务器的数据可以同步到从服务器上，为高可用以及分布式提供一个很好的基础</p></blockquote><hr><p><strong>高可用、分布式</strong></p><blockquote><p>高可用 ——&gt; Redis-Sentinel(v2.8)支持高可用<br>分布式 ——&gt; Redis-Cluster(V3.0)支持分布式</p></blockquote><hr></blockquote>]]></content>
      
      
      <categories>
          
          <category> Hadoop </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>项目笔记(ssh)</title>
      <link href="/2018/08/23/%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0(ssh)/"/>
      <url>/2018/08/23/%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0(ssh)/</url>
      
        <content type="html"><![CDATA[<h3 id="1-Dao层抽取"><a href="#1-Dao层抽取" class="headerlink" title="1.Dao层抽取"></a>1.Dao层抽取</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">(1)BaseDaoImpl&lt;T&gt;--&gt;BaseDao&lt;T&gt;</span><br><span class="line">   UserDaoImpl&lt;User&gt;--&gt;UserDao&lt;User&gt;</span><br><span class="line">(2)UserDao&lt;User&gt; extends BaseDao&lt;T&gt;</span><br><span class="line">   UserDaoImpl&lt;User&gt; extends BaseDaoImpl&lt;T&gt;</span><br></pre></td></tr></table></figure><h3 id="2-Action层抽取"><a href="#2-Action层抽取" class="headerlink" title="2.Action层抽取"></a>2.Action层抽取</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">(1)BaseAction&lt;T&gt; extends ActionSupport implements ModelDriven&lt;T&gt;</span><br><span class="line">(2)UserAction&lt;User&gt; extends BaseAction&lt;T&gt;</span><br></pre></td></tr></table></figure><h3 id="3-通过反射创建对象"><a href="#3-通过反射创建对象" class="headerlink" title="3.通过反射创建对象"></a>3.通过反射创建对象</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">//在构造方法中动态获取实体类，通过反射创建model对象</span><br><span class="line">//泛型指定user时，this为user，通过反射创建user对象赋值给model</span><br><span class="line">public BaseAction() throws InstantiationException, IllegalAccessException &#123;</span><br><span class="line">//获取父类class属性</span><br><span class="line">ParameterizedType genericsSuperclass = (ParameterizedType) this.getClass().getGenericSuperclass();</span><br><span class="line">//获取泛型数组</span><br><span class="line">Type[] actualTypeArguments = genericsSuperclass.getActualTypeArguments();</span><br><span class="line">Class&lt;T&gt; entityClass = (Class&lt;T&gt;)actualTypeArguments[0];</span><br><span class="line">//通过反射创建对象</span><br><span class="line">model = entityClass.newInstance();</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> SSH </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>hibernate项目笔记(ssh)</title>
      <link href="/2018/08/22/hibernate%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0(ssh)/"/>
      <url>/2018/08/22/hibernate%E9%A1%B9%E7%9B%AE%E7%AC%94%E8%AE%B0(ssh)/</url>
      
        <content type="html"><![CDATA[<h3 id="1-与spring整合中hibernate连接池的两种方式（web-applicationContext-xml）"><a href="#1-与spring整合中hibernate连接池的两种方式（web-applicationContext-xml）" class="headerlink" title="1.与spring整合中hibernate连接池的两种方式（web/applicationContext.xml）"></a>1.与spring整合中hibernate连接池的两种方式（web/applicationContext.xml）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">方式1：</span><br><span class="line">&lt;bean id=<span class="string">"dataSource"</span> class=<span class="string">"com.mchange.v2.c3p0.ComboPooledDataSource"</span>&gt;</span><br><span class="line">&lt;property name=<span class="string">"driverClass"</span> value=<span class="string">"com.mysql.jdbc.Driver"</span>/&gt;</span><br><span class="line">&lt;property name=<span class="string">"jdbcUrl"</span> value=<span class="string">"jdbc:mysql////bos"</span>/&gt;</span><br><span class="line">&lt;property name=<span class="string">"user"</span> value=<span class="string">"root"</span>/&gt;</span><br><span class="line">&lt;property name=<span class="string">"password"</span> value=<span class="string">"tiger"</span>/&gt;</span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">方式2：</span><br><span class="line">&lt;bean id=<span class="string">"dataSource"</span> class=<span class="string">"org.springframework.jdbc.datasource.DriverManagerDataSource"</span>&gt;  </span><br><span class="line">&lt;property name=<span class="string">"driverClassName"</span>&gt;  </span><br><span class="line">    &lt;value&gt;<span class="variable">$&#123;database.driverClass&#125;</span>&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;property name=<span class="string">"url"</span>&gt;  </span><br><span class="line">    &lt;value&gt;<span class="variable">$&#123;database.jdbcUrl&#125;</span>?characterEncoding=UTF-8&amp;amp;useSSL=<span class="literal">false</span>&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;property name=<span class="string">"username"</span>&gt;  </span><br><span class="line">    &lt;value&gt;<span class="variable">$&#123;database.user&#125;</span>&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;property name=<span class="string">"password"</span>&gt;  </span><br><span class="line">    &lt;value&gt;<span class="variable">$&#123;database.password&#125;</span>&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;  </span><br><span class="line">&lt;/bean&gt;</span><br></pre></td></tr></table></figure><h3 id="2-getHibernateTemplate-的使用方法"><a href="#2-getHibernateTemplate-的使用方法" class="headerlink" title="2.getHibernateTemplate()的使用方法"></a>2.getHibernateTemplate()的使用方法</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">(1)导入org.springframework.orm.hibernate5.support.HibernateDaoSupport包</span><br><span class="line">(2)继承HibernateDaoSupport类</span><br><span class="line">* this.getHibernateTemplate().get(entityClass,id);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> SSH </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>maven整合框架笔记(ssh)</title>
      <link href="/2018/08/22/maven%E6%95%B4%E5%90%88%E6%A1%86%E6%9E%B6%E7%AC%94%E8%AE%B0(ssh)/"/>
      <url>/2018/08/22/maven%E6%95%B4%E5%90%88%E6%A1%86%E6%9E%B6%E7%AC%94%E8%AE%B0(ssh)/</url>
      
        <content type="html"><![CDATA[<h3 id="1-配置阿里远程仓库（parent-pom）"><a href="#1-配置阿里远程仓库（parent-pom）" class="headerlink" title="1.配置阿里远程仓库（parent/pom）"></a>1.配置阿里远程仓库（parent/pom）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&lt;repositories&gt;</span><br><span class="line">    &lt;repository&gt;</span><br><span class="line">        &lt;id&gt;maven - ali&lt;/id&gt;</span><br><span class="line">        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;</span><br><span class="line">        &lt;releases&gt;</span><br><span class="line">            &lt;enabled&gt;<span class="literal">true</span>&lt;/enabled&gt;</span><br><span class="line">        &lt;/releases&gt;</span><br><span class="line">        &lt;snapshots&gt;</span><br><span class="line">            &lt;enabled&gt;<span class="literal">true</span>&lt;/enabled&gt;</span><br><span class="line">            &lt;updatePolicy&gt;always&lt;/updatePolicy&gt;</span><br><span class="line">            &lt;checksumPolicy&gt;fail&lt;/checksumPolicy&gt;</span><br><span class="line">        &lt;/snapshots&gt;</span><br><span class="line">    &lt;/repository&gt;</span><br><span class="line">&lt;/repositories&gt;</span><br></pre></td></tr></table></figure><h3 id="2-maven中spring与jdk-1-8的兼容问题（parent-pom）"><a href="#2-maven中spring与jdk-1-8的兼容问题（parent-pom）" class="headerlink" title="2.maven中spring与jdk 1.8的兼容问题（parent/pom）"></a>2.maven中spring与jdk 1.8的兼容问题（parent/pom）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 属性定义指定jar版本 --&gt;</span><br><span class="line">&lt;properties&gt;</span><br><span class="line">&lt;spring.version&gt;5.0.5.RELEASE&lt;/spring.version&gt;</span><br><span class="line">&lt;hibernate.version&gt;5.2.17.Final&lt;/hibernate.version&gt;</span><br><span class="line">&lt;struts2.version&gt;2.3.24&lt;/struts2.version&gt;</span><br><span class="line">&lt;slf4j.version&gt;1.6.6&lt;/slf4j.version&gt;</span><br><span class="line">&lt;log4j.version&gt;1.2.12&lt;/log4j.version&gt;</span><br><span class="line">&lt;shiro.version&gt;1.2.3&lt;/shiro.version&gt;</span><br><span class="line">&lt;/properties&gt;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> SSH </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>git安装与简单操作(hexo)</title>
      <link href="/2018/08/22/git%E5%AE%89%E8%A3%85%E4%B8%8E%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/"/>
      <url>/2018/08/22/git%E5%AE%89%E8%A3%85%E4%B8%8E%E7%AE%80%E5%8D%95%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<ul><li>1.安装Node.js和配置好Node.js环境</li><li>2.安装Git和配置好Git环境</li><li>3.Github账户注册和新建项目，项目必须要遵守格式：账户名.github.io<blockquote><p>在建好的项目右侧有个settings按钮，点击它，向下拉到GitHub Pages，你会看到那边有个网址，访问它，你将会惊奇的发现该项目已经被部署到网络上，能够通过外网来访问它</p></blockquote></li><li><p>4.安装Hexo</p><blockquote><p>创建文件夹<br>通过命令行进入到该文件夹<br>安装Hexo</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo -g</span><br></pre></td></tr></table></figure></blockquote></li><li><p>5.初始化该文件夹</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo init</span><br></pre></td></tr></table></figure></li><li><p>6.安装所需要的组件</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install</span><br></pre></td></tr></table></figure></li><li><p>7.联系hexo与github page</p><blockquote><p>终端输入：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git --global user.name <span class="string">"XXX"</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>终端输入：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git --global user.email <span class="string">"XXX@XXX.XXX"</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>添加密匙到ssh-agent<br>终端输入：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">eval</span> <span class="string">"<span class="variable">$(ssh-agent -s)</span>"</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>添加生成的ssh key到ssh-agent<br>终端输入</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">eval</span> <span class="string">"<span class="variable">$(ssh-agent -s)</span>"</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>github–&gt;setting–&gt;ssh and gpg keys添加id_rsa.pub文件中的ssh key</p></blockquote></li><li><p>8.配置Deployment(_config.yml)</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">deploy:</span><br><span class="line">   <span class="built_in">type</span>: git</span><br><span class="line">   repository: git@github.com:zfhzxg/zfhzxg.github.io.git</span><br><span class="line">   branch: master</span><br></pre></td></tr></table></figure></li><li><p>基础命令</p><blockquote><p>终端：<br>检测ssh是否配置成功</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh -T git@github.com</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>生成ssh密匙：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C <span class="string">"XXX@XXX.XXX"</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>生成.get：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ get init</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>cmd：<br>检测node.js是否安装成功：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">node -v</span><br><span class="line">nmp -v</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>检查hexo是否安装成功：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo -v</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>开启本地服务器：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo g</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>改变端口号：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo server -p XXXX</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>新建博客：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new post <span class="string">"博客名"</span></span><br></pre></td></tr></table></figure></blockquote><blockquote><p>安装拓展：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>生成部署：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo d -g</span><br></pre></td></tr></table></figure></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
    </entry>
    
    <entry>
      <title>git报错</title>
      <link href="/2018/08/22/git%E6%8A%A5%E9%94%99/"/>
      <url>/2018/08/22/git%E6%8A%A5%E9%94%99/</url>
      
        <content type="html"><![CDATA[<ul><li><p>安装主题报错</p><blockquote><p>报错内容： </p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl 18 transfer closed with outstanding <span class="built_in">read</span> data remaining</span><br></pre></td></tr></table></figure></blockquote><blockquote><p>解决方法： </p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.postBuffer 524288000</span><br></pre></td></tr></table></figure></blockquote></li><li><p>curl 18 transfer closed with outstanding read data remaining</p><blockquote><p>解决方法：</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global http.postBuffer 524288000</span><br></pre></td></tr></table></figure></blockquote></li></ul>]]></content>
      
      
      <categories>
          
          <category> other </category>
          
      </categories>
      
      
    </entry>
    
  
  
</search>
